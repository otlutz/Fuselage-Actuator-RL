{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark/evaluate trained agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20220812_151220-14657fa9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "Initial shape from SolutionInputDP24\n",
      "Target shape from SolutionInputDP26\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Initial shape from SolutionInputDP15\n",
      "Target shape from SolutionInputDP20\n",
      "Episode 0 reward: [0.95151471]\n",
      "****************************** Episode: 2 ******************************\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP12\n",
      "Initial shape from SolutionInputDP21\n",
      "Target shape from SolutionInputDP34\n",
      "Episode 1 reward: [0.82111646]\n",
      "****************************** Episode: 3 ******************************\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP38\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 2 reward: [0.91518016]\n",
      "****************************** Episode: 4 ******************************\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP09\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 3 reward: [0.65599812]\n",
      "****************************** Episode: 5 ******************************\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP39\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP14\n",
      "Episode 4 reward: [0.93586691]\n",
      "****************************** Episode: 6 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 5 reward: [0.9198517]\n",
      "****************************** Episode: 7 ******************************\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP13\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 6 reward: [0.95352831]\n",
      "****************************** Episode: 8 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 7 reward: [0.89016929]\n",
      "****************************** Episode: 9 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP28\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP21\n",
      "Episode 8 reward: [0.96506647]\n",
      "****************************** Episode: 10 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP34\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP00\n",
      "Episode 9 reward: [0.79860947]\n",
      "****************************** Episode: 11 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP12\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 10 reward: [0.87310571]\n",
      "****************************** Episode: 12 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP40\n",
      "Episode 11 reward: [0.94929992]\n",
      "****************************** Episode: 13 ******************************\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP08\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP17\n",
      "Episode 12 reward: [0.90742267]\n",
      "****************************** Episode: 14 ******************************\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP01\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 13 reward: [0.92351182]\n",
      "****************************** Episode: 15 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP13\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP24\n",
      "Episode 14 reward: [0.96054658]\n",
      "****************************** Episode: 16 ******************************\n",
      "Initial shape from SolutionInputDP37\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 15 reward: [0.92770111]\n",
      "****************************** Episode: 17 ******************************\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP08\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP21\n",
      "Episode 16 reward: [0.85711768]\n",
      "****************************** Episode: 18 ******************************\n",
      "Initial shape from SolutionInputDP21\n",
      "Target shape from SolutionInputDP07\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP12\n",
      "Episode 17 reward: [0.94527065]\n",
      "****************************** Episode: 19 ******************************\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP27\n",
      "Episode 18 reward: [0.93425615]\n",
      "****************************** Episode: 20 ******************************\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP06\n",
      "Target shape from SolutionInputDP12\n",
      "Episode 19 reward: [0.92209603]\n",
      "****************************** Episode: 21 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP34\n",
      "Episode 20 reward: [0.94486255]\n",
      "****************************** Episode: 22 ******************************\n",
      "Initial shape from SolutionInputDP39\n",
      "Target shape from SolutionInputDP14\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 21 reward: [0.86809929]\n",
      "****************************** Episode: 23 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP02\n",
      "Initial shape from SolutionInputDP36\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 22 reward: [0.91123026]\n",
      "****************************** Episode: 24 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP38\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP19\n",
      "Episode 23 reward: [0.88448765]\n",
      "****************************** Episode: 25 ******************************\n",
      "Initial shape from SolutionInputDP32\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP30\n",
      "Episode 24 reward: [0.93422334]\n",
      "****************************** Episode: 26 ******************************\n",
      "Initial shape from SolutionInputDP26\n",
      "Target shape from SolutionInputDP09\n",
      "Initial shape from SolutionInputDP01\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 25 reward: [0.91046855]\n",
      "****************************** Episode: 27 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP39\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP21\n",
      "Episode 26 reward: [0.97345554]\n",
      "****************************** Episode: 28 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP39\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 27 reward: [0.89503295]\n",
      "****************************** Episode: 29 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 28 reward: [0.88203169]\n",
      "****************************** Episode: 30 ******************************\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP27\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 29 reward: [0.90953978]\n",
      "****************************** Episode: 31 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP00\n",
      "Target shape from SolutionInputDP18\n",
      "Episode 30 reward: [0.89730722]\n",
      "****************************** Episode: 32 ******************************\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP11\n",
      "Episode 31 reward: [0.93321992]\n",
      "****************************** Episode: 33 ******************************\n",
      "Initial shape from SolutionInputDP37\n",
      "Target shape from SolutionInputDP05\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP08\n",
      "Episode 32 reward: [0.96655682]\n",
      "****************************** Episode: 34 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP40\n",
      "Episode 33 reward: [0.57783833]\n",
      "****************************** Episode: 35 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP28\n",
      "Episode 34 reward: [0.90827834]\n",
      "****************************** Episode: 36 ******************************\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP11\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP39\n",
      "Episode 35 reward: [0.94937781]\n",
      "****************************** Episode: 37 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP00\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP17\n",
      "Episode 36 reward: [0.98063876]\n",
      "****************************** Episode: 38 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 37 reward: [0.93293721]\n",
      "****************************** Episode: 39 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP06\n",
      "Episode 38 reward: [0.736463]\n",
      "****************************** Episode: 40 ******************************\n",
      "Initial shape from SolutionInputDP26\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP35\n",
      "Episode 39 reward: [0.86900334]\n",
      "****************************** Episode: 41 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP01\n",
      "Episode 40 reward: [0.90222776]\n",
      "****************************** Episode: 42 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP30\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP09\n",
      "Episode 41 reward: [0.9061386]\n",
      "****************************** Episode: 43 ******************************\n",
      "Initial shape from SolutionInputDP36\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP11\n",
      "Episode 42 reward: [0.95199298]\n",
      "****************************** Episode: 44 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP23\n",
      "Episode 43 reward: [0.90395789]\n",
      "****************************** Episode: 45 ******************************\n",
      "Initial shape from SolutionInputDP03\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP27\n",
      "Episode 44 reward: [0.86269923]\n",
      "****************************** Episode: 46 ******************************\n",
      "Initial shape from SolutionInputDP37\n",
      "Target shape from SolutionInputDP29\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP38\n",
      "Episode 45 reward: [0.96638584]\n",
      "****************************** Episode: 47 ******************************\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP35\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP13\n",
      "Episode 46 reward: [0.95585392]\n",
      "****************************** Episode: 48 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP31\n",
      "Episode 47 reward: [0.92337098]\n",
      "****************************** Episode: 49 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 48 reward: [0.93484915]\n",
      "****************************** Episode: 50 ******************************\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP15\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP34\n",
      "Episode 49 reward: [0.94096415]\n",
      "****************************** Episode: 51 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP16\n",
      "Initial shape from SolutionInputDP27\n",
      "Target shape from SolutionInputDP03\n",
      "Episode 50 reward: [0.9507063]\n",
      "****************************** Episode: 52 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP14\n",
      "Episode 51 reward: [0.84738552]\n",
      "****************************** Episode: 53 ******************************\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP08\n",
      "Episode 52 reward: [0.90351152]\n",
      "****************************** Episode: 54 ******************************\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP31\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 53 reward: [0.96325949]\n",
      "****************************** Episode: 55 ******************************\n",
      "Initial shape from SolutionInputDP31\n",
      "Target shape from SolutionInputDP29\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP20\n",
      "Episode 54 reward: [0.91317887]\n",
      "****************************** Episode: 56 ******************************\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 55 reward: [0.81559895]\n",
      "****************************** Episode: 57 ******************************\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP13\n",
      "Episode 56 reward: [0.74247762]\n",
      "****************************** Episode: 58 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP27\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 57 reward: [0.95880703]\n",
      "****************************** Episode: 59 ******************************\n",
      "Initial shape from SolutionInputDP36\n",
      "Target shape from SolutionInputDP07\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 58 reward: [0.94832943]\n",
      "****************************** Episode: 60 ******************************\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP27\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 59 reward: [0.90496723]\n",
      "****************************** Episode: 61 ******************************\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP09\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 60 reward: [0.90330061]\n",
      "****************************** Episode: 62 ******************************\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP16\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP13\n",
      "Episode 61 reward: [0.91752345]\n",
      "****************************** Episode: 63 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP21\n",
      "Initial shape from SolutionInputDP03\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 62 reward: [0.90271268]\n",
      "****************************** Episode: 64 ******************************\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP16\n",
      "Episode 63 reward: [0.95332871]\n",
      "****************************** Episode: 65 ******************************\n",
      "Initial shape from SolutionInputDP39\n",
      "Target shape from SolutionInputDP12\n",
      "Initial shape from SolutionInputDP27\n",
      "Target shape from SolutionInputDP17\n",
      "Episode 64 reward: [0.97409449]\n",
      "****************************** Episode: 66 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 65 reward: [0.92902137]\n",
      "****************************** Episode: 67 ******************************\n",
      "Initial shape from SolutionInputDP24\n",
      "Target shape from SolutionInputDP23\n",
      "Initial shape from SolutionInputDP31\n",
      "Target shape from SolutionInputDP01\n",
      "Episode 66 reward: [0.92349649]\n",
      "****************************** Episode: 68 ******************************\n",
      "Initial shape from SolutionInputDP15\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP27\n",
      "Episode 67 reward: [0.89733412]\n",
      "****************************** Episode: 69 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP25\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP24\n",
      "Episode 68 reward: [0.89292937]\n",
      "****************************** Episode: 70 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP37\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 69 reward: [0.92379556]\n",
      "****************************** Episode: 71 ******************************\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP30\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP30\n",
      "Episode 70 reward: [0.73198306]\n",
      "****************************** Episode: 72 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP33\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP33\n",
      "Episode 71 reward: [0.91234546]\n",
      "****************************** Episode: 73 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP09\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP33\n",
      "Episode 72 reward: [0.95095782]\n",
      "****************************** Episode: 74 ******************************\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP21\n",
      "Initial shape from SolutionInputDP24\n",
      "Target shape from SolutionInputDP32\n",
      "Episode 73 reward: [0.95906302]\n",
      "****************************** Episode: 75 ******************************\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 74 reward: [0.91598888]\n",
      "****************************** Episode: 76 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP24\n",
      "Target shape from SolutionInputDP04\n",
      "Episode 75 reward: [0.90540916]\n",
      "****************************** Episode: 77 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 76 reward: [0.91446317]\n",
      "****************************** Episode: 78 ******************************\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 77 reward: [0.89034271]\n",
      "****************************** Episode: 79 ******************************\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP10\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP26\n",
      "Episode 78 reward: [0.94750652]\n",
      "****************************** Episode: 80 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP23\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP03\n",
      "Episode 79 reward: [0.89150017]\n",
      "****************************** Episode: 81 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 80 reward: [0.94994664]\n",
      "****************************** Episode: 82 ******************************\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP08\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP23\n",
      "Episode 81 reward: [0.8417115]\n",
      "****************************** Episode: 83 ******************************\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP26\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 82 reward: [0.74233475]\n",
      "****************************** Episode: 84 ******************************\n",
      "Initial shape from SolutionInputDP27\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP40\n",
      "Episode 83 reward: [0.89655172]\n",
      "****************************** Episode: 85 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP08\n",
      "Episode 84 reward: [0.90371468]\n",
      "****************************** Episode: 86 ******************************\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP07\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP09\n",
      "Episode 85 reward: [0.93610628]\n",
      "****************************** Episode: 87 ******************************\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP10\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP32\n",
      "Episode 86 reward: [0.76303071]\n",
      "****************************** Episode: 88 ******************************\n",
      "Initial shape from SolutionInputDP21\n",
      "Target shape from SolutionInputDP34\n",
      "Initial shape from SolutionInputDP00\n",
      "Target shape from SolutionInputDP37\n",
      "Episode 87 reward: [0.93793138]\n",
      "****************************** Episode: 89 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP00\n",
      "Episode 88 reward: [0.92402566]\n",
      "****************************** Episode: 90 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP23\n",
      "Initial shape from SolutionInputDP32\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 89 reward: [0.97647392]\n",
      "****************************** Episode: 91 ******************************\n",
      "Initial shape from SolutionInputDP01\n",
      "Target shape from SolutionInputDP10\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP37\n",
      "Episode 90 reward: [0.95695867]\n",
      "****************************** Episode: 92 ******************************\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP21\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP38\n",
      "Episode 91 reward: [0.96143965]\n",
      "****************************** Episode: 93 ******************************\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP29\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP37\n",
      "Episode 92 reward: [0.95177101]\n",
      "****************************** Episode: 94 ******************************\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP12\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP31\n",
      "Episode 93 reward: [0.85752113]\n",
      "****************************** Episode: 95 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP17\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 94 reward: [0.93953808]\n",
      "****************************** Episode: 96 ******************************\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP06\n",
      "Episode 95 reward: [0.83473012]\n",
      "****************************** Episode: 97 ******************************\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP32\n",
      "Episode 96 reward: [0.95771651]\n",
      "****************************** Episode: 98 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP31\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP11\n",
      "Episode 97 reward: [0.96470238]\n",
      "****************************** Episode: 99 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP11\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP39\n",
      "Episode 98 reward: [0.96376165]\n",
      "****************************** Episode: 100 ******************************\n",
      "Initial shape from SolutionInputDP26\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP21\n",
      "Episode 99 reward: [0.7984408]\n",
      "**********************************************************\n",
      "Inital error (mean) = 0.699\n",
      "Initial error (median) = 0.613\n",
      "Initial error (stdev) = 0.466\n",
      "**********************************************************\n",
      "Final error (mean) = 0.050\n",
      "Final error (median) = 0.040\n",
      "Final error (stdev) = 0.030\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.904\n",
      "Episode Rewards (median) = 0.919\n",
      "Episode Rewards (stdev) = 0.068\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, \"Train\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220812_151220-14657fa9/files/agent_1024000steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "n_actuators = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[0][\"initError\"])\n",
    "    finalErrors.append(info[0][\"Error\"])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.91860027]\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.77087639]\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.93307591]\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.931439]\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.91618703]\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.85901116]\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.9629263]\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.97614612]\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.95309006]\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.93831042]\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.82513641]\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.73685052]\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.92827598]\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.75691897]\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 3\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.89503139]\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.89844207]\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Exit Ansys and try to reconnect\n",
      "Remote exit\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.95314451]\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.963421]\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.55233259]\n",
      "****************************** File: SolutionInputDP60.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.84063404]\n",
      "**********************************************************\n",
      "Inital error (mean) = 0.369\n",
      "Initial error (median) = 0.373\n",
      "Initial error (stdev) = 0.193\n",
      "**********************************************************\n",
      "Final error (mean) = 0.032\n",
      "Final error (median) = 0.031\n",
      "Final error (stdev) = 0.013\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.875\n",
      "Episode Rewards (median) = 0.917\n",
      "Episode Rewards (stdev) = 0.102\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220812_151220-14657fa9/files/agent_1024000steps.pt\", map_location=device))\n",
    "\n",
    "# Initialize variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v12\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[0][\"initError\"])\n",
    "    finalErrors.append(info[0][\"Error\"])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20220822_015612-3cslv0ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Initial shape from SolutionInputDP39\n",
      "Target shape from SolutionInputDP16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP30\n",
      "Episode 0 reward: [0.95935654]\n",
      "****************************** Episode: 2 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP33\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP12\n",
      "Episode 1 reward: [0.98011491]\n",
      "****************************** Episode: 3 ******************************\n",
      "Initial shape from SolutionInputDP36\n",
      "Target shape from SolutionInputDP14\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP38\n",
      "Episode 2 reward: [0.95133899]\n",
      "****************************** Episode: 4 ******************************\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP37\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP09\n",
      "Episode 3 reward: [0.98073755]\n",
      "****************************** Episode: 5 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP05\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 4 reward: [0.94470328]\n",
      "****************************** Episode: 6 ******************************\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP39\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP14\n",
      "Episode 5 reward: [0.97809012]\n",
      "****************************** Episode: 7 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 6 reward: [0.959367]\n",
      "****************************** Episode: 8 ******************************\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP13\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 7 reward: [0.97230152]\n",
      "****************************** Episode: 9 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 8 reward: [0.88680953]\n",
      "****************************** Episode: 10 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP28\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP21\n",
      "Episode 9 reward: [0.98391575]\n",
      "****************************** Episode: 11 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP34\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP00\n",
      "Episode 10 reward: [0.84330414]\n",
      "****************************** Episode: 12 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP12\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 11 reward: [0.92974812]\n",
      "****************************** Episode: 13 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP40\n",
      "Episode 12 reward: [0.97811752]\n",
      "****************************** Episode: 14 ******************************\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP08\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP17\n",
      "Episode 13 reward: [0.96181018]\n",
      "****************************** Episode: 15 ******************************\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP01\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 14 reward: [0.96722905]\n",
      "****************************** Episode: 16 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP13\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP24\n",
      "Episode 15 reward: [0.97717786]\n",
      "****************************** Episode: 17 ******************************\n",
      "Initial shape from SolutionInputDP37\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 16 reward: [0.96993172]\n",
      "****************************** Episode: 18 ******************************\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP08\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP21\n",
      "Episode 17 reward: [0.95283302]\n",
      "****************************** Episode: 19 ******************************\n",
      "Initial shape from SolutionInputDP21\n",
      "Target shape from SolutionInputDP07\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP12\n",
      "Episode 18 reward: [0.96190757]\n",
      "****************************** Episode: 20 ******************************\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP27\n",
      "Episode 19 reward: [0.97870214]\n",
      "****************************** Episode: 21 ******************************\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP06\n",
      "Target shape from SolutionInputDP12\n",
      "Episode 20 reward: [0.96089734]\n",
      "****************************** Episode: 22 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP34\n",
      "Episode 21 reward: [0.97723017]\n",
      "****************************** Episode: 23 ******************************\n",
      "Initial shape from SolutionInputDP39\n",
      "Target shape from SolutionInputDP14\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 22 reward: [0.94538627]\n",
      "****************************** Episode: 24 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP02\n",
      "Initial shape from SolutionInputDP36\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 23 reward: [0.95544323]\n",
      "****************************** Episode: 25 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP38\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP19\n",
      "Episode 24 reward: [0.95576423]\n",
      "****************************** Episode: 26 ******************************\n",
      "Initial shape from SolutionInputDP32\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP30\n",
      "Episode 25 reward: [0.97430547]\n",
      "****************************** Episode: 27 ******************************\n",
      "Initial shape from SolutionInputDP26\n",
      "Target shape from SolutionInputDP09\n",
      "Initial shape from SolutionInputDP01\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 26 reward: [0.95559677]\n",
      "****************************** Episode: 28 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP39\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP21\n",
      "Episode 27 reward: [0.97787427]\n",
      "****************************** Episode: 29 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP39\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 28 reward: [0.8741975]\n",
      "****************************** Episode: 30 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 29 reward: [0.9585664]\n",
      "****************************** Episode: 31 ******************************\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP27\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 30 reward: [0.98124252]\n",
      "****************************** Episode: 32 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP00\n",
      "Target shape from SolutionInputDP18\n",
      "Episode 31 reward: [0.9348715]\n",
      "****************************** Episode: 33 ******************************\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP11\n",
      "Episode 32 reward: [0.97376538]\n",
      "****************************** Episode: 34 ******************************\n",
      "Initial shape from SolutionInputDP37\n",
      "Target shape from SolutionInputDP05\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP08\n",
      "Episode 33 reward: [0.97035207]\n",
      "****************************** Episode: 35 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP40\n",
      "Episode 34 reward: [0.8620973]\n",
      "****************************** Episode: 36 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP28\n",
      "Episode 35 reward: [0.97005269]\n",
      "****************************** Episode: 37 ******************************\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP11\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP39\n",
      "Episode 36 reward: [0.97293757]\n",
      "****************************** Episode: 38 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP00\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP17\n",
      "Episode 37 reward: [0.97965269]\n",
      "****************************** Episode: 39 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP05\n",
      "Episode 38 reward: [0.97524921]\n",
      "****************************** Episode: 40 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP06\n",
      "Episode 39 reward: [0.88135617]\n",
      "****************************** Episode: 41 ******************************\n",
      "Initial shape from SolutionInputDP26\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP35\n",
      "Episode 40 reward: [0.94455898]\n",
      "****************************** Episode: 42 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP01\n",
      "Episode 41 reward: [0.94803445]\n",
      "****************************** Episode: 43 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP30\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP09\n",
      "Episode 42 reward: [0.96744755]\n",
      "****************************** Episode: 44 ******************************\n",
      "Initial shape from SolutionInputDP36\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP11\n",
      "Episode 43 reward: [0.97274027]\n",
      "****************************** Episode: 45 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP23\n",
      "Episode 44 reward: [0.97439556]\n",
      "****************************** Episode: 46 ******************************\n",
      "Initial shape from SolutionInputDP03\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP27\n",
      "Episode 45 reward: [0.96222435]\n",
      "****************************** Episode: 47 ******************************\n",
      "Initial shape from SolutionInputDP37\n",
      "Target shape from SolutionInputDP29\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP38\n",
      "Episode 46 reward: [0.98528339]\n",
      "****************************** Episode: 48 ******************************\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP35\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP13\n",
      "Episode 47 reward: [0.97601975]\n",
      "****************************** Episode: 49 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP31\n",
      "Episode 48 reward: [0.9439927]\n",
      "****************************** Episode: 50 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 49 reward: [0.98156077]\n",
      "****************************** Episode: 51 ******************************\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP15\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP34\n",
      "Episode 50 reward: [0.96377817]\n",
      "****************************** Episode: 52 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP16\n",
      "Initial shape from SolutionInputDP27\n",
      "Target shape from SolutionInputDP03\n",
      "Episode 51 reward: [0.97989746]\n",
      "****************************** Episode: 53 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP14\n",
      "Episode 52 reward: [0.93695692]\n",
      "****************************** Episode: 54 ******************************\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP08\n",
      "Episode 53 reward: [0.95499095]\n",
      "****************************** Episode: 55 ******************************\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP31\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 54 reward: [0.97113853]\n",
      "****************************** Episode: 56 ******************************\n",
      "Initial shape from SolutionInputDP31\n",
      "Target shape from SolutionInputDP29\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP20\n",
      "Episode 55 reward: [0.97580612]\n",
      "****************************** Episode: 57 ******************************\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 56 reward: [0.90774667]\n",
      "****************************** Episode: 58 ******************************\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP13\n",
      "Episode 57 reward: [0.912554]\n",
      "****************************** Episode: 59 ******************************\n",
      "Initial shape from SolutionInputDP28\n",
      "Target shape from SolutionInputDP27\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP02\n",
      "Episode 58 reward: [0.98007816]\n",
      "****************************** Episode: 60 ******************************\n",
      "Initial shape from SolutionInputDP36\n",
      "Target shape from SolutionInputDP07\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 59 reward: [0.98382476]\n",
      "****************************** Episode: 61 ******************************\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP27\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 60 reward: [0.95245741]\n",
      "****************************** Episode: 62 ******************************\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP09\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 61 reward: [0.97404151]\n",
      "****************************** Episode: 63 ******************************\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP16\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP13\n",
      "Episode 62 reward: [0.97149714]\n",
      "****************************** Episode: 64 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP21\n",
      "Initial shape from SolutionInputDP03\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 63 reward: [0.95160806]\n",
      "****************************** Episode: 65 ******************************\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP22\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP16\n",
      "Episode 64 reward: [0.96879992]\n",
      "****************************** Episode: 66 ******************************\n",
      "Initial shape from SolutionInputDP39\n",
      "Target shape from SolutionInputDP12\n",
      "Initial shape from SolutionInputDP27\n",
      "Target shape from SolutionInputDP17\n",
      "Episode 65 reward: [0.97599221]\n",
      "****************************** Episode: 67 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP36\n",
      "Episode 66 reward: [0.9424178]\n",
      "****************************** Episode: 68 ******************************\n",
      "Initial shape from SolutionInputDP24\n",
      "Target shape from SolutionInputDP23\n",
      "Initial shape from SolutionInputDP31\n",
      "Target shape from SolutionInputDP01\n",
      "Episode 67 reward: [0.97263565]\n",
      "****************************** Episode: 69 ******************************\n",
      "Initial shape from SolutionInputDP15\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP27\n",
      "Episode 68 reward: [0.96897445]\n",
      "****************************** Episode: 70 ******************************\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP25\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP24\n",
      "Episode 69 reward: [0.96758207]\n",
      "****************************** Episode: 71 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP37\n",
      "Initial shape from SolutionInputDP12\n",
      "Target shape from SolutionInputDP07\n",
      "Episode 70 reward: [0.96399257]\n",
      "****************************** Episode: 72 ******************************\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP30\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP30\n",
      "Episode 71 reward: [0.91337757]\n",
      "****************************** Episode: 73 ******************************\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP33\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP33\n",
      "Episode 72 reward: [0.96765348]\n",
      "****************************** Episode: 74 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP09\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP33\n",
      "Episode 73 reward: [0.97427502]\n",
      "****************************** Episode: 75 ******************************\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP21\n",
      "Initial shape from SolutionInputDP24\n",
      "Target shape from SolutionInputDP32\n",
      "Episode 74 reward: [0.96900692]\n",
      "****************************** Episode: 76 ******************************\n",
      "Initial shape from SolutionInputDP23\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 75 reward: [0.96144108]\n",
      "****************************** Episode: 77 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP04\n",
      "Initial shape from SolutionInputDP24\n",
      "Target shape from SolutionInputDP04\n",
      "Episode 76 reward: [0.93083497]\n",
      "****************************** Episode: 78 ******************************\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 77 reward: [0.95564414]\n",
      "****************************** Episode: 79 ******************************\n",
      "Initial shape from SolutionInputDP16\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 78 reward: [0.96236842]\n",
      "****************************** Episode: 80 ******************************\n",
      "Initial shape from SolutionInputDP11\n",
      "Target shape from SolutionInputDP10\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP26\n",
      "Episode 79 reward: [0.96979403]\n",
      "****************************** Episode: 81 ******************************\n",
      "Initial shape from SolutionInputDP29\n",
      "Target shape from SolutionInputDP23\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP03\n",
      "Episode 80 reward: [0.95164745]\n",
      "****************************** Episode: 82 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP36\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 81 reward: [0.98552921]\n",
      "****************************** Episode: 83 ******************************\n",
      "Initial shape from SolutionInputDP04\n",
      "Target shape from SolutionInputDP08\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP23\n",
      "Episode 82 reward: [0.94093801]\n",
      "****************************** Episode: 84 ******************************\n",
      "Initial shape from SolutionInputDP34\n",
      "Target shape from SolutionInputDP01\n",
      "Initial shape from SolutionInputDP26\n",
      "Target shape from SolutionInputDP25\n",
      "Episode 83 reward: [0.90011909]\n",
      "****************************** Episode: 85 ******************************\n",
      "Initial shape from SolutionInputDP27\n",
      "Target shape from SolutionInputDP06\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP40\n",
      "Episode 84 reward: [0.96547647]\n",
      "****************************** Episode: 86 ******************************\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP03\n",
      "Initial shape from SolutionInputDP22\n",
      "Target shape from SolutionInputDP08\n",
      "Episode 85 reward: [0.96787626]\n",
      "****************************** Episode: 87 ******************************\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP07\n",
      "Initial shape from SolutionInputDP10\n",
      "Target shape from SolutionInputDP09\n",
      "Episode 86 reward: [0.97528272]\n",
      "****************************** Episode: 88 ******************************\n",
      "Initial shape from SolutionInputDP13\n",
      "Target shape from SolutionInputDP10\n",
      "Initial shape from SolutionInputDP40\n",
      "Target shape from SolutionInputDP32\n",
      "Episode 87 reward: [0.90256501]\n",
      "****************************** Episode: 89 ******************************\n",
      "Initial shape from SolutionInputDP21\n",
      "Target shape from SolutionInputDP34\n",
      "Initial shape from SolutionInputDP00\n",
      "Target shape from SolutionInputDP37\n",
      "Episode 88 reward: [0.96855505]\n",
      "****************************** Episode: 90 ******************************\n",
      "Initial shape from SolutionInputDP38\n",
      "Target shape from SolutionInputDP40\n",
      "Initial shape from SolutionInputDP02\n",
      "Target shape from SolutionInputDP00\n",
      "Episode 89 reward: [0.97033689]\n",
      "****************************** Episode: 91 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP23\n",
      "Initial shape from SolutionInputDP32\n",
      "Target shape from SolutionInputDP10\n",
      "Episode 90 reward: [0.97428232]\n",
      "****************************** Episode: 92 ******************************\n",
      "Initial shape from SolutionInputDP01\n",
      "Target shape from SolutionInputDP10\n",
      "Initial shape from SolutionInputDP05\n",
      "Target shape from SolutionInputDP37\n",
      "Episode 91 reward: [0.97595473]\n",
      "****************************** Episode: 93 ******************************\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP21\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP38\n",
      "Episode 92 reward: [0.97701126]\n",
      "****************************** Episode: 94 ******************************\n",
      "Initial shape from SolutionInputDP09\n",
      "Target shape from SolutionInputDP29\n",
      "Initial shape from SolutionInputDP08\n",
      "Target shape from SolutionInputDP37\n",
      "Episode 93 reward: [0.97009606]\n",
      "****************************** Episode: 95 ******************************\n",
      "Initial shape from SolutionInputDP35\n",
      "Target shape from SolutionInputDP12\n",
      "Initial shape from SolutionInputDP14\n",
      "Target shape from SolutionInputDP31\n",
      "Episode 94 reward: [0.94513425]\n",
      "****************************** Episode: 96 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP17\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP22\n",
      "Episode 95 reward: [0.94002946]\n",
      "****************************** Episode: 97 ******************************\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP24\n",
      "Initial shape from SolutionInputDP18\n",
      "Target shape from SolutionInputDP06\n",
      "Episode 96 reward: [0.90972959]\n",
      "****************************** Episode: 98 ******************************\n",
      "Initial shape from SolutionInputDP30\n",
      "Target shape from SolutionInputDP18\n",
      "Initial shape from SolutionInputDP25\n",
      "Target shape from SolutionInputDP32\n",
      "Episode 97 reward: [0.96631032]\n",
      "****************************** Episode: 99 ******************************\n",
      "Initial shape from SolutionInputDP19\n",
      "Target shape from SolutionInputDP31\n",
      "Initial shape from SolutionInputDP20\n",
      "Target shape from SolutionInputDP11\n",
      "Episode 98 reward: [0.96105201]\n",
      "****************************** Episode: 100 ******************************\n",
      "Initial shape from SolutionInputDP07\n",
      "Target shape from SolutionInputDP11\n",
      "Initial shape from SolutionInputDP33\n",
      "Target shape from SolutionInputDP39\n",
      "Episode 99 reward: [0.97515346]\n",
      "**********************************************************\n",
      "Inital error (mean) = 0.716\n",
      "Initial error (median) = 0.617\n",
      "Initial error (stdev) = 0.462\n",
      "**********************************************************\n",
      "Final error (mean) = 0.025\n",
      "Final error (median) = 0.019\n",
      "Final error (stdev) = 0.019\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.957\n",
      "Episode Rewards (median) = 0.968\n",
      "Episode Rewards (stdev) = 0.028\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, \"Train\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220822_015612-3cslv0ll/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "n_actuators = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99 reward: [0.9478183]\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Episode 99 reward: [0.88967005]\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Episode 99 reward: [0.96434363]\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Episode 99 reward: [0.97140936]\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.94655712]\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.95006133]\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.97756448]\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.91967258]\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.96670955]\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.97413274]\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.86267509]\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.91814643]\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.97835507]\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.94283262]\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.913928]\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.9534976]\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.95902168]\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.97366311]\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.81971625]\n",
      "****************************** File: SolutionInputDP60.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Episode 99 reward: [0.94921319]\n",
      "**********************************************************\n",
      "Inital error (mean) = 0.369\n",
      "Initial error (median) = 0.373\n",
      "Initial error (stdev) = 0.193\n",
      "**********************************************************\n",
      "Final error (mean) = 0.017\n",
      "Final error (median) = 0.015\n",
      "Final error (stdev) = 0.006\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.939\n",
      "Episode Rewards (median) = 0.950\n",
      "Episode Rewards (stdev) = 0.041\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220822_015612-3cslv0ll/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v12\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20220823_195737-15u36mp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:813: DeprecationWarning: invalid escape sequence '\\-'\n",
      "  \"\"\"Start MAPDL locally in gRPC mode.\n"
     ]
    },
    {
     "ename": "LicenseServerConnectionError",
     "evalue": "2022/08/24 10:06:38    DENIED              ansys                           22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n\t\tRequest name ansys does not exist in the licensing pool.\n\t\tCannot connect to license server system.\n\t\t The license server manager (lmgrd) has not been started yet,\n\t\t the wrong port@host or license file is being used, or the\n\t\t port or hostname in the license file has been changed.\n\t\tFeature:       ansys\n\t\tServer name:   198.82.162.15\n\t\tLicense path:  1055@ansys.software.vt.edu;\n\t\tFlexNet Licensing error:-15,10032\n\n2022/08/24 10:06:38    DENIED              FEAT_ANSYS                      22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n\t\tFailover feature 'Ansys Mechanical Enterprise' is not available.\n\t\tRequest name ansys does not exist in the licensing pool.\n\t\tCannot connect to license server system.\n\t\t The license server manager (lmgrd) has not been started yet,\n\t\t the wrong port@host or license file is being used, or the\n\t\t port or hostname in the license file has been changed.\n\t\tFeature:       ansys\n\t\tServer name:   198.82.162.15\n\t\tLicense path:  1055@ansys.software.vt.edu;\n\t\tFlexNet Licensing error:-15,10032",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py:462\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m n_cpu \u001b[39m=\u001b[39m psutil\u001b[39m.\u001b[39mcpu_count(logical\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m    463\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1266\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[39m# pass\u001b[39;00m\n\u001b[1;32m-> 1266\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m   1268\u001b[0m \u001b[39mreturn\u001b[39;00m mapdl\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1248\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m port, actual_run_location \u001b[39m=\u001b[39m launch_grpc(\n\u001b[0;32m   1246\u001b[0m     port\u001b[39m=\u001b[39mport, verbose\u001b[39m=\u001b[39mverbose_mapdl, ip\u001b[39m=\u001b[39mip, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm\n\u001b[0;32m   1247\u001b[0m )\n\u001b[1;32m-> 1248\u001b[0m mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n\u001b[0;32m   1249\u001b[0m     ip\u001b[39m=\u001b[39mip,\n\u001b[0;32m   1250\u001b[0m     port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1251\u001b[0m     cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n\u001b[0;32m   1252\u001b[0m     loglevel\u001b[39m=\u001b[39mloglevel,\n\u001b[0;32m   1253\u001b[0m     set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n\u001b[0;32m   1254\u001b[0m     remove_temp_files\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mremove_temp_files\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m   1255\u001b[0m     log_apdl\u001b[39m=\u001b[39mlog_apdl,\n\u001b[0;32m   1256\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1257\u001b[0m )\n\u001b[0;32m   1258\u001b[0m \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:320\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, print_com, channel, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39m# connect and validate to the channel\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect()\n\u001b[0;32m    322\u001b[0m \u001b[39m# double check we have access to the local path if not\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m# explicitly specified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:383\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m connected:\n\u001b[0;32m    382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[1;32m--> 383\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to connect to MAPDL gRPC instance at \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MapdlGrpc' object has no attribute '_target_str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1248\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m port, actual_run_location \u001b[39m=\u001b[39m launch_grpc(\n\u001b[0;32m   1246\u001b[0m     port\u001b[39m=\u001b[39mport, verbose\u001b[39m=\u001b[39mverbose_mapdl, ip\u001b[39m=\u001b[39mip, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm\n\u001b[0;32m   1247\u001b[0m )\n\u001b[1;32m-> 1248\u001b[0m mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n\u001b[0;32m   1249\u001b[0m     ip\u001b[39m=\u001b[39mip,\n\u001b[0;32m   1250\u001b[0m     port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1251\u001b[0m     cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n\u001b[0;32m   1252\u001b[0m     loglevel\u001b[39m=\u001b[39mloglevel,\n\u001b[0;32m   1253\u001b[0m     set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n\u001b[0;32m   1254\u001b[0m     remove_temp_files\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mremove_temp_files\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m   1255\u001b[0m     log_apdl\u001b[39m=\u001b[39mlog_apdl,\n\u001b[0;32m   1256\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1257\u001b[0m )\n\u001b[0;32m   1258\u001b[0m \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:320\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, print_com, channel, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39m# connect and validate to the channel\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect()\n\u001b[0;32m    322\u001b[0m \u001b[39m# double check we have access to the local path if not\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m# explicitly specified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:383\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m connected:\n\u001b[0;32m    382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[1;32m--> 383\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to connect to MAPDL gRPC instance at \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MapdlGrpc' object has no attribute '_target_str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLicenseServerConnectionError\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m env_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFuselageActuators-v12\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m envs \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mvector\u001b[39m.\u001b[39;49mSyncVectorEnv(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         [make_env(env_name, \u001b[39m0\u001b[39;49m \u001b[39m+\u001b[39;49m i, i, \u001b[39m10\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Create agent\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py:54\u001b[0m, in \u001b[0;36mSyncVectorEnv.__init__\u001b[1;34m(self, env_fns, observation_space, action_space, copy, new_step_api)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_fns \u001b[39m=\u001b[39m env_fns\n\u001b[1;32m---> 54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs \u001b[39m=\u001b[39m [env_fn() \u001b[39mfor\u001b[39;00m env_fn \u001b[39min\u001b[39;00m env_fns]\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmetadata\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_fns \u001b[39m=\u001b[39m env_fns\n\u001b[1;32m---> 54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs \u001b[39m=\u001b[39m [env_fn() \u001b[39mfor\u001b[39;00m env_fn \u001b[39min\u001b[39;00m env_fns]\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmetadata\n",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 12\u001b[0m in \u001b[0;36mmake_env.<locals>.thunk\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthunk\u001b[39m():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(env_id, n_actuators\u001b[39m=\u001b[39;49mn_actions, mode\u001b[39m=\u001b[39;49mmode, record\u001b[39m=\u001b[39;49mrecord, seed\u001b[39m=\u001b[39;49mseed, port\u001b[39m=\u001b[39;49m\u001b[39m50056\u001b[39;49m\u001b[39m+\u001b[39;49midx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mwrappers\u001b[39m.\u001b[39mRecordEpisodeStatistics(env)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\envs\\registration.py:649\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[0;32m    645\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid render_mode provided: \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m. Valid render_modes: None, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(render_modes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m         )\n\u001b[0;32m    648\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m    650\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    651\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    652\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    653\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[0;32m    654\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py:76\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv.__init__\u001b[1;34m(self, render_mode, n_actuators, mode, port, file1, file2, record, seed)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSurrogate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m     \u001b[39m# Check if MAPDL server is active, and start it if it's not\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_process(\u001b[39m'\u001b[39m\u001b[39mansys\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 76\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch_ansys()\n\u001b[0;32m     77\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSurrogate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoly \u001b[39m=\u001b[39m PolynomialFeatures(degree\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py:467\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     n_cpu\u001b[39m=\u001b[39m\u001b[39mmin\u001b[39m(\u001b[39m4\u001b[39m, n_cpu) \u001b[39m#license sometimes won't let me use more than 4 processors?\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, port\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m    468\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n\u001b[0;32m    469\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning on\u001b[39m\u001b[39m\"\u001b[39m, n_cpu, \u001b[39m\"\u001b[39m\u001b[39mprocessors\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1264\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n\u001b[0;32m   1261\u001b[0m     \u001b[39m# Failed to launch for some reason.  Check if failure was due\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m     \u001b[39m# to the license check\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m license_server_check:\n\u001b[1;32m-> 1264\u001b[0m         lic_check\u001b[39m.\u001b[39;49mcheck()\n\u001b[0;32m   1265\u001b[0m         \u001b[39m# pass\u001b[39;00m\n\u001b[0;32m   1266\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\licensing.py:449\u001b[0m, in \u001b[0;36mLicenseChecker.check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_file_success \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mraise\u001b[39;00m LicenseServerConnectionError(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_file_msg))\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_checkout_success:\n\u001b[0;32m    452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mLicenseServerConnectionError\u001b[0m: 2022/08/24 10:06:38    DENIED              ansys                           22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n\t\tRequest name ansys does not exist in the licensing pool.\n\t\tCannot connect to license server system.\n\t\t The license server manager (lmgrd) has not been started yet,\n\t\t the wrong port@host or license file is being used, or the\n\t\t port or hostname in the license file has been changed.\n\t\tFeature:       ansys\n\t\tServer name:   198.82.162.15\n\t\tLicense path:  1055@ansys.software.vt.edu;\n\t\tFlexNet Licensing error:-15,10032\n\n2022/08/24 10:06:38    DENIED              FEAT_ANSYS                      22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n\t\tFailover feature 'Ansys Mechanical Enterprise' is not available.\n\t\tRequest name ansys does not exist in the licensing pool.\n\t\tCannot connect to license server system.\n\t\t The license server manager (lmgrd) has not been started yet,\n\t\t the wrong port@host or license file is being used, or the\n\t\t port or hostname in the license file has been changed.\n\t\tFeature:       ansys\n\t\tServer name:   198.82.162.15\n\t\tLicense path:  1055@ansys.software.vt.edu;\n\t\tFlexNet Licensing error:-15,10032"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 8, \"Train\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220823_195737-15u36mp0/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "n_actuators = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "Exception ignored in: <function VectorEnv.__del__ at 0x000002333F676170>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 294, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 221, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in close_extras\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in <listcomp>\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"c:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py\", line 297, in close\n",
      "    self.mapdl.exit() # close ANSYS\n",
      "AttributeError: 'FuselageActuatorsEnv' object has no attribute 'mapdl'\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP41.inp reward: [0.92973315]\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP42.inp reward: [0.9052981]\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP43.inp reward: [0.96321721]\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP44.inp reward: [0.97523797]\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP45.inp reward: [0.96364815]\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP46.inp reward: [0.95863959]\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP47.inp reward: [0.94451858]\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP48.inp reward: [0.96365639]\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP49.inp reward: [0.97932442]\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP50.inp reward: [0.97228283]\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP51.inp reward: [0.90019697]\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP52.inp reward: [0.88726577]\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP53.inp reward: [0.96775397]\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Reconnect failed - remote exit again\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 3\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP54.inp reward: [0.92546547]\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP55.inp reward: [0.89686742]\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP56.inp reward: [0.9757634]\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP57.inp reward: [0.97313783]\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP58.inp reward: [0.97145368]\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP59.inp reward: [0.72542898]\n",
      "****************************** File: SolutionInputDP60.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP60.inp reward: [0.91344054]\n",
      "**********************************************************\n",
      "Inital error (mean) = 0.369\n",
      "Initial error (median) = 0.373\n",
      "Initial error (stdev) = 0.193\n",
      "**********************************************************\n",
      "Final error (mean) = 0.017\n",
      "Final error (median) = 0.015\n",
      "Final error (stdev) = 0.005\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.935\n",
      "Episode Rewards (median) = 0.961\n",
      "Episode Rewards (stdev) = 0.057\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_name, 0 + i, i, 8, file1, file2, False) for i in range(1)]\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 8).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220823_195737-15u36mp0/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v12\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"File:\", file1, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20220823_110710-191kxpmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:813: DeprecationWarning: invalid escape sequence '\\-'\n",
      "  \"\"\"Start MAPDL locally in gRPC mode.\n"
     ]
    },
    {
     "ename": "LicenseServerConnectionError",
     "evalue": "2022/08/24 10:06:38    DENIED              ansys                           22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n\t\tRequest name ansys does not exist in the licensing pool.\n\t\tCannot connect to license server system.\n\t\t The license server manager (lmgrd) has not been started yet,\n\t\t the wrong port@host or license file is being used, or the\n\t\t port or hostname in the license file has been changed.\n\t\tFeature:       ansys\n\t\tServer name:   198.82.162.15\n\t\tLicense path:  1055@ansys.software.vt.edu;\n\t\tFlexNet Licensing error:-15,10032\n\n2022/08/24 10:06:38    DENIED              FEAT_ANSYS                      22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n\t\tFailover feature 'Ansys Mechanical Enterprise' is not available.\n\t\tRequest name ansys does not exist in the licensing pool.\n\t\tCannot connect to license server system.\n\t\t The license server manager (lmgrd) has not been started yet,\n\t\t the wrong port@host or license file is being used, or the\n\t\t port or hostname in the license file has been changed.\n\t\tFeature:       ansys\n\t\tServer name:   198.82.162.15\n\t\tLicense path:  1055@ansys.software.vt.edu;\n\t\tFlexNet Licensing error:-15,10032",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py:462\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    461\u001b[0m n_cpu \u001b[39m=\u001b[39m psutil\u001b[39m.\u001b[39mcpu_count(logical\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m--> 462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n",
      "\u001b[0;32m    463\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1266\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1265\u001b[0m         \u001b[39m# pass\u001b[39;00m\n",
      "\u001b[1;32m-> 1266\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;32m   1268\u001b[0m \u001b[39mreturn\u001b[39;00m mapdl\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1248\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1245\u001b[0m port, actual_run_location \u001b[39m=\u001b[39m launch_grpc(\n",
      "\u001b[0;32m   1246\u001b[0m     port\u001b[39m=\u001b[39mport, verbose\u001b[39m=\u001b[39mverbose_mapdl, ip\u001b[39m=\u001b[39mip, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm\n",
      "\u001b[0;32m   1247\u001b[0m )\n",
      "\u001b[1;32m-> 1248\u001b[0m mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n",
      "\u001b[0;32m   1249\u001b[0m     ip\u001b[39m=\u001b[39mip,\n",
      "\u001b[0;32m   1250\u001b[0m     port\u001b[39m=\u001b[39mport,\n",
      "\u001b[0;32m   1251\u001b[0m     cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n",
      "\u001b[0;32m   1252\u001b[0m     loglevel\u001b[39m=\u001b[39mloglevel,\n",
      "\u001b[0;32m   1253\u001b[0m     set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n",
      "\u001b[0;32m   1254\u001b[0m     remove_temp_files\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mremove_temp_files\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n",
      "\u001b[0;32m   1255\u001b[0m     log_apdl\u001b[39m=\u001b[39mlog_apdl,\n",
      "\u001b[0;32m   1256\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n",
      "\u001b[0;32m   1257\u001b[0m )\n",
      "\u001b[0;32m   1258\u001b[0m \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:320\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, print_com, channel, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    319\u001b[0m \u001b[39m# connect and validate to the channel\u001b[39;00m\n",
      "\u001b[1;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect()\n",
      "\u001b[0;32m    322\u001b[0m \u001b[39m# double check we have access to the local path if not\u001b[39;00m\n",
      "\u001b[0;32m    323\u001b[0m \u001b[39m# explicitly specified\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:383\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n",
      "\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m connected:\n",
      "\u001b[0;32m    382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n",
      "\u001b[1;32m--> 383\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to connect to MAPDL gRPC instance at \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    384\u001b[0m     )\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MapdlGrpc' object has no attribute '_target_str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1248\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1245\u001b[0m port, actual_run_location \u001b[39m=\u001b[39m launch_grpc(\n",
      "\u001b[0;32m   1246\u001b[0m     port\u001b[39m=\u001b[39mport, verbose\u001b[39m=\u001b[39mverbose_mapdl, ip\u001b[39m=\u001b[39mip, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm\n",
      "\u001b[0;32m   1247\u001b[0m )\n",
      "\u001b[1;32m-> 1248\u001b[0m mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n",
      "\u001b[0;32m   1249\u001b[0m     ip\u001b[39m=\u001b[39mip,\n",
      "\u001b[0;32m   1250\u001b[0m     port\u001b[39m=\u001b[39mport,\n",
      "\u001b[0;32m   1251\u001b[0m     cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n",
      "\u001b[0;32m   1252\u001b[0m     loglevel\u001b[39m=\u001b[39mloglevel,\n",
      "\u001b[0;32m   1253\u001b[0m     set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n",
      "\u001b[0;32m   1254\u001b[0m     remove_temp_files\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mremove_temp_files\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n",
      "\u001b[0;32m   1255\u001b[0m     log_apdl\u001b[39m=\u001b[39mlog_apdl,\n",
      "\u001b[0;32m   1256\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n",
      "\u001b[0;32m   1257\u001b[0m )\n",
      "\u001b[0;32m   1258\u001b[0m \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:320\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, print_com, channel, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    319\u001b[0m \u001b[39m# connect and validate to the channel\u001b[39;00m\n",
      "\u001b[1;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect()\n",
      "\u001b[0;32m    322\u001b[0m \u001b[39m# double check we have access to the local path if not\u001b[39;00m\n",
      "\u001b[0;32m    323\u001b[0m \u001b[39m# explicitly specified\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:383\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n",
      "\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m connected:\n",
      "\u001b[0;32m    382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n",
      "\u001b[1;32m--> 383\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to connect to MAPDL gRPC instance at \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    384\u001b[0m     )\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MapdlGrpc' object has no attribute '_target_str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[1;31mLicenseServerConnectionError\u001b[0m              Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m env_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFuselageActuators-v12\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m run_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m envs \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mvector\u001b[39m.\u001b[39;49mSyncVectorEnv(\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         [make_env(env_name, \u001b[39m0\u001b[39;49m \u001b[39m+\u001b[39;49m i, i, \u001b[39m10\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m)]\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     )\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Create agent\u001b[39;00m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py:54\u001b[0m, in \u001b[0;36mSyncVectorEnv.__init__\u001b[1;34m(self, env_fns, observation_space, action_space, copy, new_step_api)\u001b[0m\n",
      "\u001b[0;32m     39\u001b[0m \u001b[39m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n",
      "\u001b[0;32m     40\u001b[0m \n",
      "\u001b[0;32m     41\u001b[0m \u001b[39mArgs:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     51\u001b[0m \u001b[39m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n",
      "\u001b[0;32m     52\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_fns \u001b[39m=\u001b[39m env_fns\n",
      "\u001b[1;32m---> 54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs \u001b[39m=\u001b[39m [env_fn() \u001b[39mfor\u001b[39;00m env_fn \u001b[39min\u001b[39;00m env_fns]\n",
      "\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n",
      "\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmetadata\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m     39\u001b[0m \u001b[39m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n",
      "\u001b[0;32m     40\u001b[0m \n",
      "\u001b[0;32m     41\u001b[0m \u001b[39mArgs:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     51\u001b[0m \u001b[39m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n",
      "\u001b[0;32m     52\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_fns \u001b[39m=\u001b[39m env_fns\n",
      "\u001b[1;32m---> 54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs \u001b[39m=\u001b[39m [env_fn() \u001b[39mfor\u001b[39;00m env_fn \u001b[39min\u001b[39;00m env_fns]\n",
      "\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n",
      "\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmetadata\n",
      "\n",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 12\u001b[0m in \u001b[0;36mmake_env.<locals>.thunk\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthunk\u001b[39m():\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(env_id, n_actuators\u001b[39m=\u001b[39;49mn_actions, mode\u001b[39m=\u001b[39;49mmode, record\u001b[39m=\u001b[39;49mrecord, seed\u001b[39m=\u001b[39;49mseed, port\u001b[39m=\u001b[39;49m\u001b[39m50056\u001b[39;49m\u001b[39m+\u001b[39;49midx)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mwrappers\u001b[39m.\u001b[39mRecordEpisodeStatistics(env)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\envs\\registration.py:649\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    644\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n",
      "\u001b[0;32m    645\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid render_mode provided: \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m. Valid render_modes: None, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(render_modes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    646\u001b[0m         )\n",
      "\u001b[0;32m    648\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 649\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "\u001b[0;32m    650\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    651\u001b[0m     \u001b[39mif\u001b[39;00m (\n",
      "\u001b[0;32m    652\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;32m    653\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n",
      "\u001b[0;32m    654\u001b[0m     ):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py:76\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv.__init__\u001b[1;34m(self, render_mode, n_actuators, mode, port, file1, file2, record, seed)\u001b[0m\n",
      "\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSurrogate\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;32m     74\u001b[0m     \u001b[39m# Check if MAPDL server is active, and start it if it's not\u001b[39;00m\n",
      "\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_process(\u001b[39m'\u001b[39m\u001b[39mansys\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;32m---> 76\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch_ansys()\n",
      "\u001b[0;32m     77\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSurrogate\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoly \u001b[39m=\u001b[39m PolynomialFeatures(degree\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py:467\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    465\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;32m    466\u001b[0m     n_cpu\u001b[39m=\u001b[39m\u001b[39mmin\u001b[39m(\u001b[39m4\u001b[39m, n_cpu) \u001b[39m#license sometimes won't let me use more than 4 processors?\u001b[39;00m\n",
      "\u001b[1;32m--> 467\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, port\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n",
      "\u001b[0;32m    468\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n",
      "\u001b[0;32m    469\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning on\u001b[39m\u001b[39m\"\u001b[39m, n_cpu, \u001b[39m\"\u001b[39m\u001b[39mprocessors\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1264\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1260\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "\u001b[0;32m   1261\u001b[0m     \u001b[39m# Failed to launch for some reason.  Check if failure was due\u001b[39;00m\n",
      "\u001b[0;32m   1262\u001b[0m     \u001b[39m# to the license check\u001b[39;00m\n",
      "\u001b[0;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m license_server_check:\n",
      "\u001b[1;32m-> 1264\u001b[0m         lic_check\u001b[39m.\u001b[39;49mcheck()\n",
      "\u001b[0;32m   1265\u001b[0m         \u001b[39m# pass\u001b[39;00m\n",
      "\u001b[0;32m   1266\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\ansys\\mapdl\\core\\licensing.py:449\u001b[0m, in \u001b[0;36mLicenseChecker.check\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_file_success \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "\u001b[1;32m--> 449\u001b[0m     \u001b[39mraise\u001b[39;00m LicenseServerConnectionError(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_file_msg))\n",
      "\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_checkout_success:\n",
      "\u001b[0;32m    452\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mLicenseServerConnectionError\u001b[0m: 2022/08/24 10:06:38    DENIED              ansys                           22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n",
      "\t\tRequest name ansys does not exist in the licensing pool.\n",
      "\t\tCannot connect to license server system.\n",
      "\t\t The license server manager (lmgrd) has not been started yet,\n",
      "\t\t the wrong port@host or license file is being used, or the\n",
      "\t\t port or hostname in the license file has been changed.\n",
      "\t\tFeature:       ansys\n",
      "\t\tServer name:   198.82.162.15\n",
      "\t\tLicense path:  1055@ansys.software.vt.edu;\n",
      "\t\tFlexNet Licensing error:-15,10032\n",
      "\n",
      "2022/08/24 10:06:38    DENIED              FEAT_ANSYS                      22.1 (2021.1108)             1/0/0/0                 1/1/1/1   19968:FEAT_ANSYS:tlutz@DESKTOP-H0E6H7J:winx64              6892:192.168.0.204  \n",
      "\t\tFailover feature 'Ansys Mechanical Enterprise' is not available.\n",
      "\t\tRequest name ansys does not exist in the licensing pool.\n",
      "\t\tCannot connect to license server system.\n",
      "\t\t The license server manager (lmgrd) has not been started yet,\n",
      "\t\t the wrong port@host or license file is being used, or the\n",
      "\t\t port or hostname in the license file has been changed.\n",
      "\t\tFeature:       ansys\n",
      "\t\tServer name:   198.82.162.15\n",
      "\t\tLicense path:  1055@ansys.software.vt.edu;\n",
      "\t\tFlexNet Licensing error:-15,10032"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 12, \"Train\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 12).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220823_110710-191kxpmg/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "n_actuators = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "Exception ignored in: <function VectorEnv.__del__ at 0x00000236F1C020E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 294, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 221, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in close_extras\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in <listcomp>\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"c:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v12.py\", line 297, in close\n",
      "    self.mapdl.exit() # close ANSYS\n",
      "AttributeError: 'FuselageActuatorsEnv' object has no attribute 'mapdl'\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 12).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220823_110710-191kxpmg/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v12\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 12, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"File:\", file1, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20220906_113755-i6dt2th4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './wandb/run-20220906_113755-i6dt2th4/files/agent_16382976steps.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X25sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X25sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(envs, \u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X25sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m agent\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X25sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39m./wandb/run-20220906_113755-i6dt2th4/files/agent_16382976steps.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mdevice))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X25sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Load the trained policy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X25sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m initErrors \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\torch\\serialization.py:709\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    707\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 709\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    710\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    711\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    713\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    714\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\torch\\serialization.py:240\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 240\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    241\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\torch\\serialization.py:221\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 221\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './wandb/run-20220906_113755-i6dt2th4/files/agent_16382976steps.pt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, \"Surrogate\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220906_113755-i6dt2th4/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    maxForces.append(np.max(np.abs(envs[0].forces)))\n",
    "    maxDevs.append(info[\"maxDev\"][0])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results (benchmark!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\envs\\registration.py:592: UserWarning: \u001b[33mWARN: The environment FuselageActuators-v12 is out of date. You should consider upgrading to version `v13`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP41.inp reward: [0.95084065]\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP42.inp reward: [0.91269352]\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP43.inp reward: [0.96105648]\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP44.inp reward: [0.97629432]\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP45.inp reward: [0.94495343]\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP46.inp reward: [0.9510517]\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP47.inp reward: [0.96419692]\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP48.inp reward: [0.97339345]\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP49.inp reward: [0.97881379]\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP50.inp reward: [0.97453439]\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP51.inp reward: [0.9076326]\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP52.inp reward: [0.93293771]\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP53.inp reward: [0.97650937]\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP54.inp reward: [0.94547986]\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP55.inp reward: [0.92704277]\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP56.inp reward: [0.96271862]\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP57.inp reward: [0.97743708]\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP58.inp reward: [0.97428093]\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP59.inp reward: [0.83597988]\n",
      "****************************** File: SolutionInputDP60.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Research\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 6 processors\n",
      "File: C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/SolutionInputDP60.inp reward: [0.9508406]\n",
      "**********************************************************\n",
      "Inital error (mean) = 0.369\n",
      "Initial error (median) = 0.373\n",
      "Initial error (stdev) = 0.193\n",
      "Final error (max) = 0.735\n",
      "**********************************************************\n",
      "Final error (mean) = 0.014\n",
      "Final error (median) = 0.012\n",
      "Final error (stdev) = 0.004\n",
      "Final error (max) = 0.027\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.038\n",
      "Max Deviation (median) = 0.035\n",
      "Max Deviation (stdev) = 0.010\n",
      "Max Deviation (max) = 0.072\n",
      "**********************************************************\n",
      "Max Force (mean) = 162.679\n",
      "Max Force (median) = 134.015\n",
      "Max Force (stdev) = 61.790\n",
      "Max Force (max) = 288.925\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.949\n",
      "Episode Rewards (median) = 0.956\n",
      "Episode Rewards (stdev) = 0.033\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action_mean, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_name, 0 + i, i, 8, file1, file2, False) for i in range(1)]\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20220906_113755-i6dt2th4/files/agent_16382976steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v12\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs)\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"File:\", file1, \"reward:\", episodeReward)\n",
    "    initErrors.append(info[\"initError\"][0])\n",
    "    finalErrors.append(info[\"Error\"][0])\n",
    "    maxDevs.append(info[\"maxDev\"][0])\n",
    "    maxForces.append(np.max(np.abs(info[\"Forces\"][0])))\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(finalErrors[0:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run 1rh99fva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results (benchmark!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mos\u001b[39;00m \u001b[39mimport\u001b[39;00m path\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\matplotlib\\pyplot.py:49\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcycler\u001b[39;00m \u001b[39mimport\u001b[39;00m cycler\n\u001b[0;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolorbar\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\matplotlib\\colorbar.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, collections, cm, colors, contour, ticker\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39martist\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmartist\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpatches\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpatches\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\matplotlib\\collections.py:1922\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1918\u001b[0m         gc\u001b[39m.\u001b[39mrestore()\n\u001b[0;32m   1919\u001b[0m         renderer\u001b[39m.\u001b[39mclose_group(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m-> 1922\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mQuadMesh\u001b[39;00m(Collection):\n\u001b[0;32m   1923\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[39m    Class for the efficient drawing of a quadrilateral mesh.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1980\u001b[0m \u001b[39m    .. (0, meshWidth), (1, 0), (1, 1), and so on.\u001b[39;00m\n\u001b[0;32m   1981\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1983\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1984\u001b[0m         \u001b[39m# signature deprecation since=\"3.5\": Change to new signature after the\u001b[39;00m\n\u001b[0;32m   1985\u001b[0m         \u001b[39m# deprecation has expired. Also remove setting __init__.__signature__,\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m         \u001b[39m# and remove the Notes from the docstring.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\matplotlib\\artist.py:108\u001b[0m, in \u001b[0;36mArtist.__init_subclass__\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init_subclass__\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m    105\u001b[0m     \u001b[39m# Inject custom set() methods into the subclass with signature and\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39m# docstring based on the subclasses' properties.\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39;49m(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mset, \u001b[39m'\u001b[39;49m\u001b[39m_autogenerated_signature\u001b[39;49m\u001b[39m'\u001b[39;49m):\n\u001b[0;32m    109\u001b[0m         \u001b[39m# Don't overwrite cls.set if the subclass or one of its parents\u001b[39;00m\n\u001b[0;32m    110\u001b[0m         \u001b[39m# has defined a set method set itself.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m         \u001b[39m# If there was no explicit definition, cls.set is inherited from\u001b[39;00m\n\u001b[0;32m    112\u001b[0m         \u001b[39m# the hierarchy of auto-generated set methods, which hold the\u001b[39;00m\n\u001b[0;32m    113\u001b[0m         \u001b[39m# flag _autogenerated_signature.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Artist\u001b[39m.\u001b[39mset(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action_mean, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_name, 0 + i, i, 8, file1, file2, False) for i in range(1)]\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-202210xx-1rh99fva/files/agent_16383936steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files[:-1]:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v12\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Reward:\", episodeReward)\n",
    "    print(\"Final Error:\", info[0][\"Error\"])\n",
    "    initErrors.append(info[0][\"initError\"])\n",
    "    finalErrors.append(info[0][\"Error\"])\n",
    "    maxDevs.append(info[0][\"maxDev\"])\n",
    "    maxForces.append(np.max(np.abs(info[0][\"Forces\"])))\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************\n",
    "Initial error (mean) = 0.369\n",
    "Initial error (median) = 0.373\n",
    "Initial error (stdev) = 0.193\n",
    "Initial error (max) = 0.735\n",
    "**********************************************************\n",
    "Final error (mean) = 0.014\n",
    "Final error (median) = 0.014\n",
    "Final error (stdev) = 0.004\n",
    "Final error (max) = 0.026\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.038\n",
    "Max Deviation (median) = 0.035\n",
    "Max Deviation (stdev) = 0.010\n",
    "Max Deviation (max) = 0.070\n",
    "**********************************************************\n",
    "Max Force (mean) = 192.650\n",
    "Max Force (median) = 195.592\n",
    "Max Force (stdev) = 62.441\n",
    "Max Force (max) = 315.294\n",
    "**********************************************************\n",
    "Episode Rewards (mean) = 0.946\n",
    "Episode Rewards (median) = 0.961\n",
    "Episode Rewards (stdev) = 0.040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0298507100599101, 0.017254446530365584, 0.021615704528441883, 0.012942392856088397, 0.012800510961419985, 0.014857135124434255, 0.01998216653345925, 0.01642131162053982, 0.02601855027292011, 0.018140539986531385, 0.019549890548037456, 0.015642428588348228, 0.023637804940378884, 0.022819586410309598, 0.03136077142805789, 0.01513990791904582, 0.025504290414473225, 0.02225182094722002, 0.020857447868979428, 0.01606911742928942, 0.027204796655699696, 0.03848169485582721, 0.020343618576019556, 0.02110250492627685, 0.022595118497838622, 0.02257560536889411, 0.011467658426140413, 0.008720533973174092, 0.01513990791904582, 0.015944365237115524, 0.016638273395600433, 0.013802727491867976, 0.0728753895407129, 0.020047359780752224, 0.01832969143647896, 0.011862521635073658, 0.01712280019619034, 0.019145937629995308, 0.0186856890498836, 0.016603520799107464, 0.008433037214234615, 0.014971371137607305, 0.026981450051999062, 0.026491180321745075, 0.01841226591987247, 0.012875062665836644, 0.015204972105934564, 0.020525650982935344, 0.018823290760821624, 0.029631156035424783, 0.017536137245449473, 0.026138615726320768, 0.01907102335560386, 0.013871668692264517, 0.021615704528441883, 0.012824869478139819, 0.02213284310584039, 0.02563308705930903, 0.026597502570137568, 0.023473882561515527, 0.012871601154929203, 0.012871601154929203, 0.02305205718013586, 0.01197162303187552, 0.02569559274955418, 0.030403159983109596, 0.02399577908784406, 0.01518812156368766, 0.012344012656958399, 0.0390571282380963, 0.01225765169739919, 0.0691162371059295, 0.020101184200589643, 0.013812961918484329, 0.017085134577960376, 0.019666808853493218, 0.014414544157446738, 0.013031348526034986, 0.020757913769292025, 0.01086144087767555, 0.010585993119275727, 0.025457096176868817, 0.06676366205496925, 0.02489622397436662, 0.017419262406647956, 0.022908577774177766, 0.019614636655094678, 0.02410631927303329, 0.026981450051999062, 0.03828772562292926, 0.02377058072181144, 0.013254456679203549, 0.020471503855864905, 0.016890189630304118, 0.012385723722816223, 0.014455109349918633, 0.025834183736918735, 0.028364288646915273, 0.010305526223084593, 0.048235603343349524]\n"
     ]
    }
   ],
   "source": [
    "print(finalErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\envs\\registration.py:592: UserWarning: \u001b[33mWARN: The environment FuselageActuators-v12 is out of date. You should consider upgrading to version `v22`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "Initial error (mean) = 0.733\n",
      "Initial error (median) = 0.668\n",
      "Initial error (stdev) = 0.481\n",
      "Initial error (max) = 2.161\n",
      "**********************************************************\n",
      "Final error (mean) = 0.008\n",
      "Final error (median) = 0.008\n",
      "Final error (stdev) = 0.001\n",
      "Final error (max) = 0.011\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.020\n",
      "Max Deviation (median) = 0.020\n",
      "Max Deviation (stdev) = 0.006\n",
      "Max Deviation (max) = 0.034\n",
      "**********************************************************\n",
      "Max Force (mean) = 44.503\n",
      "Max Force (median) = 0.277\n",
      "Max Force (stdev) = 70.952\n",
      "Max Force (max) = 414.716\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 95.397\n",
      "Episode Rewards (median) = 95.410\n",
      "Episode Rewards (stdev) = 0.442\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v12\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, \"Surrogate\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-202210xx-1rh99fva/files/agent_16383936steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "\n",
    "for i in range(100):\n",
    "    # print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    minError=10\n",
    "    \n",
    "    for i in range(100):\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        # print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "    # print('_'*30)\n",
    "    # print(\"Episodic Reward:\", episodeReward)\n",
    "    # print(\"Final Error:\", min(errors))\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************\n",
    "Initial error (mean) = 0.751\n",
    "Initial error (median) = 0.729\n",
    "Initial error (stdev) = 0.486\n",
    "Initial error (max) = 2.120\n",
    "**********************************************************\n",
    "Final error (mean) = 0.021\n",
    "Final error (median) = 0.020\n",
    "Final error (stdev) = 0.011\n",
    "Final error (max) = 0.073\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.051\n",
    "Max Deviation (median) = 0.047\n",
    "Max Deviation (stdev) = 0.021\n",
    "Max Deviation (max) = 0.151\n",
    "**********************************************************\n",
    "Max Force (mean) = 291.501\n",
    "Max Force (median) = 281.630\n",
    "Max Force (stdev) = 93.408\n",
    "Max Force (max) = 494.694\n",
    "**********************************************************\n",
    "Episode Rewards (mean) = 0.950\n",
    "Episode Rewards (median) = 0.970\n",
    "Episode Rewards (stdev) = 0.059"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run 1rw7aiak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results (benchmark!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function VectorEnv.__del__ at 0x0000027DE482A8C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 294, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 221, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in close_extras\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in <listcomp>\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"c:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py\", line 305, in close\n",
      "    self.mapdl.exit() # close ANSYS\n",
      "AttributeError: 'FuselageActuatorsEnv' object has no attribute 'mapdl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "****************************** File: SolutionInputDP41.inp ******************************\n",
      "Reward: [0.96401438]\n",
      "Final Error: [0.01075947]\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.93329732]\n",
      "Final Error: [0.01060037]\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.97416241]\n",
      "Final Error: [0.0145414]\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.98046362]\n",
      "Final Error: [0.0112595]\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.95921704]\n",
      "Final Error: [0.01657423]\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.97494931]\n",
      "Final Error: [0.00698118]\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.97297301]\n",
      "Final Error: [0.02111887]\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.9798727]\n",
      "Final Error: [0.00971041]\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.98343845]\n",
      "Final Error: [0.01056593]\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.98104761]\n",
      "Final Error: [0.01224465]\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.93584976]\n",
      "Final Error: [0.00965094]\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.95447487]\n",
      "Final Error: [0.01093743]\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.98264627]\n",
      "Final Error: [0.0108329]\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.95945523]\n",
      "Final Error: [0.01005929]\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.95042392]\n",
      "Final Error: [0.01260509]\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.96875773]\n",
      "Final Error: [0.02818037]\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.98494262]\n",
      "Final Error: [0.00955564]\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.98113623]\n",
      "Final Error: [0.01741687]\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.87038458]\n",
      "Final Error: [0.00924635]\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.467\n",
      "Initial error (median) = 0.482\n",
      "Initial error (stdev) = 0.251\n",
      "Initial error (max) = 0.923\n",
      "**********************************************************\n",
      "Final error (mean) = 0.013\n",
      "Final error (median) = 0.011\n",
      "Final error (stdev) = 0.005\n",
      "Final error (max) = 0.028\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.042\n",
      "Max Deviation (median) = 0.041\n",
      "Max Deviation (stdev) = 0.011\n",
      "Max Deviation (max) = 0.073\n",
      "**********************************************************\n",
      "Max Force (mean) = 39.613\n",
      "Max Force (median) = 0.296\n",
      "Max Force (stdev) = 64.003\n",
      "Max Force (max) = 355.364\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.963\n",
      "Episode Rewards (median) = 0.973\n",
      "Episode Rewards (stdev) = 0.027\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action_mean, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_name, 0 + i, i, 8, file1, file2, False) for i in range(1)]\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230202_201959-1rw7aiak/files/agent_16383936steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files[:-1]:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v22\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Reward:\", episodeReward)\n",
    "    print(\"Final Error:\", info[\"Error\"])\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(info[\"Error\"])\n",
    "    maxDevs.append(info[\"maxDev\"])\n",
    "    maxForces.append(np.max(np.abs(info[\"Forces\"])))\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************\n",
    "Initial error (mean) = 0.467\n",
    "\n",
    "Initial error (median) = 0.482\n",
    "\n",
    "Initial error (stdev) = 0.251\n",
    "\n",
    "Initial error (max) = 0.923\n",
    "\n",
    "**********************************************************\n",
    "Final error (mean) = 0.013\n",
    "\n",
    "Final error (median) = 0.011\n",
    "\n",
    "Final error (stdev) = 0.005\n",
    "\n",
    "Final error (max) = 0.028\n",
    "\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.042\n",
    "\n",
    "Max Deviation (median) = 0.041\n",
    "\n",
    "Max Deviation (stdev) = 0.011\n",
    "\n",
    "Max Deviation (max) = 0.073\n",
    "**********************************************************\n",
    "Max Force (mean) = 39.613\n",
    "\n",
    "Max Force (median) = 0.296\n",
    "\n",
    "Max Force (stdev) = 64.003\n",
    "\n",
    "Max Force (max) = 355.364\n",
    "\n",
    "**********************************************************\n",
    "Episode Rewards (mean) = 0.963\n",
    "\n",
    "Episode Rewards (median) = 0.973\n",
    "\n",
    "Episode Rewards (stdev) = 0.027\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple refinements for benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test samples from DOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n",
      "Resetting the environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.           77.64316     -12.641788   -164.10995       0.\n",
      "    0.           -0.35935456    0.            0.            0.\n",
      "    0.38042971    0.            0.            4.231175    -38.10283\n",
      "   31.882853    -47.72741    -128.57443   ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Error: 0.010759471368781668\n",
      "[   0.           87.329285      0.82870483 -179.8241        0.\n",
      "    0.           -0.5641745     0.            0.            0.\n",
      "    0.6437365     0.            0.          -11.063778    -35.56475\n",
      "   10.108953    -15.6756935  -125.70817   ]\n",
      "Intermediate Error: 0.01002723042450995\n",
      "[   0.           85.818436      8.455743   -186.74683       0.\n",
      "    0.           -0.7507243     0.            0.            0.\n",
      "    0.94481623    0.            0.          -10.627416    -26.231667\n",
      "  -11.358032      3.6207485  -132.32411   ]\n",
      "Intermediate Error: 0.010042940233643364\n",
      "[   0.          82.090195    16.287361  -191.31284      0.\n",
      "    0.          -0.9349557    0.           0.           0.\n",
      "    1.2459487    0.           0.         -10.11258    -17.59326\n",
      "  -33.853115    23.129026  -138.23785  ]\n",
      "Intermediate Error: 0.010053380033033023\n",
      "[   0.          78.1914      24.045918  -195.74991      0.\n",
      "    0.          -1.1204238    0.           0.           0.\n",
      "    1.5453703    0.           0.          -9.343428    -8.8063965\n",
      "  -56.315258    42.460358  -144.374    ]\n",
      "Intermediate Error: 0.01007099169386261\n",
      "[ 0.0000000e+00  7.4242538e+01  3.1828373e+01 -2.0015665e+02\n",
      "  0.0000000e+00  0.0000000e+00 -1.3073423e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  1.8424683e+00  0.0000000e+00\n",
      "  0.0000000e+00 -8.6009893e+00 -2.9896736e-02 -7.8787010e+01\n",
      "  6.1833923e+01 -1.5052960e+02]\n",
      "Intermediate Error: 0.010093363061348143\n",
      "[   0.          70.274666    39.6281    -204.56721      0.\n",
      "    0.          -1.4957591    0.           0.           0.\n",
      "    2.1372342    0.           0.          -7.876526     8.7529745\n",
      " -101.25058     81.238945  -156.72337  ]\n",
      "Intermediate Error: 0.010120721888762148\n",
      "[   0.          66.2893      47.4466    -208.9825       0.\n",
      "    0.          -1.6856713    0.           0.           0.\n",
      "    2.4296668    0.           0.          -7.174422    17.539219\n",
      " -123.707275   100.67903   -162.95132  ]\n",
      "Intermediate Error: 0.010152980963569628\n",
      "[   0.          62.2871      55.283543  -213.40297      0.\n",
      "    0.          -1.8770741    0.           0.           0.\n",
      "    2.7197738    0.           0.          -6.494108    26.329079\n",
      " -146.15704    120.15379   -169.21356  ]\n",
      "Intermediate Error: 0.010190056390995442\n",
      "[   0.         58.268257   63.138756 -217.82857     0.          0.\n",
      "   -2.069962    0.          0.          0.          3.007563    0.\n",
      "    0.         -5.835328   35.122414 -168.60017   139.66315  -175.5097  ]\n",
      "Intermediate Error: 0.010231879813634425\n",
      "______________________________\n",
      "Best Error: 0.01002723042450995\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.          101.94037     112.33746       0.7588992     0.\n",
      "    0.           -0.31516832    0.            0.            0.\n",
      "    0.            0.24147417    0.         -131.63402     -18.13797\n",
      "  -24.565685    186.17943      48.072937  ]\n",
      "Intermediate Error: 0.010600373626039224\n",
      "[ 0.0000000e+00  1.3037450e+02  1.2059840e+02 -3.8097744e+01\n",
      "  0.0000000e+00  0.0000000e+00 -5.3604442e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  1.9103102e-01  0.0000000e+00\n",
      "  0.0000000e+00 -1.3824396e+02 -1.6144333e+00 -3.4192211e+01\n",
      "  2.0914790e+02  3.6127754e+01]\n",
      "Intermediate Error: 0.009969701527096916\n",
      "[   0.          128.54065     130.4643      -45.579308      0.\n",
      "    0.           -0.70605785    0.            0.            0.\n",
      "    0.4189028     0.            0.         -141.32074       5.8921647\n",
      "  -57.131737    232.1721       31.387814  ]\n",
      "Intermediate Error: 0.01005580960807925\n",
      "[   0.         124.437294   139.23392    -51.006035     0.\n",
      "    0.          -0.8736318    0.           0.           0.\n",
      "    0.6529665    0.           0.        -140.9295      15.093899\n",
      "  -79.91551    252.61269     24.371197 ]\n",
      "Intermediate Error: 0.010143748003957552\n",
      "[   0.         119.89024    148.066      -55.968834     0.\n",
      "    0.          -1.0419754    0.           0.           0.\n",
      "    0.8852073    0.           0.        -140.55988     24.14861\n",
      " -102.906906   273.1456      17.492325 ]\n",
      "Intermediate Error: 0.010241842491574888\n",
      "[   0.         115.30085    156.8949     -60.912262     0.\n",
      "    0.          -1.2117856    0.           0.           0.\n",
      "    1.1153374    0.           0.        -140.15143     33.23511\n",
      " -125.887955   293.6696      10.542599 ]\n",
      "Intermediate Error: 0.010343615388014397\n",
      "[   0.         110.69058    165.74057    -65.85279      0.\n",
      "    0.          -1.3830951    0.           0.           0.\n",
      "    1.3432399    0.           0.        -139.76118     42.32003\n",
      " -148.86908    314.2297       3.5650306]\n",
      "Intermediate Error: 0.010449303020377792\n",
      "[   0.         106.06566    174.6014     -70.79722      0.\n",
      "    0.          -1.5559099    0.           0.           0.\n",
      "    1.5689187    0.           0.        -139.38676     51.40682\n",
      " -171.8468     334.82327     -3.4443655]\n",
      "Intermediate Error: 0.01055859145307492\n",
      "[   0.         101.42647    183.47755    -75.745636     0.\n",
      "    0.          -1.7302251    0.           0.           0.\n",
      "    1.7923777    0.           0.        -139.02885     60.494823\n",
      " -194.82158    355.45096    -10.484535 ]\n",
      "Intermediate Error: 0.010671411828547236\n",
      "[   0.          96.77325    192.36885    -80.698105     0.\n",
      "    0.          -1.9060361    0.           0.           0.\n",
      "    2.0136228    0.           0.        -138.6871      69.584045\n",
      " -217.79356    376.11255    -17.555325 ]\n",
      "Intermediate Error: 0.010787626628039676\n",
      "______________________________\n",
      "Best Error: 0.009969701527096916\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 3\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[ 0.0000000e+00 -1.3984306e+02  1.3191159e+02  1.4652353e+02\n",
      "  0.0000000e+00 -2.1517658e-01  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -3.1167361e-01  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -2.0024791e+02 -2.7147543e+02 -1.5114243e+02\n",
      "  1.6081332e+02  7.6612129e+01]\n",
      "Intermediate Error: 0.014541401610992153\n",
      "[ 0.0000000e+00 -1.5955812e+02  1.4232372e+02  1.5335014e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  2.6877248e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -2.3351823e-01 -1.9409406e+02 -2.6115073e+02 -1.7780324e+02\n",
      "  1.7907141e+02  6.8723862e+01]\n",
      "Intermediate Error: 0.014025047627488813\n",
      "[   0.         -167.67639     154.12048     148.07388       0.\n",
      "    0.            0.            0.            0.53040254    0.\n",
      "    0.            0.           -0.4686985  -193.34021    -251.6415\n",
      " -200.55359     200.30254      59.50844   ]\n",
      "Intermediate Error: 0.014149263610910885\n",
      "[   0.         -174.50192     166.27881     141.61487       0.\n",
      "    0.            0.            0.            0.79148775    0.\n",
      "    0.            0.           -0.70497763 -193.70612    -242.4573\n",
      " -223.08945     222.40224      50.758656  ]\n",
      "Intermediate Error: 0.01425300457634147\n",
      "[   0.        -181.14029    178.44688    134.9573       0.\n",
      "    0.           0.           0.           1.05325      0.\n",
      "    0.           0.          -0.9422725 -194.1417    -233.26442\n",
      " -245.5687     244.56657     41.987892 ]\n",
      "Intermediate Error: 0.014359854443869504\n",
      "[   0.        -187.76288    190.62216    128.28134      0.\n",
      "    0.           0.           0.           1.3158282    0.\n",
      "    0.           0.          -1.180547  -194.59225   -224.0845\n",
      " -268.05533    266.77124     33.21078  ]\n",
      "Intermediate Error: 0.014469370588013786\n",
      "[   0.        -194.38545    202.79945    121.602196     0.\n",
      "    0.           0.           0.           1.5792438    0.\n",
      "    0.           0.          -1.4198008 -195.04001   -214.9105\n",
      " -290.55078    289.00348     24.417606 ]\n",
      "Intermediate Error: 0.014581698675478314\n",
      "[   0.        -201.01048    214.97878    114.92255      0.\n",
      "    0.           0.           0.           1.8434999    0.\n",
      "    0.           0.          -1.6600338 -195.4843    -205.74294\n",
      " -313.05612    311.26306     15.608847 ]\n",
      "Intermediate Error: 0.014696766729511977\n",
      "[   0.        -207.6381     227.16002    108.242584     0.\n",
      "    0.           0.           0.           2.108598     0.\n",
      "    0.           0.          -1.9012468 -195.92479   -196.58168\n",
      " -335.57144    333.54977      6.784417 ]\n",
      "Intermediate Error: 0.01481451438790746\n",
      "[   0.        -214.26833    239.34314    101.56236      0.\n",
      "    0.           0.           0.           2.3745396    0.\n",
      "    0.           0.          -2.1434405 -196.36137   -187.42676\n",
      " -358.0968     355.86356     -2.0555906]\n",
      "Intermediate Error: 0.014934894460827427\n",
      "______________________________\n",
      "Best Error: 0.014025047627488813\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 3\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 7\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[ 0.0000000e+00  8.2328636e+01  2.0158418e+02  1.7722627e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -1.7099147e-01  4.2806947e-01\n",
      "  0.0000000e+00 -1.0340015e+02 -1.0779680e+01 -3.0175701e+01\n",
      "  2.3783923e+02 -2.4030346e+01]\n",
      "Intermediate Error: 0.011259501984226566\n",
      "[ 0.00000000e+00  1.02623428e+02  2.11603668e+02  1.48054916e+02\n",
      "  0.00000000e+00  0.00000000e+00 -2.44687602e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.00372839e-01\n",
      "  0.00000000e+00 -1.08852135e+02  3.04257584e+00 -4.44657631e+01\n",
      "  2.62351349e+02 -3.00909500e+01]\n",
      "Intermediate Error: 0.01067723585177382\n",
      "[ 0.00000000e+00  1.00211136e+02  2.21608673e+02  1.40494064e+02\n",
      "  0.00000000e+00  0.00000000e+00 -4.52282548e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.57698542e-01  0.00000000e+00\n",
      "  0.00000000e+00 -1.09252792e+02  1.01366377e+01 -6.75322266e+01\n",
      "  2.83733582e+02 -3.46738319e+01]\n",
      "Intermediate Error: 0.010698303043139179\n",
      "[   0.           95.90361     230.95221     134.79198       0.\n",
      "    0.           -0.6582965     0.            0.            0.\n",
      "    0.31893814    0.            0.         -107.29285      18.33476\n",
      "  -90.651474    303.46713     -40.69124   ]\n",
      "Intermediate Error: 0.010801022456063462\n",
      "[   0.          91.284836   240.36435    129.37851      0.\n",
      "    0.          -0.8655821    0.           0.           0.\n",
      "    0.477816     0.           0.        -105.40102     26.377562\n",
      " -113.90663    323.29114    -46.645153 ]\n",
      "Intermediate Error: 0.010854561996224657\n",
      "[   0.          86.6305     249.78662    123.970955     0.\n",
      "    0.          -1.0746008    0.           0.           0.\n",
      "    0.6341367    0.           0.        -103.49947     34.447273\n",
      " -137.14174    343.1252     -52.66996  ]\n",
      "Intermediate Error: 0.010914353734363525\n",
      "[   0.           81.9513      259.23276     118.56155       0.\n",
      "    0.           -1.2853649     0.            0.            0.\n",
      "    0.78783035    0.            0.         -101.6279       42.518425\n",
      " -160.36635     363.00055     -58.733784  ]\n",
      "Intermediate Error: 0.010979944720074688\n",
      "[   0.          77.25128    268.70126    113.14596      0.\n",
      "    0.          -1.497874     0.           0.           0.\n",
      "    0.9389076    0.           0.         -99.78378     50.59372\n",
      " -183.5782     382.91473    -64.83947  ]\n",
      "Intermediate Error: 0.011051305274603586\n",
      "[   0.          72.53073    278.1921     107.72426      0.\n",
      "    0.          -1.7121214    0.           0.           0.\n",
      "    1.0873778    0.           0.         -97.96736     58.672592\n",
      " -206.77782    402.8681     -70.98608  ]\n",
      "Intermediate Error: 0.01112830505336978\n",
      "[   0.          67.78994    287.70505    102.29646      0.\n",
      "    0.          -1.9281002    0.           0.           0.\n",
      "    1.2332512    0.           0.         -96.17818     66.75499\n",
      " -229.96548    422.86044    -77.173294 ]\n",
      "Intermediate Error: 0.011210762741597066\n",
      "______________________________\n",
      "Best Error: 0.01067723585177382\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.           65.24738     -68.95616    -266.0804        0.\n",
      "    0.           -0.38868436    0.            0.            0.\n",
      "    0.38117906    0.            0.            7.6992817   -95.85055\n",
      "   28.402752   -134.26642    -167.51794   ]\n",
      "Intermediate Error: 0.01657423003315362\n",
      "[ 0.0000000e+00  8.6911217e+01 -5.5848129e+01 -2.8883945e+02\n",
      "  0.0000000e+00  0.0000000e+00 -5.8065814e-01 -1.6390683e-01\n",
      "  0.0000000e+00  0.0000000e+00  5.6975883e-01  0.0000000e+00\n",
      "  0.0000000e+00 -5.3938904e+00 -8.8381996e+01  9.6950073e+00\n",
      " -1.0137172e+02  0.0000000e+00]\n",
      "Intermediate Error: 0.3866764420070699\n",
      "[   0.           78.91218     -37.789513   -301.34137       0.\n",
      "    0.           -0.73087       0.            0.            0.\n",
      "    0.99055934    0.            0.           37.998863   -116.57972\n",
      "    8.572185   -131.03682    -141.02736   ]\n",
      "Exit Ansys and try to reconnect\n",
      "Remote exit\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.014754099340752645\n",
      "[   0.          83.49443    -28.959358  -311.03183      0.\n",
      "    0.          -0.9022825    0.           0.           0.\n",
      "    1.2038196    0.           0.          35.09      -109.05156\n",
      "  -14.009591  -107.37954   -143.98166  ]\n",
      "Intermediate Error: 0.014509865480430673\n",
      "[   0.          81.78654    -22.405918  -316.1748       0.\n",
      "    0.          -1.0645026    0.           0.           0.\n",
      "    1.4300376    0.           0.          37.172386  -100.62696\n",
      "  -37.72494    -88.23811   -148.68956  ]\n",
      "Intermediate Error: 0.014417916249067837\n",
      "[   0.          79.01139    -15.8702345 -320.25748      0.\n",
      "    0.          -1.2266666    0.           0.           0.\n",
      "    1.6549493    0.           0.          39.59616    -92.283875\n",
      "  -61.78605    -69.25568   -153.3947   ]\n",
      "Intermediate Error: 0.014296388866344338\n",
      "[   0.          76.10607     -9.333206  -324.2509       0.\n",
      "    0.          -1.3904102    0.           0.           0.\n",
      "    1.8771193    0.           0.          42.07974    -83.893234\n",
      "  -85.83807    -50.303684  -158.20367  ]\n",
      "Intermediate Error: 0.014176582353038493\n",
      "[   0.          73.157814    -2.765658  -328.23743      0.\n",
      "    0.          -1.555881     0.           0.           0.\n",
      "    2.0963006    0.           0.          44.52549    -75.49453\n",
      " -109.87433    -31.310854  -163.06207  ]\n",
      "Intermediate Error: 0.014060670655209223\n",
      "[   0.          70.18075      3.8318238 -332.23154      0.\n",
      "    0.          -1.723095     0.           0.           0.\n",
      "    2.3124902    0.           0.          46.930977   -67.08514\n",
      " -133.88959    -12.2765465 -167.9719   ]\n",
      "Intermediate Error: 0.013949188123710543\n",
      "[   0.          67.17633     10.459385  -336.2342       0.\n",
      "    0.          -1.8920462    0.           0.           0.\n",
      "    2.5257       0.           0.          49.295223   -58.665882\n",
      " -157.88425      6.8001404 -172.93175  ]\n",
      "Intermediate Error: 0.013842199837883047\n",
      "______________________________\n",
      "Best Error: 0.013842199837883047\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.         -199.10013     -14.47582     163.98413       0.\n",
      "    0.            0.            0.            0.37255457    0.\n",
      "    0.6948917     0.            0.          136.33995     -16.472647\n",
      "  -60.089832    -93.106636   -128.19115   ]\n",
      "Intermediate Error: 0.0069811756235382645\n",
      "[ 0.0000000e+00 -2.3145992e+02 -8.7227364e+00  1.8697029e+02\n",
      "  0.0000000e+00  0.0000000e+00 -1.1872431e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  9.1521090e-01  0.0000000e+00\n",
      "  0.0000000e+00  1.5332510e+02 -4.5713882e+00 -9.0296982e+01\n",
      " -8.2945175e+01 -1.3838104e+02]\n",
      "Intermediate Error: 0.007496411322002431\n",
      "[ 0.0000000e+00 -2.3962996e+02  2.1976185e-01  1.8489133e+02\n",
      "  0.0000000e+00  0.0000000e+00 -2.8207463e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  1.0847460e+00  0.0000000e+00\n",
      "  0.0000000e+00  1.5592693e+02  4.7753468e+00 -1.1369674e+02\n",
      " -6.4076286e+01 -1.4608090e+02]\n",
      "Intermediate Error: 0.007621647112870966\n",
      "[   0.         -244.98682       9.668832    180.0006        0.\n",
      "    0.           -0.45197424    0.            0.            0.\n",
      "    1.2457628     0.            0.          156.53796      13.714666\n",
      " -136.42558     -43.852173   -153.1343    ]\n",
      "Intermediate Error: 0.00767819667649259\n",
      "[   0.        -250.00674     19.174232   174.75024      0.\n",
      "    0.          -0.6240343    0.           0.           0.\n",
      "    1.403959     0.           0.         156.93445     22.634697\n",
      " -159.05261    -23.47013   -160.17432  ]\n",
      "Intermediate Error: 0.007732238991207675\n",
      "[   0.        -255.0005      28.70095    169.45683      0.\n",
      "    0.          -0.7977006    0.           0.           0.\n",
      "    1.5599598    0.           0.         157.28748     31.548431\n",
      " -181.66844     -3.0352955 -167.23384  ]\n",
      "Intermediate Error: 0.0077910324635280725\n",
      "[   0.        -260.0025      38.24224    164.15479      0.\n",
      "    0.          -0.9729072    0.           0.           0.\n",
      "    1.7138516    0.           0.         157.6233      40.46224\n",
      " -204.28082     17.434734  -174.32246  ]\n",
      "Intermediate Error: 0.007855539548800439\n",
      "[   0.        -265.017       47.79752    158.84863      0.\n",
      "    0.          -1.1496418    0.           0.           0.\n",
      "    1.865648     0.           0.         157.94447     49.376133\n",
      " -226.8913      37.93847   -181.44032  ]\n",
      "Intermediate Error: 0.00792568638505681\n",
      "[   0.        -270.0443      57.366535   153.5389       0.\n",
      "    0.          -1.3278989    0.           0.           0.\n",
      "    2.0153553    0.           0.         158.25163     58.29016\n",
      " -249.50017     58.475533  -188.58734  ]\n",
      "Intermediate Error: 0.008001325284301202\n",
      "[   0.        -275.0844      66.94913    148.2257       0.\n",
      "    0.          -1.5076737    0.           0.           0.\n",
      "    2.1629786    0.           0.         158.54506     67.20426\n",
      " -272.10767     79.04579   -195.76326  ]\n",
      "Intermediate Error: 0.008082325089301953\n",
      "______________________________\n",
      "Best Error: 0.0069811756235382645\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 3\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 4\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[ 0.00000000e+00  1.14856766e+02 -1.23885033e+02 -2.62526031e+02\n",
      "  0.00000000e+00  0.00000000e+00 -3.83874863e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.24206015e-01\n",
      "  0.00000000e+00  3.30341873e+01  1.40577164e+02  4.64828377e+01\n",
      " -5.44417953e+01  7.73472748e+01]\n",
      "Intermediate Error: 0.021118867790121676\n",
      "[ 0.0000000e+00  1.3468925e+02 -1.1278997e+02 -2.9149570e+02\n",
      "  0.0000000e+00  0.0000000e+00 -6.1917543e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  1.2668075e-01  0.0000000e+00\n",
      "  0.0000000e+00  1.2480648e+01  1.4540874e+02  2.5914682e+01\n",
      " -2.0785976e+01  8.4459183e+01]\n",
      "Intermediate Error: 0.011428426471937132\n",
      "[   0.          134.90303    -104.79677    -299.7357        0.\n",
      "    0.           -0.826497      0.            0.            0.\n",
      "    0.30716443    0.            0.           10.492147    155.36479\n",
      "    2.7430325     0.8591919    78.70974   ]\n",
      "Intermediate Error: 0.011572828863009074\n",
      "[   0.          131.82062     -97.20522    -304.65027       0.\n",
      "    0.           -1.029073      0.            0.            0.\n",
      "    0.49236467    0.            0.           10.343503    165.54468\n",
      "  -21.35996      21.374548     72.60236   ]\n",
      "Intermediate Error: 0.011606452120270921\n",
      "[   0.         128.16974    -89.69294   -309.0184       0.\n",
      "    0.          -1.2315567    0.           0.           0.\n",
      "    0.6768874    0.           0.          10.540994   175.80267\n",
      "  -45.595947    41.6762      66.37126  ]\n",
      "Intermediate Error: 0.011617501254714967\n",
      "[   0.          124.40504     -82.18007    -313.29175       0.\n",
      "    0.           -1.4347283     0.            0.            0.\n",
      "    0.85973954    0.            0.           10.780073    186.07387\n",
      "  -69.85205      61.966537     60.09954   ]\n",
      "Intermediate Error: 0.011627077096178304\n",
      "[   0.         120.60876    -74.654305  -317.5524       0.\n",
      "    0.          -1.6387261    0.           0.           0.\n",
      "    1.0407573    0.           0.          11.011449   196.35028\n",
      "  -94.10604     82.277504    53.7972   ]\n",
      "Intermediate Error: 0.011639252284278246\n",
      "[   0.         116.79545    -67.113464  -321.8147       0.\n",
      "    0.          -1.8435718    0.           0.           0.\n",
      "    1.2199167    0.           0.          11.22627    206.63019\n",
      " -118.35434    102.61496     47.466568 ]\n",
      "Intermediate Error: 0.011654688611984739\n",
      "[   0.         112.96779    -59.557285  -326.0811       0.\n",
      "    0.          -2.0492663    0.           0.           0.\n",
      "    1.397219     0.           0.          11.423202   216.91324\n",
      " -142.59647    122.97984     41.108208 ]\n",
      "Intermediate Error: 0.011673500905642606\n",
      "[   0.         109.12638    -51.98586   -330.35202      0.\n",
      "    0.          -2.2558072    0.           0.           0.\n",
      "    1.5726695    0.           0.          11.602214   227.19933\n",
      " -166.8325     143.37222     34.7224   ]\n",
      "Intermediate Error: 0.011695672031183822\n",
      "______________________________\n",
      "Best Error: 0.011428426471937132\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 3\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 9\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[  0.          40.31163    125.95794    111.20278      0.\n",
      "   0.           0.           0.           0.24566916   0.\n",
      "   0.           0.29183418   0.         -22.476322     9.285212\n",
      " -13.442357   124.93426    -91.633995  ]\n",
      "Intermediate Error: 0.009710406551043223\n",
      "[   0.           55.138565    135.12755      84.29939       0.\n",
      "    0.           -0.19725354    0.            0.43041384    0.\n",
      "    0.            0.            0.          -25.034117     22.745674\n",
      "  -28.569454    146.63972    -103.04346   ]\n",
      "Intermediate Error: 0.010064602441318557\n",
      "[   0.           51.500065    145.8371       77.26792       0.\n",
      "    0.           -0.36394405    0.            0.62806594    0.\n",
      "    0.            0.            0.          -26.359846     30.554256\n",
      "  -52.23267     169.59752    -108.85535   ]\n",
      "Intermediate Error: 0.010202707890793387\n",
      "[   0.          46.5758     155.83734     71.348076     0.\n",
      "    0.          -0.5299947    0.           0.8284105    0.\n",
      "    0.           0.           0.         -25.486351    39.474617\n",
      "  -75.71089    190.89262   -116.21362  ]\n",
      "Intermediate Error: 0.010304875897207922\n",
      "[   0.          41.37739    165.89279     65.71124      0.\n",
      "    0.          -0.6972284    0.           1.0295354    0.\n",
      "    0.           0.           0.         -24.657354    48.285786\n",
      "  -99.32129    212.27818   -123.48418  ]\n",
      "Intermediate Error: 0.010403874305988518\n",
      "[   0.           36.15024     175.95105      60.0821        0.\n",
      "    0.           -0.86606574    0.            1.2313265     0.\n",
      "    0.            0.            0.          -23.809263     57.118397\n",
      " -122.92166     233.66995    -130.81284   ]\n",
      "Intermediate Error: 0.010507389706809884\n",
      "[   0.          30.904984   186.02539     54.453472     0.\n",
      "    0.          -1.0365212    0.           1.4337605    0.\n",
      "    0.           0.           0.         -22.979193    65.94932\n",
      " -146.52055    255.09758   -138.1703   ]\n",
      "Intermediate Error: 0.010615230962813804\n",
      "[   0.          25.645473   196.1145      48.82115      0.\n",
      "    0.          -1.208596     0.           1.6368406    0.\n",
      "    0.           0.           0.         -22.165003    74.7811\n",
      " -170.1157     276.55893   -145.5593   ]\n",
      "Intermediate Error: 0.010727410543768781\n",
      "[   0.          20.371943   206.21848     43.185173     0.\n",
      "    0.          -1.3822851    0.           1.8405706    0.\n",
      "    0.           0.           0.         -21.367046    83.61323\n",
      " -193.70753    298.05438   -152.97905  ]\n",
      "Intermediate Error: 0.010843741426909593\n",
      "[   0.          15.0846     216.33713     37.54551      0.\n",
      "    0.          -1.5575836    0.           2.0449548    0.\n",
      "    0.           0.           0.         -20.584995    92.44569\n",
      " -217.29622    319.58377   -160.42937  ]\n",
      "Intermediate Error: 0.010964083747962814\n",
      "______________________________\n",
      "Best Error: 0.009710406551043223\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.           81.73379     -88.54377    -215.39848       0.\n",
      "    0.           -0.30106747    0.            0.            0.\n",
      "    0.           -0.23622793    0.          -33.146633     58.571346\n",
      "    5.8414454   -11.313155    122.06532   ]\n",
      "Intermediate Error: 0.010565930007010492\n",
      "[ 0.0000000e+00  8.1184418e+01 -7.5971268e+01 -2.2197667e+02\n",
      "  0.0000000e+00  1.0363277e-01 -4.1516531e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -4.5289989e+01  5.9429180e+01 -2.1015266e+01\n",
      "  1.8733536e+01  1.2371613e+02]\n",
      "Intermediate Error: 0.012142864541955201\n",
      "[ 0.0000000e+00  7.9016991e+01 -6.8834724e+01 -2.2805486e+02\n",
      "  0.0000000e+00  2.1123832e-01 -5.2709138e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00 -4.4393372e+01  7.0254044e+01 -4.4170479e+01\n",
      "  3.8367344e+01  1.1641907e+02]\n",
      "Intermediate Error: 0.012260604995861174\n",
      "[   0.           75.39436     -61.28879    -232.5086        0.\n",
      "    0.31666866   -0.6380546     0.            0.            0.\n",
      "    0.            0.            0.          -44.212334     80.131775\n",
      "  -68.27345      58.78582     110.316414  ]\n",
      "Intermediate Error: 0.012255915293661376\n",
      "[   0.           71.70769     -53.815006   -236.94687       0.\n",
      "    0.42074454   -0.7505677     0.            0.            0.\n",
      "    0.            0.            0.          -43.808094     90.172775\n",
      "  -92.30011      79.04009     103.97025   ]\n",
      "Intermediate Error: 0.012258252715450769\n",
      "[   0.           67.97816     -46.313923   -241.36148       0.\n",
      "    0.52337235   -0.86465144    0.            0.            0.\n",
      "    0.            0.            0.          -43.443787    100.199356\n",
      " -116.33564      99.34753      97.610245  ]\n",
      "Intermediate Error: 0.012261857733439262\n",
      "[   0.          64.22935    -38.795174  -245.77971      0.\n",
      "    0.6245654   -0.9803425    0.           0.           0.\n",
      "    0.           0.           0.         -43.099503   110.23275\n",
      " -140.36176    119.68898     91.209015 ]\n",
      "Intermediate Error: 0.012269184914048527\n",
      "[   0.           60.46152     -31.257143   -250.20102       0.\n",
      "    0.72432023   -1.0976348     0.            0.            0.\n",
      "    0.            0.            0.          -42.779625    120.26956\n",
      " -164.38043     140.06824      84.77114   ]\n",
      "Intermediate Error: 0.012280117520697024\n",
      "[   0.          56.67527    -23.70022   -254.62584      0.\n",
      "    0.8226353   -1.2165232    0.           0.           0.\n",
      "    0.           0.           0.         -42.4834     130.31012\n",
      " -188.39162    160.48479     78.29643  ]\n",
      "Intermediate Error: 0.012294617181020423\n",
      "[   0.          52.87077    -16.124563  -259.05408      0.\n",
      "    0.9195087   -1.3370018    0.           0.           0.\n",
      "    0.           0.           0.         -42.210556   140.35426\n",
      " -212.39561    180.93854     71.785255 ]\n",
      "Intermediate Error: 0.012312696060493783\n",
      "______________________________\n",
      "Best Error: 0.010565930007010492\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.            1.6618414    65.15569     -15.240434      0.\n",
      "    0.            0.            0.20283966    0.25826186    0.\n",
      "    0.            0.            0.          -34.97149    -117.1669\n",
      "  -17.96341      14.142701   -139.44055   ]\n",
      "Intermediate Error: 0.01224465076558846\n",
      "[   0.           11.376398     77.110855    -34.584404      0.\n",
      "    0.           -0.19488686    0.            0.            0.\n",
      "    0.            0.           -0.2146482   -36.29998    -102.68853\n",
      "  -33.755005     35.928837   -149.9845    ]\n",
      "Intermediate Error: 0.011138292450385133\n",
      "[   0.            7.0723042    88.76393     -42.15547       0.\n",
      "    0.           -0.3677305     0.            0.            0.\n",
      "    0.            0.           -0.42698473  -37.46112     -94.55693\n",
      "  -56.021263     56.975838   -155.66566   ]\n",
      "Intermediate Error: 0.01109266840560933\n",
      "[   0.            1.5459557    99.86938     -48.61324       0.\n",
      "    0.           -0.54014504    0.            0.            0.\n",
      "    0.            0.           -0.6405308   -36.872658    -85.58341\n",
      "  -78.24128      76.74135    -162.47168   ]\n",
      "Intermediate Error: 0.011122239357091779\n",
      "[   0.           -4.2166934   111.01036     -54.83297       0.\n",
      "    0.           -0.71392787    0.            0.            0.\n",
      "    0.            0.           -0.8552592   -36.292328    -76.685776\n",
      " -100.56349      96.56353    -169.23508   ]\n",
      "Intermediate Error: 0.011157827296122911\n",
      "[   0.         -10.007542   122.1553     -61.04234      0.\n",
      "    0.          -0.8894352    0.           0.           0.\n",
      "    0.           0.          -1.0712261  -35.69801    -67.77395\n",
      " -122.879196   116.398895  -176.0494   ]\n",
      "Intermediate Error: 0.01119908224282786\n",
      "[   0.         -15.814812   133.31398    -67.25085      0.\n",
      "    0.          -1.0666833    0.           0.           0.\n",
      "    0.           0.          -1.2884262  -35.11835    -58.864635\n",
      " -145.19331    136.26984   -182.8932   ]\n",
      "Intermediate Error: 0.011245017892108316\n",
      "[   0.         -21.635275   144.48552    -73.461914     0.\n",
      "    0.          -1.2456722    0.           0.           0.\n",
      "    0.           0.          -1.5068564  -34.5521     -49.956116\n",
      " -167.50421    156.17503   -189.76822  ]\n",
      "Intermediate Error: 0.011295587205004369\n",
      "[   0.         -27.468668   155.66995    -79.67556      0.\n",
      "    0.          -1.4263968    0.           0.           0.\n",
      "    0.           0.          -1.7265129  -33.99946    -41.048767\n",
      " -189.81224    176.11475   -196.67381  ]\n",
      "Intermediate Error: 0.011350692763580658\n",
      "[   0.         -33.314793   166.8671     -85.89181      0.\n",
      "    0.          -1.6088521    0.           0.           0.\n",
      "    0.           0.          -1.9473919  -33.46012    -32.142616\n",
      " -212.11757    196.08884   -203.60976  ]\n",
      "Intermediate Error: 0.011410278422548102\n",
      "______________________________\n",
      "Best Error: 0.01109266840560933\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 3\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 4\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.         -252.43753      56.31481     355.3639        0.\n",
      "    0.            0.40133992    0.            0.            0.\n",
      "    0.4972489     0.            0.          116.089455     36.910492\n",
      " -111.829735     26.160122    -26.72136   ]\n",
      "Intermediate Error: 0.009650943657340118\n",
      "[ 0.0000000e+00 -2.9799280e+02  5.9031223e+01  3.9308746e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  1.1478124e-01  0.0000000e+00  6.8280971e-01  0.0000000e+00\n",
      "  0.0000000e+00  1.4767749e+02  5.6465721e+01 -1.4325127e+02\n",
      "  2.5874489e+01 -4.3939453e+01]\n",
      "Intermediate Error: 0.009948746656279695\n",
      "[ 0.0000000e+00 -3.0827405e+02  6.8209732e+01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00 -9.0544686e-02  0.0000000e+00\n",
      "  1.9356252e-01  0.0000000e+00  7.8284913e-01  0.0000000e+00\n",
      "  0.0000000e+00  1.5068359e+02  6.6167259e+01 -1.6734238e+02\n",
      "  4.3237366e+01 -5.0491409e+01]\n",
      "Intermediate Error: 0.6079313772042434\n",
      "[ 0.0000000e+00 -3.7850745e+02  2.3294083e+02  2.8987680e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  2.4160433e-01\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  4.0888539e-01\n",
      "  0.0000000e+00  1.4364212e+02  5.0350990e+01 -2.3467386e+02\n",
      "  1.9113916e+02 -1.2024035e+02]\n",
      "Intermediate Error: 0.010510627067346977\n",
      "[ 0.0000000e+00 -3.9899927e+02  2.3713031e+02  2.9958701e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  1.0717725e-01  0.0000000e+00  1.2905867e-01  0.0000000e+00\n",
      "  0.0000000e+00  1.6121541e+02  6.6969109e+01 -2.5910138e+02\n",
      "  1.9806583e+02 -1.3413046e+02]\n",
      "Intermediate Error: 0.010745151836655366\n",
      "[ 0.00000000e+00 -4.06392914e+02  2.46633255e+02  2.96877625e+02\n",
      "  0.00000000e+00  0.00000000e+00 -1.05964445e-01  0.00000000e+00\n",
      "  1.99240983e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.62542343e+02  7.59880981e+01 -2.82825470e+02\n",
      "  2.16881088e+02 -1.39779144e+02]\n",
      "Intermediate Error: 0.010809442986172885\n",
      "[ 0.0000000e+00 -4.1145929e+02  2.5610287e+02  2.9166333e+02\n",
      "  0.0000000e+00  0.0000000e+00 -2.1785192e-01  0.0000000e+00\n",
      "  2.9012960e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  1.6337701e+02  8.5456802e+01 -3.0555722e+02\n",
      "  2.3582852e+02 -1.4598930e+02]\n",
      "Intermediate Error: 0.010859424470570879\n",
      "[ 0.0000000e+00 -4.1634500e+02  2.6567947e+02  2.8625952e+02\n",
      "  0.0000000e+00  0.0000000e+00 -3.3186972e-01  0.0000000e+00\n",
      "  3.8124228e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  1.6390840e+02  9.4791840e+01 -3.2828345e+02\n",
      "  2.5502078e+02 -1.5206964e+02]\n",
      "Intermediate Error: 0.010902369416871373\n",
      "[   0.         -421.21115     275.27078     280.8123        0.\n",
      "    0.           -0.44772238    0.            0.47283083    0.\n",
      "    0.            0.            0.          164.41516     104.13946\n",
      " -350.984       274.24786    -158.19893   ]\n",
      "Intermediate Error: 0.01095022652872495\n",
      "[   0.         -426.09143     284.88098     275.3586        0.\n",
      "    0.           -0.5653441     0.            0.56491935    0.\n",
      "    0.            0.            0.          164.89519     113.4861\n",
      " -373.67667     293.51575    -164.36163   ]\n",
      "Intermediate Error: 0.011002797968162963\n",
      "______________________________\n",
      "Best Error: 0.009650943657340118\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 3\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 4\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.          113.75261    -117.05201    -267.94672       0.\n",
      "    0.           -0.31237364    0.            0.            0.\n",
      "    0.           -0.40320396    0.           93.755104     95.71225\n",
      "   78.05329    -147.96483     -98.24378   ]\n",
      "Intermediate Error: 0.010937428661521285\n",
      "[ 0.0000000e+00  1.1756257e+02 -1.0386194e+02 -2.8857178e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  1.8350196e-01  0.0000000e+00 -1.2484481e-01  0.0000000e+00\n",
      "  0.0000000e+00  7.9623955e+01  9.6876877e+01  5.6922451e+01\n",
      " -1.1907265e+02 -9.8822372e+01]\n",
      "Intermediate Error: 0.009898626280366295\n",
      "[ 0.0000000e+00  1.1309325e+02 -9.1983185e+01 -2.9715610e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  3.8143826e-01 -1.2657407e-01  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  7.7677284e+01  1.0540463e+02  3.5837944e+01\n",
      " -9.6224106e+01 -1.0772052e+02]\n",
      "Intermediate Error: 0.0099299290559176\n",
      "[ 0.00000000e+00  1.06979256e+02 -8.01837769e+01 -3.03890717e+02\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  5.81550539e-01 -2.54285157e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  7.65892868e+01  1.14027283e+02  1.42261982e+01\n",
      " -7.37546692e+01 -1.16746864e+02]\n",
      "Intermediate Error: 0.009935401951563158\n",
      "[   0.          100.61947     -68.4397     -310.3723        0.\n",
      "    0.            0.            0.            0.78262067   -0.38152152\n",
      "    0.            0.            0.           75.71014     122.7034\n",
      "   -7.452341    -51.400326   -125.84865   ]\n",
      "Intermediate Error: 0.009947099089972215\n",
      "[   0.          94.215775   -56.705536  -316.80502      0.\n",
      "    0.           0.           0.           0.9844531   -0.508073\n",
      "    0.           0.           0.          74.86909    131.37582\n",
      "  -29.15948    -29.046928  -134.958    ]\n",
      "Intermediate Error: 0.009963586162345908\n",
      "[   0.          87.80711    -44.976593  -323.22842      0.\n",
      "    0.           0.           0.           1.1870158   -0.6339009\n",
      "    0.           0.           0.          74.04493    140.0427\n",
      "  -50.883713    -6.681391  -144.07178  ]\n",
      "Intermediate Error: 0.009984395459916463\n",
      "[   0.           81.39999     -33.251705   -329.649         0.\n",
      "    0.            0.            0.            1.3903027    -0.75900024\n",
      "    0.            0.            0.           73.23325     148.70314\n",
      "  -72.62338      15.699207   -153.18877   ]\n",
      "Intermediate Error: 0.010009435838396889\n",
      "[   0.          74.99552    -21.53067   -336.0679       0.\n",
      "    0.           0.           0.           1.5943125   -0.8833717\n",
      "    0.           0.           0.          72.433304   157.35704\n",
      "  -94.378136    38.095314  -162.30884  ]\n",
      "Intermediate Error: 0.010038663848235464\n",
      "[   0.          68.59391     -9.813429  -342.48544      0.\n",
      "    0.           0.           0.           1.7990448   -1.0070167\n",
      "    0.           0.           0.          71.64491    166.00441\n",
      " -116.14787     60.507015  -171.43199  ]\n",
      "Intermediate Error: 0.010072068024069876\n",
      "______________________________\n",
      "Best Error: 0.009898626280366295\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 3\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[ 0.00000000e+00  5.63262863e+01  7.20592070e+00 -1.51039114e+01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.54198110e-01\n",
      "  0.00000000e+00  0.00000000e+00 -3.69214028e-01  0.00000000e+00\n",
      "  0.00000000e+00 -5.69917603e+01  1.06898384e+02 -2.85190296e+01\n",
      "  1.21615242e+02  1.83464325e+02]\n",
      "Intermediate Error: 0.010832903607044878\n",
      "[  0.          55.3739      18.493965   -23.802402     0.\n",
      "   0.           0.           0.           0.19880067   0.\n",
      "  -0.46780178   0.           0.         -58.99758    115.617485\n",
      " -51.185623   144.05344    173.86578   ]\n",
      "Intermediate Error: 0.010742476305629606\n",
      "[  0.          51.379677    28.108082   -30.105808     0.\n",
      "   0.           0.           0.           0.39551762   0.\n",
      "  -0.564972     0.           0.         -59.455196   125.78677\n",
      " -75.01417    165.18602    167.13542   ]\n",
      "Intermediate Error: 0.010657184039621523\n",
      "[ 0.0000000e+00  4.6600876e+01  3.7640442e+01 -3.5642536e+01\n",
      "  0.0000000e+00  7.1350060e-02  0.0000000e+00  0.0000000e+00\n",
      "  5.9347904e-01  0.0000000e+00 -6.6251373e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  1.3599612e+02 -9.9046417e+01\n",
      "  1.8606055e+02  1.6023671e+02]\n",
      "Intermediate Error: 0.06180835345700439\n",
      "[   0.          37.226967    57.62146    -46.146156     0.\n",
      "    0.           0.           0.           0.8103968    0.\n",
      "   -0.7838055    0.           0.         -25.564148   109.89021\n",
      " -131.27304    219.37221    156.79361  ]\n",
      "Intermediate Error: 0.011116536505311386\n",
      "[   0.          34.86715     66.56433    -54.400093     0.\n",
      "    0.           0.           0.           1.0090476    0.\n",
      "   -0.8965134    0.           0.         -24.486895   122.122765\n",
      " -153.5338     239.16559    147.87766  ]\n",
      "Intermediate Error: 0.010644332935003732\n",
      "[   0.          29.938828    76.37976    -59.917385     0.\n",
      "    0.           0.           0.           1.2091827    0.\n",
      "   -1.0109991    0.           0.         -25.044907   132.09456\n",
      " -177.5816     260.49817    141.09717  ]\n",
      "Intermediate Error: 0.010630029836225723\n",
      "[   0.          24.96467     86.03708    -65.4525       0.\n",
      "    0.           0.           0.           1.4101075    0.\n",
      "   -1.1265583    0.           0.         -25.168398   142.37643\n",
      " -201.47986    281.4958     133.89708  ]\n",
      "Intermediate Error: 0.010625022068837042\n",
      "[   0.          19.93825     95.724      -70.9445       0.\n",
      "    0.           0.           0.           1.6115497    0.\n",
      "   -1.2440325    0.           0.         -25.342379   152.62178\n",
      " -225.40668    302.56198    126.71307  ]\n",
      "Intermediate Error: 0.010620200786238819\n",
      "[   0.          14.899399   105.42057    -76.441185     0.\n",
      "    0.           0.           0.           1.8135167    0.\n",
      "   -1.363379     0.           0.         -25.524223   162.87308\n",
      " -249.32741    323.6565     119.49032  ]\n",
      "Intermediate Error: 0.010620175482823043\n",
      "______________________________\n",
      "Best Error: 0.010620175482823043\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 3\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 4\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[  0.         -78.41317    -30.846199    66.477455     0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.20102972  -0.09926432   0.          66.82045     69.224205\n",
      " -40.450855   -23.349787    29.669092  ]\n",
      "Intermediate Error: 0.010059291529489386\n",
      "[ 0.0000000e+00 -1.0229908e+02 -2.3000917e+01  8.0252327e+01\n",
      "  0.0000000e+00  0.0000000e+00 -5.7123914e-02  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  3.0277926e-01  0.0000000e+00\n",
      "  0.0000000e+00  7.5060562e+01  7.7408524e+01 -6.8676460e+01\n",
      " -8.6353512e+00  2.2044514e+01]\n",
      "Intermediate Error: 0.010148971635895534\n",
      "[   0.         -109.195145    -14.122014     76.95631       0.\n",
      "    0.           -0.14517543    0.            0.            0.\n",
      "    0.37269884    0.            0.           76.56607      87.23316\n",
      "  -91.101616     10.063404     14.347024  ]\n",
      "Intermediate Error: 0.010071251287858194\n",
      "[   0.         -114.410355     -4.771406     72.01783       0.\n",
      "    0.           -0.23770437    0.            0.            0.\n",
      "    0.436298      0.            0.           76.47441      96.506485\n",
      " -113.266914     29.917389      7.350223  ]\n",
      "Intermediate Error: 0.010014115992962664\n",
      "[   0.         -119.38796       4.6022263    66.81868       0.\n",
      "    0.           -0.33218482    0.            0.            0.\n",
      "    0.49766737    0.            0.           76.28283     105.8003\n",
      " -135.34152      49.848137      0.30352354]\n",
      "Intermediate Error: 0.009964056243397494\n",
      "[   0.         -124.352234     13.994887     61.59337       0.\n",
      "    0.           -0.42821825    0.            0.            0.\n",
      "    0.5571438     0.            0.           76.05446     115.08362\n",
      " -157.41313      69.82953      -6.7578516 ]\n",
      "Intermediate Error: 0.009917932876778838\n",
      "[   0.        -129.32385     23.398966    56.361526     0.\n",
      "    0.          -0.5257642    0.           0.           0.\n",
      "    0.6147946    0.           0.          75.813705   124.36648\n",
      " -179.48343     89.84375    -13.846846 ]\n",
      "Intermediate Error: 0.00987565565503516\n",
      "[   0.         -134.30612      32.814392     51.126835      0.\n",
      "    0.           -0.62481266    0.            0.            0.\n",
      "    0.67062813    0.            0.           75.56149     133.64815\n",
      " -201.55403     109.89053     -20.962475  ]\n",
      "Intermediate Error: 0.009837242371923849\n",
      "[   0.         -139.29922      42.240902     45.889587      0.\n",
      "    0.           -0.72535896    0.            0.            0.\n",
      "    0.72464895    0.            0.           75.29845     142.92877\n",
      " -223.62512     129.96947     -28.104761  ]\n",
      "Intermediate Error: 0.009802724242231557\n",
      "[   0.        -144.3031      51.678383    40.649883     0.\n",
      "    0.          -0.827399     0.           0.           0.\n",
      "    0.7768607    0.           0.          75.02478    152.20825\n",
      " -245.69691    150.08046    -35.27349  ]\n",
      "Intermediate Error: 0.009772166876737563\n",
      "______________________________\n",
      "Best Error: 0.009772166876737563\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 3\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[ 0.0000000e+00  1.0070864e+02 -9.7132540e+00 -5.3508080e+01\n",
      "  0.0000000e+00  1.1426105e-01 -1.3110979e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  5.2526295e+01  1.4893619e+02  3.2663979e+01\n",
      "  1.6654423e+01 -9.0032396e+00]\n",
      "Intermediate Error: 0.01260508964338522\n",
      "[ 0.0000000e+00  1.0144859e+02  1.1208305e+00 -6.2274651e+01\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  1.9186899e-01  0.0000000e+00\n",
      "  9.8739475e-02  4.4972855e+01  1.4960730e+02  6.2573719e+00\n",
      "  4.2298981e+01 -5.4335680e+00]\n",
      "Intermediate Error: 0.01200228002737365\n",
      "[  0.          99.52788      8.248518   -68.193214     0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.40109372   0.           0.19363868  48.62528    158.47585\n",
      " -17.553541    59.86011     -9.938784  ]\n",
      "Intermediate Error: 0.011818883630624956\n",
      "[  0.          96.51497     15.709498   -72.93913      0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.6071803    0.           0.28684056  51.688652   166.61066\n",
      " -42.043304    78.02934    -13.604175  ]\n",
      "Intermediate Error: 0.011628001273331216\n",
      "[  0.          93.436485    23.13802    -77.68264      0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.8103972    0.           0.37795845  54.883904   174.87604\n",
      " -66.451164    96.09803    -17.48391   ]\n",
      "Intermediate Error: 0.011447353372868502\n",
      "[  0.          90.30837     30.608273   -82.41721      0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   1.0104022    0.           0.46703875  58.01587    183.1392\n",
      " -90.840744   114.227684   -21.408178  ]\n",
      "Intermediate Error: 0.011272562793154884\n",
      "[ 0.0000000e+00  8.7148018e+01  3.8112484e+01 -8.7162903e+01\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -8.8024579e-02\n",
      "  0.0000000e+00  0.0000000e+00  1.2072263e+00  0.0000000e+00\n",
      "  0.0000000e+00  6.1100006e+01  1.9141597e+02 -1.1519891e+02\n",
      "  1.3240358e+02 -2.5396524e+01]\n",
      "Intermediate Error: 0.011144248629571866\n",
      "[ 0.0000000e+00  8.3934830e+01  4.5575668e+01 -9.1826035e+01\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00 -1.7648330e-01\n",
      "  0.0000000e+00  0.0000000e+00  1.4013089e+00  0.0000000e+00\n",
      "  0.0000000e+00  6.4358070e+01  1.9996008e+02 -1.3947975e+02\n",
      "  1.5051198e+02 -2.9510149e+01]\n",
      "Intermediate Error: 0.01095894163337375\n",
      "[   0.           80.67247      53.143776    -96.53514       0.\n",
      "    0.            0.           -0.26545158    0.            0.\n",
      "    1.5920957     0.            0.           67.37473     208.28171\n",
      " -163.79262     168.77509     -33.61189   ]\n",
      "Intermediate Error: 0.010805350567563413\n",
      "[   0.           77.39728      60.74052    -101.27443       0.\n",
      "    0.            0.           -0.35482535    0.            0.\n",
      "    1.7797595     0.            0.           70.35293     216.62843\n",
      " -188.06462     187.07617     -37.78838   ]\n",
      "Intermediate Error: 0.010658328714727874\n",
      "______________________________\n",
      "Best Error: 0.010658328714727874\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.         -235.42572    -148.1465       39.11265       0.\n",
      "    0.            0.            0.            0.            0.\n",
      "    0.5450984    -0.25327936    0.           89.25031      52.39535\n",
      "  -98.1065      -92.580574    163.60669   ]\n",
      "Intermediate Error: 0.028180365909559246\n",
      "[ 0.0000000e+00 -3.3576111e+02 -1.4171138e+02  1.3241394e+02\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  1.6892983e-01  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -2.2219296e-01  1.1739362e+02  5.6789768e+01 -1.5608983e+02\n",
      " -8.5387833e+01  1.6927435e+02]\n",
      "Intermediate Error: 0.013935775904246455\n",
      "[ 0.0000000e+00 -3.5690256e+02 -1.3427737e+02  1.4164735e+02\n",
      "  0.0000000e+00  0.0000000e+00 -2.3118418e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      " -4.6300244e-01  1.2598508e+02  7.1800568e+01 -1.8175664e+02\n",
      " -7.1862175e+01  1.5667435e+02]\n",
      "Intermediate Error: 0.013601068322593227\n",
      "[   0.         -365.8935     -124.02075     139.01265       0.\n",
      "    0.           -0.48424828    0.            0.            0.\n",
      "    0.            0.           -0.70556056  124.74276      83.846886\n",
      " -205.02351     -51.478867    147.73273   ]\n",
      "Intermediate Error: 0.013538440827370574\n",
      "[   0.         -372.50708    -113.456795    133.99445       0.\n",
      "    0.           -0.7422445     0.            0.            0.\n",
      "    0.            0.           -0.94891906  122.22779      95.75938\n",
      " -227.6035      -30.269032    138.92033   ]\n",
      "Intermediate Error: 0.013516027616117147\n",
      "[   0.        -378.70856   -102.825516   128.57236      0.\n",
      "    0.          -1.0017594    0.           0.           0.\n",
      "    0.           0.          -1.1927713  119.45507    107.61075\n",
      " -250.09688     -8.862263   130.1708   ]\n",
      "Intermediate Error: 0.013500217683798521\n",
      "[   0.        -384.83246    -92.18706    123.07792      0.\n",
      "    0.          -1.2622117    0.           0.           0.\n",
      "    0.           0.          -1.4370761  116.64875    119.44941\n",
      " -272.58713     12.591784   121.42576  ]\n",
      "Intermediate Error: 0.013487798964181187\n",
      "[   0.        -390.94025    -81.55077    117.572464     0.\n",
      "    0.          -1.5234936    0.           0.           0.\n",
      "    0.           0.          -1.6818271  113.84558    131.28119\n",
      " -295.09116     34.069256   112.67842  ]\n",
      "Intermediate Error: 0.01347816016484896\n",
      "[   0.        -397.04282    -70.91841    112.06682      0.\n",
      "    0.          -1.7855852    0.           0.           0.\n",
      "    0.           0.          -1.9270252  111.05239    143.10739\n",
      " -317.6118      55.56565    103.92726  ]\n",
      "Intermediate Error: 0.01347119684797691\n",
      "[   0.        -403.14218    -60.290253   106.56293      0.\n",
      "    0.          -2.0484817    0.           0.           0.\n",
      "    0.           0.          -2.1726725  108.270355   154.92824\n",
      " -340.1495      77.08017     95.172    ]\n",
      "Intermediate Error: 0.013466897499429642\n",
      "______________________________\n",
      "Best Error: 0.013466897499429642\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.         -126.91177     129.54031     263.39926       0.\n",
      "    0.            0.            0.            0.            0.\n",
      "    0.32671228    0.26630047    0.           -3.6367786   -75.35722\n",
      "  -85.7992       94.73591     -80.590775  ]\n",
      "Intermediate Error: 0.00955564468742671\n",
      "[ 0.0000000e+00 -1.5063242e+02  1.3416013e+02  2.8102890e+02\n",
      "  0.0000000e+00  0.0000000e+00 -2.1256213e-01  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  5.4532701e-01  0.0000000e+00\n",
      "  0.0000000e+00  1.9508663e+01 -5.4666374e+01 -1.1085814e+02\n",
      "  1.0124619e+02 -9.8379463e+01]\n",
      "Intermediate Error: 0.008589915694968997\n",
      "[   0.        -157.95264    143.28426    278.76443      0.\n",
      "    0.          -0.4573803    0.           0.           0.\n",
      "    0.7106501    0.           0.          21.166334   -45.867912\n",
      " -135.16725    120.52566   -104.11116  ]\n",
      "Intermediate Error: 0.008552597880261005\n",
      "[ 0.00000000e+00 -1.62812119e+02  1.52338806e+02  2.73841888e+02\n",
      "  0.00000000e+00  0.00000000e+00 -7.08297610e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.60121933e-01  2.23233032e+01 -3.65368958e+01 -1.58426605e+02\n",
      "  1.39924210e+02 -1.10377174e+02]\n",
      "Intermediate Error: 0.008527771867102608\n",
      "[   0.         -167.48909     161.44673     268.76138       0.\n",
      "    0.           -0.96132296    0.            0.            0.\n",
      "    0.            0.           -0.32178468   23.312845    -27.17427\n",
      " -181.64165     159.48631    -116.55828   ]\n",
      "Intermediate Error: 0.008562043675488299\n",
      "[   0.        -172.16629    170.59793    263.64462      0.\n",
      "    0.          -1.2161214    0.           0.           0.\n",
      "    0.           0.          -0.4848776   24.206936   -17.876266\n",
      " -204.85965    179.12833   -122.747795 ]\n",
      "Intermediate Error: 0.008585566794153943\n",
      "[   0.        -176.85364    179.76666    258.5151       0.\n",
      "    0.          -1.4726598    0.           0.           0.\n",
      "    0.           0.          -0.6494266   25.07954     -8.57012\n",
      " -228.064      198.80507   -128.97945  ]\n",
      "Intermediate Error: 0.008615766998110024\n",
      "[   0.        -181.55835    188.9545     253.38068      0.\n",
      "    0.          -1.7309191    0.           0.           0.\n",
      "    0.           0.          -0.8154228   25.927696     0.7387266\n",
      " -251.25975    218.5196    -135.24733  ]\n",
      "Intermediate Error: 0.008652261654721029\n",
      "[   0.         -186.28049     198.16075     248.24153       0.\n",
      "    0.           -1.9908924     0.            0.            0.\n",
      "    0.            0.           -0.98286015   26.753021     10.050987\n",
      " -274.44687     238.27078    -141.55208   ]\n",
      "Intermediate Error: 0.008695011905520774\n",
      "[   0.        -191.01999    207.38527    243.09789      0.\n",
      "    0.          -2.2525735    0.           0.           0.\n",
      "    0.           0.          -1.1517326   27.555809    19.366451\n",
      " -297.62573    258.0585    -147.8933   ]\n",
      "Intermediate Error: 0.008743850733099452\n",
      "______________________________\n",
      "Best Error: 0.008527771867102608\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 2\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 3\n",
      "Reconnect failed - remote exit again\n",
      "Check Ansys license server connection\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 5\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[  0.           7.7166853  -68.744194   -84.73019      0.\n",
      "   0.           0.           0.           0.          -0.2005303\n",
      "  -0.30173576   0.           0.         -11.000935   125.53522\n",
      " -24.93671     48.532043   193.19028   ]\n",
      "Intermediate Error: 0.017416870468183597\n",
      "[  0.         -11.242232   -53.718487   -75.692154     0.\n",
      "   0.           0.           0.           0.22516558   0.\n",
      "  -0.54198945   0.           0.         -16.622005   130.24109\n",
      " -55.0737      74.69945    187.51964   ]\n",
      "Intermediate Error: 0.02011284258711406\n",
      "[  0.         -17.668133   -43.284832   -81.618164     0.\n",
      "   0.           0.           0.           0.43900025   0.\n",
      "  -0.79143447   0.           0.         -18.1678     143.82614\n",
      " -77.920395    95.45191    177.23096   ]\n",
      "Intermediate Error: 0.020014977761851113\n",
      "[   0.         -24.06155    -32.172047   -87.37205      0.\n",
      "    0.           0.           0.           0.6525649    0.\n",
      "   -1.0447856    0.           0.         -21.441103   156.17543\n",
      " -101.38952    117.675766   168.43849  ]\n",
      "Intermediate Error: 0.019954455104180795\n",
      "[   0.          -30.303001    -21.126854    -93.303085      0.\n",
      "    0.            0.            0.            0.86684525    0.\n",
      "   -1.2988867     0.            0.          -24.568756    168.66306\n",
      " -124.74908     139.79356     159.44142   ]\n",
      "Intermediate Error: 0.019915456092566754\n",
      "[   0.         -36.54347    -10.07212    -99.22916      0.\n",
      "    0.           0.           0.           1.0818135    0.\n",
      "   -1.5538955    0.           0.         -27.720814   181.12207\n",
      " -148.13528    161.9648     150.46103  ]\n",
      "Intermediate Error: 0.019877033609572232\n",
      "[   0.         -42.780598     0.9787893 -105.15682      0.\n",
      "    0.           0.           0.           1.2974901    0.\n",
      "   -1.8097563    0.           0.         -30.862745   193.57721\n",
      " -171.53448    184.16035    141.46477  ]\n",
      "Intermediate Error: 0.01984114244979304\n",
      "[   0.         -49.017014    12.027312  -111.08288      0.\n",
      "    0.           0.           0.           1.5138751    0.\n",
      "   -2.0664759    0.           0.         -33.997757   206.02539\n",
      " -194.9492     206.38322    132.45679  ]\n",
      "Intermediate Error: 0.01980741467381657\n",
      "[   0.         -55.252663    23.073196  -117.007454     0.\n",
      "    0.           0.           0.           1.7309693    0.\n",
      "   -2.3240566    0.           0.         -37.12519    218.46709\n",
      " -218.3792     228.63286    123.43653  ]\n",
      "Intermediate Error: 0.019775844862417135\n",
      "[   0.         -61.48757     34.116478  -122.930504     0.\n",
      "    0.           0.           0.           1.9487731    0.\n",
      "   -2.582502     0.           0.         -40.245087   230.90224\n",
      " -241.82454    250.90929    114.40408  ]\n",
      "Intermediate Error: 0.019746504565054532\n",
      "______________________________\n",
      "Best Error: 0.017416870468183597\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Resetting the environment\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "[   0.            2.7612827   -36.665066    -55.104088      0.\n",
      "    0.            0.            0.            0.16640417    0.\n",
      "    0.16597046    0.            0.           94.00754      55.42959\n",
      "   21.32256     -79.78332    -114.006714  ]\n",
      "Intermediate Error: 0.009246345066535445\n",
      "[ 0.0000000e+00 -1.0484042e+01 -2.1946056e+01 -5.1537529e+01\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  2.6486802e-01 -9.4369330e-02  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  8.4605011e+01  5.5038254e+01 -6.7436256e+00\n",
      " -5.0500114e+01 -1.1148355e+02]\n",
      "Intermediate Error: 0.008943407723385477\n",
      "[ 0.00000000e+00 -1.58981113e+01 -1.22232428e+01 -5.77440414e+01\n",
      "  0.00000000e+00  0.00000000e+00 -9.70122516e-02  0.00000000e+00\n",
      "  3.65051866e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  8.58349609e+01  6.49113617e+01 -2.71634159e+01\n",
      " -3.10361176e+01 -1.21320526e+02]\n",
      "Intermediate Error: 0.008900773618047832\n",
      "[   0.          -21.74636      -1.6959848   -63.262707      0.\n",
      "    0.           -0.19462058    0.            0.46527964    0.\n",
      "    0.            0.            0.           85.14407      73.25828\n",
      "  -48.491745     -9.891375   -129.3243    ]\n",
      "Intermediate Error: 0.008918222209585654\n",
      "[   0.          -27.45679       8.737531    -68.96039       0.\n",
      "    0.           -0.29377323    0.            0.5663516     0.\n",
      "    0.            0.            0.           84.66951      81.80264\n",
      "  -69.68167      11.083664   -137.58897   ]\n",
      "Intermediate Error: 0.008934469334901256\n",
      "[   0.         -33.18017     19.190615   -74.6456       0.\n",
      "    0.          -0.3942107    0.           0.6682221    0.\n",
      "    0.           0.           0.          84.15919     90.3127\n",
      "  -90.89766     32.116676  -145.83385  ]\n",
      "Intermediate Error: 0.008956871837407567\n",
      "[   0.          -38.90566      29.646141    -80.33455       0.\n",
      "    0.           -0.49594855    0.            0.7709093     0.\n",
      "    0.            0.            0.           83.65447      98.821594\n",
      " -112.119675     53.172096   -154.09943   ]\n",
      "Intermediate Error: 0.008984035412371767\n",
      "[   0.         -44.635902    40.106262   -86.02378      0.\n",
      "    0.          -0.59898      0.           0.8744139    0.\n",
      "    0.           0.           0.          83.15051    107.32486\n",
      " -133.35088     74.25429   -162.38025  ]\n",
      "Intermediate Error: 0.009016069716196801\n",
      "[   0.          -50.37063      50.570606    -91.71363       0.\n",
      "    0.           -0.7033032     0.            0.97873807    0.\n",
      "    0.            0.            0.           82.64817     115.82312\n",
      " -154.59093      95.36253    -170.67702   ]\n",
      "Intermediate Error: 0.009052924438564418\n",
      "[   0.         -56.10985     61.03919    -97.40403      0.\n",
      "    0.          -0.8089159    0.           1.0838836    0.\n",
      "    0.           0.           0.          82.14743    124.31626\n",
      " -175.83998    116.49689   -178.98952  ]\n",
      "Intermediate Error: 0.009094565792584794\n",
      "______________________________\n",
      "Best Error: 0.008900773618047832\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.467\n",
      "Initial error (median) = 0.482\n",
      "Initial error (stdev) = 0.251\n",
      "Initial error (max) = 0.923\n",
      "**********************************************************\n",
      "Final error (mean) = 0.011\n",
      "Final error (median) = 0.011\n",
      "Final error (stdev) = 0.002\n",
      "Final error (max) = 0.017\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.038\n",
      "Max Deviation (median) = 0.037\n",
      "Max Deviation (stdev) = 0.006\n",
      "Max Deviation (max) = 0.048\n",
      "**********************************************************\n",
      "Max Force (mean) = 245.144\n",
      "Max Force (median) = 245.697\n",
      "Max Force (stdev) = 74.906\n",
      "Max Force (max) = 403.142\n",
      "**********************************************************\n",
      "Best Forces: [array([   0.      ,   58.268257,   63.138756, -217.82857 ,    0.      ,\n",
      "          0.      ,   -2.069962,    0.      ,    0.      ,    0.      ,\n",
      "          3.007563,    0.      ,    0.      ,   -5.835328,   35.122414,\n",
      "       -168.60017 ,  139.66315 , -175.5097  ], dtype=float32), array([   0.       ,   96.77325  ,  192.36885  ,  -80.698105 ,\n",
      "          0.       ,    0.       ,   -1.9060361,    0.       ,\n",
      "          0.       ,    0.       ,    2.0136228,    0.       ,\n",
      "          0.       , -138.6871   ,   69.584045 , -217.79356  ,\n",
      "        376.11255  ,  -17.555325 ], dtype=float32), array([   0.       , -214.26833  ,  239.34314  ,  101.56236  ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          2.3745396,    0.       ,    0.       ,    0.       ,\n",
      "         -2.1434405, -196.36137  , -187.42676  , -358.0968   ,\n",
      "        355.86356  ,   -2.0555906], dtype=float32), array([   0.       ,   67.78994  ,  287.70505  ,  102.29646  ,\n",
      "          0.       ,    0.       ,   -1.9281002,    0.       ,\n",
      "          0.       ,    0.       ,    1.2332512,    0.       ,\n",
      "          0.       ,  -96.17818  ,   66.75499  , -229.96548  ,\n",
      "        422.86044  ,  -77.173294 ], dtype=float32), array([   0.       ,   67.17633  ,   10.459385 , -336.2342   ,\n",
      "          0.       ,    0.       ,   -1.8920462,    0.       ,\n",
      "          0.       ,    0.       ,    2.5257   ,    0.       ,\n",
      "          0.       ,   49.295223 ,  -58.665882 , -157.88425  ,\n",
      "          6.8001404, -172.93175  ], dtype=float32), array([   0.       , -275.0844   ,   66.94913  ,  148.2257   ,\n",
      "          0.       ,    0.       ,   -1.5076737,    0.       ,\n",
      "          0.       ,    0.       ,    2.1629786,    0.       ,\n",
      "          0.       ,  158.54506  ,   67.20426  , -272.10767  ,\n",
      "         79.04579  , -195.76326  ], dtype=float32), array([   0.       ,  109.12638  ,  -51.98586  , -330.35202  ,\n",
      "          0.       ,    0.       ,   -2.2558072,    0.       ,\n",
      "          0.       ,    0.       ,    1.5726695,    0.       ,\n",
      "          0.       ,   11.602214 ,  227.19933  , -166.8325   ,\n",
      "        143.37222  ,   34.7224   ], dtype=float32), array([   0.       ,   15.0846   ,  216.33713  ,   37.54551  ,\n",
      "          0.       ,    0.       ,   -1.5575836,    0.       ,\n",
      "          2.0449548,    0.       ,    0.       ,    0.       ,\n",
      "          0.       ,  -20.584995 ,   92.44569  , -217.29622  ,\n",
      "        319.58377  , -160.42937  ], dtype=float32), array([   0.       ,   52.87077  ,  -16.124563 , -259.05408  ,\n",
      "          0.       ,    0.9195087,   -1.3370018,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       ,  -42.210556 ,  140.35426  , -212.39561  ,\n",
      "        180.93854  ,   71.785255 ], dtype=float32), array([   0.       ,  -33.314793 ,  166.8671   ,  -85.89181  ,\n",
      "          0.       ,    0.       ,   -1.6088521,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "         -1.9473919,  -33.46012  ,  -32.142616 , -212.11757  ,\n",
      "        196.08884  , -203.60976  ], dtype=float32), array([   0.        , -426.09143   ,  284.88098   ,  275.3586    ,\n",
      "          0.        ,    0.        ,   -0.5653441 ,    0.        ,\n",
      "          0.56491935,    0.        ,    0.        ,    0.        ,\n",
      "          0.        ,  164.89519   ,  113.4861    , -373.67667   ,\n",
      "        293.51575   , -164.36163   ], dtype=float32), array([   0.       ,   68.59391  ,   -9.813429 , -342.48544  ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          1.7990448,   -1.0070167,    0.       ,    0.       ,\n",
      "          0.       ,   71.64491  ,  166.00441  , -116.14787  ,\n",
      "         60.507015 , -171.43199  ], dtype=float32), array([   0.       ,   14.899399 ,  105.42057  ,  -76.441185 ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          1.8135167,    0.       ,   -1.363379 ,    0.       ,\n",
      "          0.       ,  -25.524223 ,  162.87308  , -249.32741  ,\n",
      "        323.6565   ,  119.49032  ], dtype=float32), array([   0.       , -144.3031   ,   51.678383 ,   40.649883 ,\n",
      "          0.       ,    0.       ,   -0.827399 ,    0.       ,\n",
      "          0.       ,    0.       ,    0.7768607,    0.       ,\n",
      "          0.       ,   75.02478  ,  152.20825  , -245.69691  ,\n",
      "        150.08046  ,  -35.27349  ], dtype=float32), array([   0.        ,   77.39728   ,   60.74052   , -101.27443   ,\n",
      "          0.        ,    0.        ,    0.        ,   -0.35482535,\n",
      "          0.        ,    0.        ,    1.7797595 ,    0.        ,\n",
      "          0.        ,   70.35293   ,  216.62843   , -188.06462   ,\n",
      "        187.07617   ,  -37.78838   ], dtype=float32), array([   0.       , -403.14218  ,  -60.290253 ,  106.56293  ,\n",
      "          0.       ,    0.       ,   -2.0484817,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "         -2.1726725,  108.270355 ,  154.92824  , -340.1495   ,\n",
      "         77.08017  ,   95.172    ], dtype=float32), array([   0.       , -191.01999  ,  207.38527  ,  243.09789  ,\n",
      "          0.       ,    0.       ,   -2.2525735,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "         -1.1517326,   27.555809 ,   19.366451 , -297.62573  ,\n",
      "        258.0585   , -147.8933   ], dtype=float32), array([   0.       ,  -61.48757  ,   34.116478 , -122.930504 ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          1.9487731,    0.       ,   -2.582502 ,    0.       ,\n",
      "          0.       ,  -40.245087 ,  230.90224  , -241.82454  ,\n",
      "        250.90929  ,  114.40408  ], dtype=float32), array([   0.       ,  -56.10985  ,   61.03919  ,  -97.40403  ,\n",
      "          0.       ,    0.       ,   -0.8089159,    0.       ,\n",
      "          1.0838836,    0.       ,    0.       ,    0.       ,\n",
      "          0.       ,   82.14743  ,  124.31626  , -175.83998  ,\n",
      "        116.49689  , -178.98952  ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "    #env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[idx[-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd#.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action_mean, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#     [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "# )\n",
    "\n",
    "envs = make_env(env_name, 0, 0, 10, file1, file2, False)\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230202_201959-1rw7aiak/files/agent_16383936steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "bestForces = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files[:-1]:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    # env_name = \"FuselageActuators-v22\"\n",
    "    # envs = gym.vector.SyncVectorEnv(\n",
    "    #     [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    # )\n",
    "\n",
    "    envs = make_env(env_name, 0, 0, 10, file1, file2, False)\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    # initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(10):\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "            bestForce = info[\"Forces\"]\n",
    "    print('_'*30)\n",
    "    print(\"Best Error:\", minError)\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    bestForces.append(bestForce)\n",
    "\n",
    "    envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Best Forces:\", bestForces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 11, 12, 11, 11, 11, 11, 11, 11, 12, 11, 13, 13, 11, 13, 13, 12,\n",
       "       11, 12], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(bestForces, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial error (mean) = 0.467\n",
    "Initial error (median) = 0.482\n",
    "Initial error (stdev) = 0.251\n",
    "Initial error (max) = 0.923\n",
    "**********************************************************\n",
    "Final error (mean) = 0.011\n",
    "Final error (median) = 0.011\n",
    "Final error (stdev) = 0.002\n",
    "Final error (max) = 0.017\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.039\n",
    "Max Deviation (median) = 0.038\n",
    "Max Deviation (stdev) = 0.006\n",
    "Max Deviation (max) = 0.047\n",
    "**********************************************************\n",
    "Max Force (mean) = 41.837\n",
    "Max Force (median) = 0.370\n",
    "Max Force (stdev) = 68.257\n",
    "Max Force (max) = 356.915"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "______________________________\n",
      "Initial Error 1.409499695489975\n",
      "Best Error: 0.022528682478405358\n",
      "****************************** Episode: 2 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6146374866299307\n",
      "Best Error: 0.014846525888562013\n",
      "****************************** Episode: 3 ******************************\n",
      "______________________________\n",
      "Initial Error 0.04961305486189449\n",
      "Best Error: 0.012661854768269967\n",
      "****************************** Episode: 4 ******************************\n",
      "______________________________\n",
      "Initial Error 1.810665534719634\n",
      "Best Error: 0.01796157227331766\n",
      "****************************** Episode: 5 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6504018599821338\n",
      "Best Error: 0.022893587444894132\n",
      "****************************** Episode: 6 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5765460732187481\n",
      "Best Error: 0.009272740621735949\n",
      "****************************** Episode: 7 ******************************\n",
      "______________________________\n",
      "Initial Error 0.0310229910135403\n",
      "Best Error: 0.011453469134700842\n",
      "****************************** Episode: 8 ******************************\n",
      "______________________________\n",
      "Initial Error 0.8851902575144411\n",
      "Best Error: 0.010999484825976267\n",
      "****************************** Episode: 9 ******************************\n",
      "______________________________\n",
      "Initial Error 1.1785794543235406\n",
      "Best Error: 0.010265062280533333\n",
      "****************************** Episode: 10 ******************************\n",
      "______________________________\n",
      "Initial Error 0.47012663098201396\n",
      "Best Error: 0.014543505590169469\n",
      "****************************** Episode: 11 ******************************\n",
      "______________________________\n",
      "Initial Error 0.23515675709564737\n",
      "Best Error: 0.008161342527414349\n",
      "****************************** Episode: 12 ******************************\n",
      "______________________________\n",
      "Initial Error 1.4643245537982066\n",
      "Best Error: 0.01983623609511105\n",
      "****************************** Episode: 13 ******************************\n",
      "______________________________\n",
      "Initial Error 1.9647975520981928\n",
      "Best Error: 0.021050775378745806\n",
      "****************************** Episode: 14 ******************************\n",
      "______________________________\n",
      "Initial Error 0.34174046890942406\n",
      "Best Error: 0.021075757649843204\n",
      "****************************** Episode: 15 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5442926220736876\n",
      "Best Error: 0.014755133292531777\n",
      "****************************** Episode: 16 ******************************\n",
      "______________________________\n",
      "Initial Error 0.7504094130556273\n",
      "Best Error: 0.013169742835690241\n",
      "****************************** Episode: 17 ******************************\n",
      "______________________________\n",
      "Initial Error 1.6421327540018473\n",
      "Best Error: 0.018924096119592147\n",
      "****************************** Episode: 18 ******************************\n",
      "______________________________\n",
      "Initial Error 1.5278591800526384\n",
      "Best Error: 0.01966689250382582\n",
      "****************************** Episode: 19 ******************************\n",
      "______________________________\n",
      "Initial Error 1.2821465349677241\n",
      "Best Error: 0.02075744421310428\n",
      "****************************** Episode: 20 ******************************\n",
      "______________________________\n",
      "Initial Error 0.9447617415563114\n",
      "Best Error: 0.0152671126311091\n",
      "****************************** Episode: 21 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6588850291808339\n",
      "Best Error: 0.015227618699823602\n",
      "****************************** Episode: 22 ******************************\n",
      "______________________________\n",
      "Initial Error 1.7497850696102792\n",
      "Best Error: 0.019916799308755912\n",
      "****************************** Episode: 23 ******************************\n",
      "______________________________\n",
      "Initial Error 0.8111408322954649\n",
      "Best Error: 0.011834379912099251\n",
      "****************************** Episode: 24 ******************************\n",
      "______________________________\n",
      "Initial Error 0.21510371403658501\n",
      "Best Error: 0.012354376145658026\n",
      "****************************** Episode: 25 ******************************\n",
      "______________________________\n",
      "Initial Error 1.7205942499523443\n",
      "Best Error: 0.020375290433605614\n",
      "****************************** Episode: 26 ******************************\n",
      "______________________________\n",
      "Initial Error 1.780476309043343\n",
      "Best Error: 0.015894441079305027\n",
      "****************************** Episode: 27 ******************************\n",
      "______________________________\n",
      "Initial Error 0.17623476289724446\n",
      "Best Error: 0.02287479071675315\n",
      "****************************** Episode: 28 ******************************\n",
      "______________________________\n",
      "Initial Error 1.4301816727704115\n",
      "Best Error: 0.02418137264815346\n",
      "****************************** Episode: 29 ******************************\n",
      "______________________________\n",
      "Initial Error 1.3073534476138637\n",
      "Best Error: 0.019276626121770227\n",
      "****************************** Episode: 30 ******************************\n",
      "______________________________\n",
      "Initial Error 1.1245841188499897\n",
      "Best Error: 0.017254638566295492\n",
      "****************************** Episode: 31 ******************************\n",
      "______________________________\n",
      "Initial Error 0.34231296450786003\n",
      "Best Error: 0.011684663412265832\n",
      "****************************** Episode: 32 ******************************\n",
      "______________________________\n",
      "Initial Error 0.04961305486189449\n",
      "Best Error: 0.012661854768269967\n",
      "****************************** Episode: 33 ******************************\n",
      "______________________________\n",
      "Initial Error 1.4643245537982066\n",
      "Best Error: 0.018830493925220806\n",
      "****************************** Episode: 34 ******************************\n",
      "______________________________\n",
      "Initial Error 1.4641331092704981\n",
      "Best Error: 0.01838837414092607\n",
      "****************************** Episode: 35 ******************************\n",
      "______________________________\n",
      "Initial Error 1.3986750651833015\n",
      "Best Error: 0.022021354969215902\n",
      "****************************** Episode: 36 ******************************\n",
      "______________________________\n",
      "Initial Error 1.7689859929045995\n",
      "Best Error: 0.02544926722624171\n",
      "****************************** Episode: 37 ******************************\n",
      "______________________________\n",
      "Initial Error 0.9024698853577253\n",
      "Best Error: 0.017425489326950823\n",
      "****************************** Episode: 38 ******************************\n",
      "______________________________\n",
      "Initial Error 0.9538359409203798\n",
      "Best Error: 0.01310042715801757\n",
      "****************************** Episode: 39 ******************************\n",
      "______________________________\n",
      "Initial Error 0.21923194294885134\n",
      "Best Error: 0.013381679366707018\n",
      "****************************** Episode: 40 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6384381848679985\n",
      "Best Error: 0.015257091824267882\n",
      "****************************** Episode: 41 ******************************\n",
      "______________________________\n",
      "Initial Error 1.3728037017906651\n",
      "Best Error: 0.02059205108345582\n",
      "****************************** Episode: 42 ******************************\n",
      "______________________________\n",
      "Initial Error 2.000423747556299\n",
      "Best Error: 0.024843969097469266\n",
      "****************************** Episode: 43 ******************************\n",
      "______________________________\n",
      "Initial Error 1.8658637177725932\n",
      "Best Error: 0.018615230972076735\n",
      "****************************** Episode: 44 ******************************\n",
      "______________________________\n",
      "Initial Error 0.826409763734319\n",
      "Best Error: 0.007331532049789495\n",
      "****************************** Episode: 45 ******************************\n",
      "______________________________\n",
      "Initial Error 1.2611483856704198\n",
      "Best Error: 0.013072222196272925\n",
      "****************************** Episode: 46 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6250970472552699\n",
      "Best Error: 0.013773839690024258\n",
      "****************************** Episode: 47 ******************************\n",
      "______________________________\n",
      "Initial Error 0.41105223874094016\n",
      "Best Error: 0.016566258142343644\n",
      "****************************** Episode: 48 ******************************\n",
      "______________________________\n",
      "Initial Error 0.21627479597610114\n",
      "Best Error: 0.015201091788337567\n",
      "****************************** Episode: 49 ******************************\n",
      "______________________________\n",
      "Initial Error 0.3902280735364395\n",
      "Best Error: 0.017430719170042422\n",
      "****************************** Episode: 50 ******************************\n",
      "______________________________\n",
      "Initial Error 0.3329405958087078\n",
      "Best Error: 0.02299222228100995\n",
      "****************************** Episode: 51 ******************************\n",
      "______________________________\n",
      "Initial Error 1.5596830548755378\n",
      "Best Error: 0.014385866729865058\n",
      "****************************** Episode: 52 ******************************\n",
      "______________________________\n",
      "Initial Error 2.078271290671967\n",
      "Best Error: 0.023809188848100214\n",
      "****************************** Episode: 53 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5765460732187481\n",
      "Best Error: 0.009281043911271951\n",
      "****************************** Episode: 54 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6600547087414697\n",
      "Best Error: 0.01008057054054268\n",
      "****************************** Episode: 55 ******************************\n",
      "______________________________\n",
      "Initial Error 0.15388459308341076\n",
      "Best Error: 0.008798478471615044\n",
      "****************************** Episode: 56 ******************************\n",
      "______________________________\n",
      "Initial Error 0.48660116730409575\n",
      "Best Error: 0.019196150461264314\n",
      "****************************** Episode: 57 ******************************\n",
      "______________________________\n",
      "Initial Error 0.34231296450786003\n",
      "Best Error: 0.011684663412265832\n",
      "****************************** Episode: 58 ******************************\n",
      "______________________________\n",
      "Initial Error 0.20974462459991602\n",
      "Best Error: 0.009326488003164102\n",
      "****************************** Episode: 59 ******************************\n",
      "______________________________\n",
      "Initial Error 0.4214894114881455\n",
      "Best Error: 0.01187386530024062\n",
      "****************************** Episode: 60 ******************************\n",
      "______________________________\n",
      "Initial Error 2.2894557229214616\n",
      "Best Error: 0.019503436428415514\n",
      "****************************** Episode: 61 ******************************\n",
      "______________________________\n",
      "Initial Error 1.6799371228720827\n",
      "Best Error: 0.019254040030237292\n",
      "****************************** Episode: 62 ******************************\n",
      "______________________________\n",
      "Initial Error 1.0194768201777091\n",
      "Best Error: 0.01263028591155481\n",
      "****************************** Episode: 63 ******************************\n",
      "______________________________\n",
      "Initial Error 0.24071843759689715\n",
      "Best Error: 0.01109318868559832\n",
      "****************************** Episode: 64 ******************************\n",
      "______________________________\n",
      "Initial Error 1.7533347922192493\n",
      "Best Error: 0.017119669333073826\n",
      "****************************** Episode: 65 ******************************\n",
      "______________________________\n",
      "Initial Error 2.665461317858638\n",
      "Best Error: 0.026623817611029894\n",
      "****************************** Episode: 66 ******************************\n",
      "______________________________\n",
      "Initial Error 0.2476104918840943\n",
      "Best Error: 0.00891324933363066\n",
      "****************************** Episode: 67 ******************************\n",
      "______________________________\n",
      "Initial Error 0.7172763273216816\n",
      "Best Error: 0.023667593435280014\n",
      "****************************** Episode: 68 ******************************\n",
      "______________________________\n",
      "Initial Error 0.3074430839204945\n",
      "Best Error: 0.012725408834760869\n",
      "****************************** Episode: 69 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5441998143380157\n",
      "Best Error: 0.019379319524762274\n",
      "****************************** Episode: 70 ******************************\n",
      "______________________________\n",
      "Initial Error 0.40267075047646256\n",
      "Best Error: 0.005470007540560009\n",
      "****************************** Episode: 71 ******************************\n",
      "______________________________\n",
      "Initial Error 0.8525455811667744\n",
      "Best Error: 0.013167336180523207\n",
      "****************************** Episode: 72 ******************************\n",
      "______________________________\n",
      "Initial Error 0.46490403683954135\n",
      "Best Error: 0.017053162570962378\n",
      "****************************** Episode: 73 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5191011930887864\n",
      "Best Error: 0.014146768516379086\n",
      "****************************** Episode: 74 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6274649366290295\n",
      "Best Error: 0.022632225103247614\n",
      "****************************** Episode: 75 ******************************\n",
      "______________________________\n",
      "Initial Error 0.39365923001652037\n",
      "Best Error: 0.013928295925349634\n",
      "****************************** Episode: 76 ******************************\n",
      "______________________________\n",
      "Initial Error 1.0520991162158326\n",
      "Best Error: 0.012882551697606593\n",
      "****************************** Episode: 77 ******************************\n",
      "______________________________\n",
      "Initial Error 0.12207030514341408\n",
      "Best Error: 0.013803789466868326\n",
      "****************************** Episode: 78 ******************************\n",
      "______________________________\n",
      "Initial Error 0.826409763734319\n",
      "Best Error: 0.007331532049789495\n",
      "****************************** Episode: 79 ******************************\n",
      "______________________________\n",
      "Initial Error 1.061875677376209\n",
      "Best Error: 0.01663889988434717\n",
      "****************************** Episode: 80 ******************************\n",
      "______________________________\n",
      "Initial Error 0.8712446102988347\n",
      "Best Error: 0.014231924318393532\n",
      "****************************** Episode: 81 ******************************\n",
      "______________________________\n",
      "Initial Error 0.13062750955769664\n",
      "Best Error: 0.011654481860015074\n",
      "****************************** Episode: 82 ******************************\n",
      "______________________________\n",
      "Initial Error 2.016544118699111\n",
      "Best Error: 0.021916358319723654\n",
      "****************************** Episode: 83 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5396619475381735\n",
      "Best Error: 0.015938416053161302\n",
      "****************************** Episode: 84 ******************************\n",
      "______________________________\n",
      "Initial Error 1.2047923267135479\n",
      "Best Error: 0.027322017360083044\n",
      "****************************** Episode: 85 ******************************\n",
      "______________________________\n",
      "Initial Error 1.3197116811132676\n",
      "Best Error: 0.0176714036392881\n",
      "****************************** Episode: 86 ******************************\n",
      "______________________________\n",
      "Initial Error 1.376115709912784\n",
      "Best Error: 0.019454163598451014\n",
      "****************************** Episode: 87 ******************************\n",
      "______________________________\n",
      "Initial Error 1.058671239372216\n",
      "Best Error: 0.013466719679096371\n",
      "****************************** Episode: 88 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5318931352921025\n",
      "Best Error: 0.009167882883437908\n",
      "****************************** Episode: 89 ******************************\n",
      "______________________________\n",
      "Initial Error 0.31228556196447366\n",
      "Best Error: 0.012807908665885387\n",
      "****************************** Episode: 90 ******************************\n",
      "______________________________\n",
      "Initial Error 0.34897825492685375\n",
      "Best Error: 0.017580071586425782\n",
      "****************************** Episode: 91 ******************************\n",
      "______________________________\n",
      "Initial Error 0.5363103240131607\n",
      "Best Error: 0.013506579342006391\n",
      "****************************** Episode: 92 ******************************\n",
      "______________________________\n",
      "Initial Error 0.6504018599821338\n",
      "Best Error: 0.022780518795168035\n",
      "****************************** Episode: 93 ******************************\n",
      "______________________________\n",
      "Initial Error 1.254750888503533\n",
      "Best Error: 0.01739472428910316\n",
      "****************************** Episode: 94 ******************************\n",
      "______________________________\n",
      "Initial Error 0.8712446102988347\n",
      "Best Error: 0.014231924318393532\n",
      "****************************** Episode: 95 ******************************\n",
      "______________________________\n",
      "Initial Error 0.9071610750577889\n",
      "Best Error: 0.014641723807450767\n",
      "****************************** Episode: 96 ******************************\n",
      "______________________________\n",
      "Initial Error 0.9447617415563114\n",
      "Best Error: 0.0152671126311091\n",
      "****************************** Episode: 97 ******************************\n",
      "______________________________\n",
      "Initial Error 0.7729738425086539\n",
      "Best Error: 0.01852648826909442\n",
      "****************************** Episode: 98 ******************************\n",
      "______________________________\n",
      "Initial Error 2.0152935654092206\n",
      "Best Error: 0.022138330863339394\n",
      "****************************** Episode: 99 ******************************\n",
      "______________________________\n",
      "Initial Error 1.4196624994113178\n",
      "Best Error: 0.02086419605904991\n",
      "****************************** Episode: 100 ******************************\n",
      "______________________________\n",
      "Initial Error 0.2523233388020709\n",
      "Best Error: 0.01149484097615141\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.915\n",
      "Initial error (median) = 0.819\n",
      "Initial error (stdev) = 0.599\n",
      "Initial error (max) = 2.665\n",
      "**********************************************************\n",
      "Final error (mean) = 0.016\n",
      "Final error (median) = 0.015\n",
      "Final error (stdev) = 0.005\n",
      "Final error (max) = 0.027\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.055\n",
      "Max Deviation (median) = 0.052\n",
      "Max Deviation (stdev) = 0.018\n",
      "Max Deviation (max) = 0.098\n",
      "**********************************************************\n",
      "Max Force (mean) = 424.104\n",
      "Max Force (median) = 348.344\n",
      "Max Force (stdev) = 236.795\n",
      "Max Force (max) = 1220.125\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.187\n",
      "Episode Rewards (median) = 0.298\n",
      "Episode Rewards (stdev) = 0.490\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions):\n",
    "    env = gym.make(env_id, n_actuators=n_actions, mode=\"Surrogate\", seed=seed, port=50056+idx)\n",
    "    #env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[idx[-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd#.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action_mean, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "run_name = \"dummy\"\n",
    "envs = make_env(env_name, 0 , 0, 10)\n",
    "\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230202_201959-1rw7aiak/files/agent_16383936steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(100):\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        # print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "    print('_'*30)\n",
    "    print(\"Initial Error\", info[\"initError\"])\n",
    "    print(\"Best Error:\", minError)\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial error (mean) = 0.915\n",
    "Initial error (median) = 0.819\n",
    "Initial error (stdev) = 0.599\n",
    "Initial error (max) = 2.665\n",
    "**********************************************************\n",
    "Final error (mean) = 0.016\n",
    "Final error (median) = 0.015\n",
    "Final error (stdev) = 0.005\n",
    "Final error (max) = 0.027\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.055\n",
    "Max Deviation (median) = 0.052\n",
    "Max Deviation (stdev) = 0.018\n",
    "Max Deviation (max) = 0.098\n",
    "**********************************************************\n",
    "Max Force (mean) = 424.104\n",
    "Max Force (median) = 348.344\n",
    "Max Force (stdev) = 236.795\n",
    "Max Force (max) = 1220.125\n",
    "**********************************************************\n",
    "Episode Rewards (mean) = 0.187\n",
    "Episode Rewards (median) = 0.298\n",
    "Episode Rewards (stdev) = 0.490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "Initial error (mean) = 0.936\n",
      "Initial error (median) = 0.907\n",
      "Initial error (stdev) = 0.622\n",
      "Initial error (max) = 2.665\n",
      "**********************************************************\n",
      "Final error (mean) = 0.018\n",
      "Final error (median) = 0.016\n",
      "Final error (stdev) = 0.007\n",
      "Final error (max) = 0.062\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.056\n",
      "Max Deviation (median) = 0.053\n",
      "Max Deviation (stdev) = 0.020\n",
      "Max Deviation (max) = 0.157\n",
      "**********************************************************\n",
      "Max Force (mean) = 61.372\n",
      "Max Force (median) = 0.376\n",
      "Max Force (stdev) = 98.788\n",
      "Max Force (max) = 587.291\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.962\n",
      "Episode Rewards (median) = 0.983\n",
      "Episode Rewards (stdev) = 0.051\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action_mean, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, \"Surrogate\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230202_201959-1rw7aiak/files/agent_16383936steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    # print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(1):\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        # print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "    # print('_'*30)\n",
    "    # print(\"Episodic Reward:\", episodeReward)\n",
    "    # print(\"Final Error:\", min(errors))\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run di4gu1ls (rpo on v22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: [0.79062737]\n",
      "Final Error: [0.06260109]\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.60651544]\n",
      "Final Error: [0.06253247]\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.8572601]\n",
      "Final Error: [0.08033405]\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Reward: [0.74844222]\n",
      "Final Error: [0.14498156]\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py:468\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    467\u001b[0m n_cpu \u001b[39m=\u001b[39m psutil\u001b[39m.\u001b[39mcpu_count(logical\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 468\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m    469\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1266\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[39m# pass\u001b[39;00m\n\u001b[1;32m-> 1266\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m   1268\u001b[0m \u001b[39mreturn\u001b[39;00m mapdl\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1248\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m port, actual_run_location \u001b[39m=\u001b[39m launch_grpc(\n\u001b[0;32m   1246\u001b[0m     port\u001b[39m=\u001b[39mport, verbose\u001b[39m=\u001b[39mverbose_mapdl, ip\u001b[39m=\u001b[39mip, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm\n\u001b[0;32m   1247\u001b[0m )\n\u001b[1;32m-> 1248\u001b[0m mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n\u001b[0;32m   1249\u001b[0m     ip\u001b[39m=\u001b[39mip,\n\u001b[0;32m   1250\u001b[0m     port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1251\u001b[0m     cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n\u001b[0;32m   1252\u001b[0m     loglevel\u001b[39m=\u001b[39mloglevel,\n\u001b[0;32m   1253\u001b[0m     set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n\u001b[0;32m   1254\u001b[0m     remove_temp_files\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mremove_temp_files\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m   1255\u001b[0m     log_apdl\u001b[39m=\u001b[39mlog_apdl,\n\u001b[0;32m   1256\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1257\u001b[0m )\n\u001b[0;32m   1258\u001b[0m \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:320\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, print_com, channel, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39m# connect and validate to the channel\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect()\n\u001b[0;32m    322\u001b[0m \u001b[39m# double check we have access to the local path if not\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m# explicitly specified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:383\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m connected:\n\u001b[0;32m    382\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[1;32m--> 383\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to connect to MAPDL gRPC instance at \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MapdlGrpc' object has no attribute '_target_str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X53sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFile: \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X53sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39m# Perform test and track error\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X53sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m obs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(envs\u001b[39m.\u001b[39;49mreset())\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X53sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m#initErrors.append(envs.error_initial)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#X53sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m episodeReward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py:120\u001b[0m, in \u001b[0;36mVectorEnv.reset\u001b[1;34m(self, seed, return_info, options)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39m\"\"\"Reset all parallel environments and return a batch of initial observations.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m    A batch of observations from the vectorized environment.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_async(seed\u001b[39m=\u001b[39mseed, return_info\u001b[39m=\u001b[39mreturn_info, options\u001b[39m=\u001b[39moptions)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_wait(seed\u001b[39m=\u001b[39;49mseed, return_info\u001b[39m=\u001b[39;49mreturn_info, options\u001b[39m=\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py:130\u001b[0m, in \u001b[0;36mSyncVectorEnv.reset_wait\u001b[1;34m(self, seed, return_info, options)\u001b[0m\n\u001b[0;32m    127\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_info\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m return_info\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_info:\n\u001b[1;32m--> 130\u001b[0m     observation \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    131\u001b[0m     observations\u001b[39m.\u001b[39mappend(observation)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\record_episode_statistics.py:100\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     99\u001b[0m     \u001b[39m\"\"\"Resets the environment using kwargs and resets the episode returns and lengths.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     observations \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_returns \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_lengths \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:427\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[ObsType, Tuple[ObsType, \u001b[39mdict\u001b[39m]]:\n\u001b[0;32m    426\u001b[0m     \u001b[39m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\time_limit.py:83\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:42\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:427\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[ObsType, Tuple[ObsType, \u001b[39mdict\u001b[39m]]:\n\u001b[0;32m    426\u001b[0m     \u001b[39m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 427\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\env_checker.py:45\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_reset \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:200\u001b[0m, in \u001b[0;36menv_reset_passive_checker\u001b[1;34m(env, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    196\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFuture gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[39m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn_info\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    203\u001b[0m         result, \u001b[39mtuple\u001b[39m\n\u001b[0;32m    204\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe result returned by `env.reset(return_info=True)` was not a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py:161\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSurrogate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_process(\u001b[39m'\u001b[39m\u001b[39mansys\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 161\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch_ansys()  \n\u001b[0;32m    163\u001b[0m \u001b[39m# Set forces to zero\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforces \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m18\u001b[39m, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32) \n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py:473\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    472\u001b[0m     n_cpu\u001b[39m=\u001b[39m\u001b[39mmin\u001b[39m(\u001b[39m4\u001b[39m, n_cpu) \u001b[39m#license sometimes won't let me use more than 4 processors?\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, port\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m    474\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n\u001b[0;32m    475\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning on\u001b[39m\u001b[39m\"\u001b[39m, n_cpu, \u001b[39m\"\u001b[39m\u001b[39mprocessors\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1248\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, verbose_mapdl, license_server_check, license_type, print_com, **kwargs)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgrpc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1245\u001b[0m     port, actual_run_location \u001b[39m=\u001b[39m launch_grpc(\n\u001b[0;32m   1246\u001b[0m         port\u001b[39m=\u001b[39mport, verbose\u001b[39m=\u001b[39mverbose_mapdl, ip\u001b[39m=\u001b[39mip, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm\n\u001b[0;32m   1247\u001b[0m     )\n\u001b[1;32m-> 1248\u001b[0m     mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n\u001b[0;32m   1249\u001b[0m         ip\u001b[39m=\u001b[39mip,\n\u001b[0;32m   1250\u001b[0m         port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1251\u001b[0m         cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n\u001b[0;32m   1252\u001b[0m         loglevel\u001b[39m=\u001b[39mloglevel,\n\u001b[0;32m   1253\u001b[0m         set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n\u001b[0;32m   1254\u001b[0m         remove_temp_files\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mremove_temp_files\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m   1255\u001b[0m         log_apdl\u001b[39m=\u001b[39mlog_apdl,\n\u001b[0;32m   1256\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1257\u001b[0m     )\n\u001b[0;32m   1258\u001b[0m     \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1259\u001b[0m         mapdl\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m actual_run_location\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:320\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, print_com, channel, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_channel \u001b[39m=\u001b[39m channel\n\u001b[0;32m    319\u001b[0m \u001b[39m# connect and validate to the channel\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect()\n\u001b[0;32m    322\u001b[0m \u001b[39m# double check we have access to the local path if not\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39m# explicitly specified\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:367\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39mwhile\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m<\u001b[39m max_time \u001b[39mand\u001b[39;00m i \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_attempts:\n\u001b[0;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mConnection attempt \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 367\u001b[0m     connected \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(\n\u001b[0;32m    368\u001b[0m         timeout\u001b[39m=\u001b[39;49mattempt_timeout, set_no_abort\u001b[39m=\u001b[39;49mset_no_abort\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    370\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    371\u001b[0m     \u001b[39mif\u001b[39;00m connected:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:467\u001b[0m, in \u001b[0;36mMapdlGrpc._connect\u001b[1;34m(self, timeout, set_no_abort, enable_health_check)\u001b[0m\n\u001b[0;32m    465\u001b[0m tstart \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    466\u001b[0m \u001b[39mwhile\u001b[39;00m ((time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m tstart) \u001b[39m<\u001b[39m timeout) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39m_matured:\n\u001b[1;32m--> 467\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39m_matured:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        idx = torch.argsort(abs(x))\n",
    "        lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action_mean, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_name, 0 + i, i, 8, file1, file2, False) for i in range(1)]\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230203_112925-di4gu1ls/files/agent_1023936steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files[:-1]:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v22\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Reward:\", episodeReward)\n",
    "    print(\"Final Error:\", info[\"Error\"])\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(info[\"Error\"])\n",
    "    maxDevs.append(info[\"maxDev\"])\n",
    "    maxForces.append(np.max(np.abs(info[\"Forces\"])))\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20230203_132128-2rw3jtp8 (rpo on v22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "Episode 0 reward: [0.96858875] error: [0.04427414]\n",
      "****************************** Episode: 2 ******************************\n",
      "Episode 1 reward: [0.47729179] error: [0.02593315]\n",
      "****************************** Episode: 3 ******************************\n",
      "Episode 2 reward: [0.90736648] error: [0.06024902]\n",
      "****************************** Episode: 4 ******************************\n",
      "Episode 3 reward: [0.34802085] error: [0.02022634]\n",
      "****************************** Episode: 5 ******************************\n",
      "Episode 4 reward: [0.96735534] error: [0.03847433]\n",
      "****************************** Episode: 6 ******************************\n",
      "Episode 5 reward: [0.81543913] error: [0.04340074]\n",
      "****************************** Episode: 7 ******************************\n",
      "Episode 6 reward: [0.95621471] error: [0.08602923]\n",
      "****************************** Episode: 8 ******************************\n",
      "Episode 7 reward: [0.92885557] error: [0.03872339]\n",
      "****************************** Episode: 9 ******************************\n",
      "Episode 8 reward: [0.96646782] error: [0.05506429]\n",
      "****************************** Episode: 10 ******************************\n",
      "Episode 9 reward: [0.97486107] error: [0.03223179]\n",
      "****************************** Episode: 11 ******************************\n",
      "Episode 10 reward: [0.92839549] error: [0.04717914]\n",
      "****************************** Episode: 12 ******************************\n",
      "Episode 11 reward: [0.98083137] error: [0.01554846]\n",
      "****************************** Episode: 13 ******************************\n",
      "Episode 12 reward: [0.9762589] error: [0.0408488]\n",
      "****************************** Episode: 14 ******************************\n",
      "Episode 13 reward: [0.63587963] error: [0.06417067]\n",
      "****************************** Episode: 15 ******************************\n",
      "Episode 14 reward: [0.97389371] error: [0.03413015]\n",
      "****************************** Episode: 16 ******************************\n",
      "Episode 15 reward: [0.86938071] error: [0.04471268]\n",
      "****************************** Episode: 17 ******************************\n",
      "Episode 16 reward: [0.97021921] error: [0.04360874]\n",
      "****************************** Episode: 18 ******************************\n",
      "Episode 17 reward: [0.9638794] error: [0.05052099]\n",
      "****************************** Episode: 19 ******************************\n",
      "Episode 18 reward: [0.93229832] error: [0.06109872]\n",
      "****************************** Episode: 20 ******************************\n",
      "Episode 19 reward: [0.83862133] error: [0.03537936]\n",
      "****************************** Episode: 21 ******************************\n",
      "Episode 20 reward: [0.95784219] error: [0.05787439]\n",
      "****************************** Episode: 22 ******************************\n",
      "Episode 21 reward: [0.9715967] error: [0.05299668]\n",
      "****************************** Episode: 23 ******************************\n",
      "Episode 22 reward: [0.97334485] error: [0.03361609]\n",
      "****************************** Episode: 24 ******************************\n",
      "Episode 23 reward: [0.77540657] error: [0.09231963]\n",
      "****************************** Episode: 25 ******************************\n",
      "Episode 24 reward: [0.92845343] error: [0.02791948]\n",
      "****************************** Episode: 26 ******************************\n",
      "Episode 25 reward: [0.95304068] error: [0.07324165]\n",
      "****************************** Episode: 27 ******************************\n",
      "Episode 26 reward: [0.93948865] error: [0.03488758]\n",
      "****************************** Episode: 28 ******************************\n",
      "Episode 27 reward: [0.87347372] error: [0.01947044]\n",
      "****************************** Episode: 29 ******************************\n",
      "Episode 28 reward: [0.86938071] error: [0.04471268]\n",
      "****************************** Episode: 30 ******************************\n",
      "Episode 29 reward: [0.93684591] error: [0.02661878]\n",
      "****************************** Episode: 31 ******************************\n",
      "Episode 30 reward: [0.9672909] error: [0.05494922]\n",
      "****************************** Episode: 32 ******************************\n",
      "Episode 31 reward: [0.82043729] error: [0.04322406]\n",
      "****************************** Episode: 33 ******************************\n",
      "Episode 32 reward: [0.98735479] error: [0.03370531]\n",
      "****************************** Episode: 34 ******************************\n",
      "Episode 33 reward: [0.92435437] error: [0.05425882]\n",
      "****************************** Episode: 35 ******************************\n",
      "Episode 34 reward: [0.87425731] error: [0.06842915]\n",
      "****************************** Episode: 36 ******************************\n",
      "Episode 35 reward: [0.94504742] error: [0.04684958]\n",
      "****************************** Episode: 37 ******************************\n",
      "Episode 36 reward: [0.89897874] error: [0.05244026]\n",
      "****************************** Episode: 38 ******************************\n",
      "Episode 37 reward: [0.94734042] error: [0.02072993]\n",
      "****************************** Episode: 39 ******************************\n",
      "Episode 38 reward: [0.78678406] error: [0.02602733]\n",
      "****************************** Episode: 40 ******************************\n",
      "Episode 39 reward: [0.97197184] error: [0.02976242]\n",
      "****************************** Episode: 41 ******************************\n",
      "Episode 40 reward: [0.49580108] error: [0.06586225]\n",
      "****************************** Episode: 42 ******************************\n",
      "Episode 41 reward: [0.94155462] error: [0.03154075]\n",
      "****************************** Episode: 43 ******************************\n",
      "Episode 42 reward: [0.95016197] error: [0.06577184]\n",
      "****************************** Episode: 44 ******************************\n",
      "Episode 43 reward: [0.9502479] error: [0.05267112]\n",
      "****************************** Episode: 45 ******************************\n",
      "Episode 44 reward: [0.85597229] error: [0.04497777]\n",
      "****************************** Episode: 46 ******************************\n",
      "Episode 45 reward: [0.9569637] error: [0.02308081]\n",
      "****************************** Episode: 47 ******************************\n",
      "Episode 46 reward: [0.91656929] error: [0.10468476]\n",
      "****************************** Episode: 48 ******************************\n",
      "Episode 47 reward: [0.97733544] error: [0.02056041]\n",
      "****************************** Episode: 49 ******************************\n",
      "Episode 48 reward: [0.91575592] error: [0.06511847]\n",
      "****************************** Episode: 50 ******************************\n",
      "Episode 49 reward: [0.95583587] error: [0.06269816]\n",
      "****************************** Episode: 51 ******************************\n",
      "Episode 50 reward: [0.97280643] error: [0.04349158]\n",
      "****************************** Episode: 52 ******************************\n",
      "Episode 51 reward: [0.75207621] error: [0.04191956]\n",
      "****************************** Episode: 53 ******************************\n",
      "Episode 52 reward: [0.76941916] error: [0.03540949]\n",
      "****************************** Episode: 54 ******************************\n",
      "Episode 53 reward: [0.96085231] error: [0.04238884]\n",
      "****************************** Episode: 55 ******************************\n",
      "Episode 54 reward: [0.90736648] error: [0.06024902]\n",
      "****************************** Episode: 56 ******************************\n",
      "Episode 55 reward: [0.83056483] error: [0.03121981]\n",
      "****************************** Episode: 57 ******************************\n",
      "Episode 56 reward: [0.96542974] error: [0.05559392]\n",
      "****************************** Episode: 58 ******************************\n",
      "Episode 57 reward: [0.98295308] error: [0.02982845]\n",
      "****************************** Episode: 59 ******************************\n",
      "Episode 58 reward: [0.97713432] error: [0.02074286]\n",
      "****************************** Episode: 60 ******************************\n",
      "Episode 59 reward: [0.95729648] error: [0.07064684]\n",
      "****************************** Episode: 61 ******************************\n",
      "Episode 60 reward: [0.69792159] error: [0.0188562]\n",
      "****************************** Episode: 62 ******************************\n",
      "Episode 61 reward: [0.69792159] error: [0.0188562]\n",
      "****************************** Episode: 63 ******************************\n",
      "Episode 62 reward: [0.92082162] error: [0.0527693]\n",
      "****************************** Episode: 64 ******************************\n",
      "Episode 63 reward: [0.93316468] error: [0.03553387]\n",
      "****************************** Episode: 65 ******************************\n",
      "Episode 64 reward: [0.95506538] error: [0.0634289]\n",
      "****************************** Episode: 66 ******************************\n",
      "Episode 65 reward: [0.95302208] error: [0.05091316]\n",
      "****************************** Episode: 67 ******************************\n",
      "Episode 66 reward: [0.88123452] error: [0.05075065]\n",
      "****************************** Episode: 68 ******************************\n",
      "Episode 67 reward: [0.95645286] error: [0.05142695]\n",
      "****************************** Episode: 69 ******************************\n",
      "Episode 68 reward: [0.96912683] error: [0.04365812]\n",
      "****************************** Episode: 70 ******************************\n",
      "Episode 69 reward: [0.86915028] error: [0.22975607]\n",
      "****************************** Episode: 71 ******************************\n",
      "Episode 70 reward: [0.97555432] error: [0.02380594]\n",
      "****************************** Episode: 72 ******************************\n",
      "Episode 71 reward: [0.88959571] error: [0.28632873]\n",
      "****************************** Episode: 73 ******************************\n",
      "Episode 72 reward: [0.85433036] error: [0.06572107]\n",
      "****************************** Episode: 74 ******************************\n",
      "Episode 73 reward: [0.95433656] error: [0.04655282]\n",
      "****************************** Episode: 75 ******************************\n",
      "Episode 74 reward: [0.9195042] error: [0.03030799]\n",
      "****************************** Episode: 76 ******************************\n",
      "Episode 75 reward: [0.9571675] error: [0.02599056]\n",
      "****************************** Episode: 77 ******************************\n",
      "Episode 76 reward: [0.97215392] error: [0.03289871]\n",
      "****************************** Episode: 78 ******************************\n",
      "Episode 77 reward: [0.9411219] error: [0.04282137]\n",
      "****************************** Episode: 79 ******************************\n",
      "Episode 78 reward: [0.72820309] error: [0.05058541]\n",
      "****************************** Episode: 80 ******************************\n",
      "Episode 79 reward: [0.6454145] error: [0.03933915]\n",
      "****************************** Episode: 81 ******************************\n",
      "Episode 80 reward: [0.75106425] error: [0.04479472]\n",
      "****************************** Episode: 82 ******************************\n",
      "Episode 81 reward: [0.95791518] error: [0.04970003]\n",
      "****************************** Episode: 83 ******************************\n",
      "Episode 82 reward: [0.93722934] error: [0.16626969]\n",
      "****************************** Episode: 84 ******************************\n",
      "Episode 83 reward: [0.97299816] error: [0.03405333]\n",
      "****************************** Episode: 85 ******************************\n",
      "Episode 84 reward: [0.95640559] error: [0.02436648]\n",
      "****************************** Episode: 86 ******************************\n",
      "Episode 85 reward: [0.97221142] error: [0.03251588]\n",
      "****************************** Episode: 87 ******************************\n",
      "Episode 86 reward: [0.64703992] error: [0.03482894]\n",
      "****************************** Episode: 88 ******************************\n",
      "Episode 87 reward: [0.97213743] error: [0.04686441]\n",
      "****************************** Episode: 89 ******************************\n",
      "Episode 88 reward: [0.95016197] error: [0.06577184]\n",
      "****************************** Episode: 90 ******************************\n",
      "Episode 89 reward: [0.96998906] error: [0.03968053]\n",
      "****************************** Episode: 91 ******************************\n",
      "Episode 90 reward: [0.97989574] error: [0.03860347]\n",
      "****************************** Episode: 92 ******************************\n",
      "Episode 91 reward: [0.97256187] error: [0.02913589]\n",
      "****************************** Episode: 93 ******************************\n",
      "Episode 92 reward: [0.97579339] error: [0.03323141]\n",
      "****************************** Episode: 94 ******************************\n",
      "Episode 93 reward: [0.9631633] error: [0.05125344]\n",
      "****************************** Episode: 95 ******************************\n",
      "Episode 94 reward: [0.72106397] error: [0.0335443]\n",
      "****************************** Episode: 96 ******************************\n",
      "Episode 95 reward: [0.91480094] error: [0.09279868]\n",
      "****************************** Episode: 97 ******************************\n",
      "Episode 96 reward: [0.97096296] error: [0.03836496]\n",
      "****************************** Episode: 98 ******************************\n",
      "Episode 97 reward: [0.932175] error: [0.09899077]\n",
      "****************************** Episode: 99 ******************************\n",
      "Episode 98 reward: [0.93132057] error: [0.03959686]\n",
      "****************************** Episode: 100 ******************************\n",
      "Episode 99 reward: [0.92476286] error: [0.14404133]\n",
      "**********************************************************\n",
      "Inital error (mean) = 0.936\n",
      "Initial error (median) = 0.907\n",
      "Initial error (stdev) = 0.622\n",
      "Initial error (max) = 2.665\n",
      "**********************************************************\n",
      "Final error (mean) = 0.051\n",
      "Final error (median) = 0.044\n",
      "Final error (stdev) = 0.038\n",
      "Final error (max) = 0.286\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.123\n",
      "Max Deviation (median) = 0.108\n",
      "Max Deviation (stdev) = 0.073\n",
      "Max Deviation (max) = 0.566\n",
      "**********************************************************\n",
      "Max Force (mean) = 45.586\n",
      "Max Force (median) = 24.541\n",
      "Max Force (stdev) = 56.321\n",
      "Max Force (max) = 338.483\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.897\n",
      "Episode Rewards (median) = 0.943\n",
      "Episode Rewards (stdev) = 0.116\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-3*torch.ones(1, np.prod(envs.single_action_space.shape)))  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        action_mean = torch.tanh(self.fc3(x))\n",
    "        # Use hardshrink to enforce max number of nonzero outputs\n",
    "        # idx = torch.argsort(abs(x))\n",
    "        # lambd = abs(x[0][idx[0][-(self.n_actions+1)]]).item()\n",
    "        # action_mean = F.hardshrink(x, lambd=lambd) # sets outputs whose magnitudes are smaller than lambda to zero\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, \"Surrogate\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cpu\")\n",
    "agent = Agent(envs, 10).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230203_132128-2rw3jtp8/files/agent_1023936steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Episode\", i, \"reward:\", episodeReward, \"error:\", info[\"Error\"])\n",
    "\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(info[\"Error\"])\n",
    "    maxForces.append(np.max(np.abs(info[\"Forces\"])))\n",
    "    maxDevs.append(info[\"maxDev\"])\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Inital error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run_20230205_144415_19zo595q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "surrogate eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "Initial error (mean) = 0.913\n",
      "Initial error (median) = 0.841\n",
      "Initial error (stdev) = 0.619\n",
      "Initial error (max) = 2.725\n",
      "**********************************************************\n",
      "Final error (mean) = 0.007\n",
      "Final error (median) = 0.007\n",
      "Final error (stdev) = 0.001\n",
      "Final error (max) = 0.008\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.022\n",
      "Max Deviation (median) = 0.022\n",
      "Max Deviation (stdev) = 0.005\n",
      "Max Deviation (max) = 0.030\n",
      "**********************************************************\n",
      "Max Force (mean) = 35.055\n",
      "Max Force (median) = 0.221\n",
      "Max Force (stdev) = 59.562\n",
      "Max Force (max) = 391.326\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 96.574\n",
      "Episode Rewards (median) = 96.604\n",
      "Episode Rewards (stdev) = 0.373\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, mode, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=mode, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod() + np.prod(env.single_action_space.shape), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "run_name = \"dummy\"\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, \"Surrogate\", False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Actor(envs).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run_20230205_144415_19zo595q/files/agent_16383936steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    # print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(100):\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        # print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "    # print('_'*30)\n",
    "    # print(\"Episodic Reward:\", episodeReward)\n",
    "    # print(\"Final Error:\", min(errors))\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20230207_092039-33bs2orw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4094997]\n",
      "Best Error: [0.04207955]\n",
      "****************************** Episode: 2 ******************************\n",
      "______________________________\n",
      "Initial Error [0.04961305]\n",
      "Best Error: [0.03306554]\n",
      "****************************** Episode: 3 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65040186]\n",
      "Best Error: [0.05778247]\n",
      "****************************** Episode: 4 ******************************\n",
      "______________________________\n",
      "Initial Error [0.03102299]\n",
      "Best Error: [0.05137268]\n",
      "****************************** Episode: 5 ******************************\n",
      "______________________________\n",
      "Initial Error [1.17857945]\n",
      "Best Error: [0.06762843]\n",
      "****************************** Episode: 6 ******************************\n",
      "______________________________\n",
      "Initial Error [0.23515676]\n",
      "Best Error: [0.03223966]\n",
      "****************************** Episode: 7 ******************************\n",
      "______________________________\n",
      "Initial Error [1.96479755]\n",
      "Best Error: [0.06501089]\n",
      "****************************** Episode: 8 ******************************\n",
      "______________________________\n",
      "Initial Error [0.54429262]\n",
      "Best Error: [0.03872624]\n",
      "****************************** Episode: 9 ******************************\n",
      "______________________________\n",
      "Initial Error [1.64213275]\n",
      "Best Error: [0.04276806]\n",
      "****************************** Episode: 10 ******************************\n",
      "______________________________\n",
      "Initial Error [1.28214653]\n",
      "Best Error: [0.11992381]\n",
      "****************************** Episode: 11 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65888503]\n",
      "Best Error: [0.05125708]\n",
      "****************************** Episode: 12 ******************************\n",
      "______________________________\n",
      "Initial Error [0.81114083]\n",
      "Best Error: [0.02036134]\n",
      "****************************** Episode: 13 ******************************\n",
      "______________________________\n",
      "Initial Error [1.72059425]\n",
      "Best Error: [0.08143018]\n",
      "****************************** Episode: 14 ******************************\n",
      "______________________________\n",
      "Initial Error [0.17623476]\n",
      "Best Error: [0.0121313]\n",
      "****************************** Episode: 15 ******************************\n",
      "______________________________\n",
      "Initial Error [1.30735345]\n",
      "Best Error: [0.0490439]\n",
      "****************************** Episode: 16 ******************************\n",
      "______________________________\n",
      "Initial Error [0.34231296]\n",
      "Best Error: [0.03446274]\n",
      "****************************** Episode: 17 ******************************\n",
      "______________________________\n",
      "Initial Error [1.46432455]\n",
      "Best Error: [0.07162937]\n",
      "****************************** Episode: 18 ******************************\n",
      "______________________________\n",
      "Initial Error [1.39867507]\n",
      "Best Error: [0.06104263]\n",
      "****************************** Episode: 19 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90246989]\n",
      "Best Error: [0.0288677]\n",
      "****************************** Episode: 20 ******************************\n",
      "______________________________\n",
      "Initial Error [0.21923194]\n",
      "Best Error: [0.02409447]\n",
      "****************************** Episode: 21 ******************************\n",
      "______________________________\n",
      "Initial Error [1.3728037]\n",
      "Best Error: [0.03324711]\n",
      "****************************** Episode: 22 ******************************\n",
      "______________________________\n",
      "Initial Error [1.86586372]\n",
      "Best Error: [0.09801321]\n",
      "****************************** Episode: 23 ******************************\n",
      "______________________________\n",
      "Initial Error [1.26114839]\n",
      "Best Error: [0.06365015]\n",
      "****************************** Episode: 24 ******************************\n",
      "______________________________\n",
      "Initial Error [0.41105224]\n",
      "Best Error: [0.03310408]\n",
      "****************************** Episode: 25 ******************************\n",
      "______________________________\n",
      "Initial Error [0.39022807]\n",
      "Best Error: [0.01538546]\n",
      "****************************** Episode: 26 ******************************\n",
      "______________________________\n",
      "Initial Error [1.55968305]\n",
      "Best Error: [0.08312417]\n",
      "****************************** Episode: 27 ******************************\n",
      "______________________________\n",
      "Initial Error [0.57654607]\n",
      "Best Error: [0.02994328]\n",
      "****************************** Episode: 28 ******************************\n",
      "______________________________\n",
      "Initial Error [0.15388459]\n",
      "Best Error: [0.04480363]\n",
      "****************************** Episode: 29 ******************************\n",
      "______________________________\n",
      "Initial Error [0.34231296]\n",
      "Best Error: [0.03446274]\n",
      "****************************** Episode: 30 ******************************\n",
      "______________________________\n",
      "Initial Error [0.42148941]\n",
      "Best Error: [0.02573209]\n",
      "****************************** Episode: 31 ******************************\n",
      "______________________________\n",
      "Initial Error [1.67993712]\n",
      "Best Error: [0.12133537]\n",
      "****************************** Episode: 32 ******************************\n",
      "______________________________\n",
      "Initial Error [0.24071844]\n",
      "Best Error: [0.02820392]\n",
      "****************************** Episode: 33 ******************************\n",
      "______________________________\n",
      "Initial Error [2.66546132]\n",
      "Best Error: [0.11391419]\n",
      "****************************** Episode: 34 ******************************\n",
      "______________________________\n",
      "Initial Error [0.71727633]\n",
      "Best Error: [0.09395002]\n",
      "****************************** Episode: 35 ******************************\n",
      "______________________________\n",
      "Initial Error [0.54419981]\n",
      "Best Error: [0.14265272]\n",
      "****************************** Episode: 36 ******************************\n",
      "______________________________\n",
      "Initial Error [0.85254558]\n",
      "Best Error: [0.05816063]\n",
      "****************************** Episode: 37 ******************************\n",
      "______________________________\n",
      "Initial Error [0.51910119]\n",
      "Best Error: [0.03420443]\n",
      "****************************** Episode: 38 ******************************\n",
      "______________________________\n",
      "Initial Error [0.39365923]\n",
      "Best Error: [0.01690483]\n",
      "****************************** Episode: 39 ******************************\n",
      "______________________________\n",
      "Initial Error [0.12207031]\n",
      "Best Error: [0.0410929]\n",
      "****************************** Episode: 40 ******************************\n",
      "______________________________\n",
      "Initial Error [1.06187568]\n",
      "Best Error: [0.02645346]\n",
      "****************************** Episode: 41 ******************************\n",
      "______________________________\n",
      "Initial Error [0.13062751]\n",
      "Best Error: [0.03793012]\n",
      "****************************** Episode: 42 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53966195]\n",
      "Best Error: [0.0282541]\n",
      "****************************** Episode: 43 ******************************\n",
      "______________________________\n",
      "Initial Error [1.31971168]\n",
      "Best Error: [0.06238973]\n",
      "****************************** Episode: 44 ******************************\n",
      "______________________________\n",
      "Initial Error [1.05867124]\n",
      "Best Error: [0.0746261]\n",
      "****************************** Episode: 45 ******************************\n",
      "______________________________\n",
      "Initial Error [0.31228556]\n",
      "Best Error: [0.03953301]\n",
      "****************************** Episode: 46 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53631032]\n",
      "Best Error: [0.03138894]\n",
      "****************************** Episode: 47 ******************************\n",
      "______________________________\n",
      "Initial Error [1.25475089]\n",
      "Best Error: [0.14552854]\n",
      "****************************** Episode: 48 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90716108]\n",
      "Best Error: [0.05325067]\n",
      "****************************** Episode: 49 ******************************\n",
      "______________________________\n",
      "Initial Error [0.77297384]\n",
      "Best Error: [0.0137197]\n",
      "****************************** Episode: 50 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4196625]\n",
      "Best Error: [0.11921416]\n",
      "****************************** Episode: 51 ******************************\n",
      "______________________________\n",
      "Initial Error [1.59933301]\n",
      "Best Error: [0.10383215]\n",
      "****************************** Episode: 52 ******************************\n",
      "______________________________\n",
      "Initial Error [0.16908246]\n",
      "Best Error: [0.04234315]\n",
      "****************************** Episode: 53 ******************************\n",
      "______________________________\n",
      "Initial Error [0.15356651]\n",
      "Best Error: [0.02831904]\n",
      "****************************** Episode: 54 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08279282]\n",
      "Best Error: [0.0633298]\n",
      "****************************** Episode: 55 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65040186]\n",
      "Best Error: [0.05778247]\n",
      "****************************** Episode: 56 ******************************\n",
      "______________________________\n",
      "Initial Error [0.18425814]\n",
      "Best Error: [0.02211395]\n",
      "****************************** Episode: 57 ******************************\n",
      "______________________________\n",
      "Initial Error [1.60814327]\n",
      "Best Error: [0.15523456]\n",
      "****************************** Episode: 58 ******************************\n",
      "______________________________\n",
      "Initial Error [1.74978507]\n",
      "Best Error: [0.09744276]\n",
      "****************************** Episode: 59 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90716108]\n",
      "Best Error: [0.05427092]\n",
      "****************************** Episode: 60 ******************************\n",
      "______________________________\n",
      "Initial Error [1.65435627]\n",
      "Best Error: [0.10485899]\n",
      "****************************** Episode: 61 ******************************\n",
      "______________________________\n",
      "Initial Error [0.06242154]\n",
      "Best Error: [0.02898684]\n",
      "****************************** Episode: 62 ******************************\n",
      "______________________________\n",
      "Initial Error [0.06242154]\n",
      "Best Error: [0.02898684]\n",
      "****************************** Episode: 63 ******************************\n",
      "______________________________\n",
      "Initial Error [0.66646101]\n",
      "Best Error: [0.04474231]\n",
      "****************************** Episode: 64 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53166308]\n",
      "Best Error: [0.03517702]\n",
      "****************************** Episode: 65 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4115821]\n",
      "Best Error: [0.03948505]\n",
      "****************************** Episode: 66 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08376787]\n",
      "Best Error: [0.05752694]\n",
      "****************************** Episode: 67 ******************************\n",
      "______________________________\n",
      "Initial Error [0.42731814]\n",
      "Best Error: [0.04567307]\n",
      "****************************** Episode: 68 ******************************\n",
      "______________________________\n",
      "Initial Error [1.18094905]\n",
      "Best Error: [0.08075056]\n",
      "****************************** Episode: 69 ******************************\n",
      "______________________________\n",
      "Initial Error [1.41411192]\n",
      "Best Error: [0.07136099]\n",
      "****************************** Episode: 70 ******************************\n",
      "______________________________\n",
      "Initial Error [1.75587745]\n",
      "Best Error: [0.12620524]\n",
      "****************************** Episode: 71 ******************************\n",
      "______________________________\n",
      "Initial Error [0.97382989]\n",
      "Best Error: [0.07238061]\n",
      "****************************** Episode: 72 ******************************\n",
      "______________________________\n",
      "Initial Error [2.59345648]\n",
      "Best Error: [0.28403088]\n",
      "****************************** Episode: 73 ******************************\n",
      "______________________________\n",
      "Initial Error [0.45116515]\n",
      "Best Error: [0.05925721]\n",
      "****************************** Episode: 74 ******************************\n",
      "______________________________\n",
      "Initial Error [1.01947682]\n",
      "Best Error: [0.06554948]\n",
      "****************************** Episode: 75 ******************************\n",
      "______________________________\n",
      "Initial Error [0.37651639]\n",
      "Best Error: [0.01925396]\n",
      "****************************** Episode: 76 ******************************\n",
      "______________________________\n",
      "Initial Error [0.60679524]\n",
      "Best Error: [0.03146831]\n",
      "****************************** Episode: 77 ******************************\n",
      "______________________________\n",
      "Initial Error [1.1814484]\n",
      "Best Error: [0.06984012]\n",
      "****************************** Episode: 78 ******************************\n",
      "______________________________\n",
      "Initial Error [0.72728863]\n",
      "Best Error: [0.04124776]\n",
      "****************************** Episode: 79 ******************************\n",
      "______________________________\n",
      "Initial Error [0.18611475]\n",
      "Best Error: [0.03441635]\n",
      "****************************** Episode: 80 ******************************\n",
      "______________________________\n",
      "Initial Error [0.11094406]\n",
      "Best Error: [0.01919553]\n",
      "****************************** Episode: 81 ******************************\n",
      "______________________________\n",
      "Initial Error [0.17994489]\n",
      "Best Error: [0.03320211]\n",
      "****************************** Episode: 82 ******************************\n",
      "______________________________\n",
      "Initial Error [1.18094905]\n",
      "Best Error: [0.03881389]\n",
      "****************************** Episode: 83 ******************************\n",
      "______________________________\n",
      "Initial Error [2.64884414]\n",
      "Best Error: [0.06617653]\n",
      "****************************** Episode: 84 ******************************\n",
      "______________________________\n",
      "Initial Error [1.26114839]\n",
      "Best Error: [0.04624053]\n",
      "****************************** Episode: 85 ******************************\n",
      "______________________________\n",
      "Initial Error [0.55893599]\n",
      "Best Error: [0.03410979]\n",
      "****************************** Episode: 86 ******************************\n",
      "______________________________\n",
      "Initial Error [1.17011665]\n",
      "Best Error: [0.08783399]\n",
      "****************************** Episode: 87 ******************************\n",
      "______________________________\n",
      "Initial Error [0.0986767]\n",
      "Best Error: [0.02346092]\n",
      "****************************** Episode: 88 ******************************\n",
      "______________________________\n",
      "Initial Error [1.68198425]\n",
      "Best Error: [0.14215414]\n",
      "****************************** Episode: 89 ******************************\n",
      "______________________________\n",
      "Initial Error [1.31971168]\n",
      "Best Error: [0.06238973]\n",
      "****************************** Episode: 90 ******************************\n",
      "______________________________\n",
      "Initial Error [1.32220241]\n",
      "Best Error: [0.10236635]\n",
      "****************************** Episode: 91 ******************************\n",
      "______________________________\n",
      "Initial Error [1.92016434]\n",
      "Best Error: [0.14165277]\n",
      "****************************** Episode: 92 ******************************\n",
      "______________________________\n",
      "Initial Error [1.06187568]\n",
      "Best Error: [0.04518891]\n",
      "****************************** Episode: 93 ******************************\n",
      "______________________________\n",
      "Initial Error [1.37282397]\n",
      "Best Error: [0.06988816]\n",
      "****************************** Episode: 94 ******************************\n",
      "______________________________\n",
      "Initial Error [1.39136871]\n",
      "Best Error: [0.05356382]\n",
      "****************************** Episode: 95 ******************************\n",
      "______________________________\n",
      "Initial Error [0.12025804]\n",
      "Best Error: [0.0108564]\n",
      "****************************** Episode: 96 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08919835]\n",
      "Best Error: [0.10051365]\n",
      "****************************** Episode: 97 ******************************\n",
      "______________________________\n",
      "Initial Error [1.32124186]\n",
      "Best Error: [0.05189938]\n",
      "****************************** Episode: 98 ******************************\n",
      "______________________________\n",
      "Initial Error [1.45950261]\n",
      "Best Error: [0.08578852]\n",
      "****************************** Episode: 99 ******************************\n",
      "______________________________\n",
      "Initial Error [0.57654607]\n",
      "Best Error: [0.03240329]\n",
      "****************************** Episode: 100 ******************************\n",
      "______________________________\n",
      "Initial Error [1.91449785]\n",
      "Best Error: [0.20902966]\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.936\n",
      "Initial error (median) = 0.907\n",
      "Initial error (stdev) = 0.622\n",
      "Initial error (max) = 2.665\n",
      "**********************************************************\n",
      "Final error (mean) = 0.062\n",
      "Final error (median) = 0.050\n",
      "Final error (stdev) = 0.043\n",
      "Final error (max) = 0.284\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.159\n",
      "Max Deviation (median) = 0.135\n",
      "Max Deviation (stdev) = 0.119\n",
      "Max Deviation (max) = 0.920\n",
      "**********************************************************\n",
      "Max Force (mean) = 169.797\n",
      "Max Force (median) = 44.289\n",
      "Max Force (stdev) = 338.433\n",
      "Max Force (max) = 8324.552\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = -11.289\n",
      "Episode Rewards (median) = -11.323\n",
      "Episode Rewards (stdev) = 2.323\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"Surrogate\", seed=seed, port=50056+idx)\n",
    "        # env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "run_name = \"dummy\"\n",
    "# envs = make_env(env_name, 0 , 0, 10)\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Actor(envs).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230207_092039-33bs2orw/files/agent_1023999steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(100):\n",
    "        with torch.no_grad():\n",
    "            action = agent.forward(obs)\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        # print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "    print('_'*30)\n",
    "    print(\"Initial Error\", info[\"initError\"])\n",
    "    print(\"Best Error:\", minError)\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20230207_092039-33bs2orw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4094997]\n",
      "Best Error: [0.04207955]\n",
      "****************************** Episode: 2 ******************************\n",
      "______________________________\n",
      "Initial Error [0.04961305]\n",
      "Best Error: [0.03306554]\n",
      "****************************** Episode: 3 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65040186]\n",
      "Best Error: [0.05778247]\n",
      "****************************** Episode: 4 ******************************\n",
      "______________________________\n",
      "Initial Error [0.03102299]\n",
      "Best Error: [0.05137268]\n",
      "****************************** Episode: 5 ******************************\n",
      "______________________________\n",
      "Initial Error [1.17857945]\n",
      "Best Error: [0.06762843]\n",
      "****************************** Episode: 6 ******************************\n",
      "______________________________\n",
      "Initial Error [0.23515676]\n",
      "Best Error: [0.03223966]\n",
      "****************************** Episode: 7 ******************************\n",
      "______________________________\n",
      "Initial Error [1.96479755]\n",
      "Best Error: [0.06501089]\n",
      "****************************** Episode: 8 ******************************\n",
      "______________________________\n",
      "Initial Error [0.54429262]\n",
      "Best Error: [0.03872624]\n",
      "****************************** Episode: 9 ******************************\n",
      "______________________________\n",
      "Initial Error [1.64213275]\n",
      "Best Error: [0.04276806]\n",
      "****************************** Episode: 10 ******************************\n",
      "______________________________\n",
      "Initial Error [1.28214653]\n",
      "Best Error: [0.11992381]\n",
      "****************************** Episode: 11 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65888503]\n",
      "Best Error: [0.05125708]\n",
      "****************************** Episode: 12 ******************************\n",
      "______________________________\n",
      "Initial Error [0.81114083]\n",
      "Best Error: [0.02036134]\n",
      "****************************** Episode: 13 ******************************\n",
      "______________________________\n",
      "Initial Error [1.72059425]\n",
      "Best Error: [0.08143018]\n",
      "****************************** Episode: 14 ******************************\n",
      "______________________________\n",
      "Initial Error [0.17623476]\n",
      "Best Error: [0.0121313]\n",
      "****************************** Episode: 15 ******************************\n",
      "______________________________\n",
      "Initial Error [1.30735345]\n",
      "Best Error: [0.0490439]\n",
      "****************************** Episode: 16 ******************************\n",
      "______________________________\n",
      "Initial Error [0.34231296]\n",
      "Best Error: [0.03446274]\n",
      "****************************** Episode: 17 ******************************\n",
      "______________________________\n",
      "Initial Error [1.46432455]\n",
      "Best Error: [0.07162937]\n",
      "****************************** Episode: 18 ******************************\n",
      "______________________________\n",
      "Initial Error [1.39867507]\n",
      "Best Error: [0.06104263]\n",
      "****************************** Episode: 19 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90246989]\n",
      "Best Error: [0.0288677]\n",
      "****************************** Episode: 20 ******************************\n",
      "______________________________\n",
      "Initial Error [0.21923194]\n",
      "Best Error: [0.02409447]\n",
      "****************************** Episode: 21 ******************************\n",
      "______________________________\n",
      "Initial Error [1.3728037]\n",
      "Best Error: [0.03324711]\n",
      "****************************** Episode: 22 ******************************\n",
      "______________________________\n",
      "Initial Error [1.86586372]\n",
      "Best Error: [0.09801321]\n",
      "****************************** Episode: 23 ******************************\n",
      "______________________________\n",
      "Initial Error [1.26114839]\n",
      "Best Error: [0.06365015]\n",
      "****************************** Episode: 24 ******************************\n",
      "______________________________\n",
      "Initial Error [0.41105224]\n",
      "Best Error: [0.03310408]\n",
      "****************************** Episode: 25 ******************************\n",
      "______________________________\n",
      "Initial Error [0.39022807]\n",
      "Best Error: [0.01538546]\n",
      "****************************** Episode: 26 ******************************\n",
      "______________________________\n",
      "Initial Error [1.55968305]\n",
      "Best Error: [0.08312417]\n",
      "****************************** Episode: 27 ******************************\n",
      "______________________________\n",
      "Initial Error [0.57654607]\n",
      "Best Error: [0.02994328]\n",
      "****************************** Episode: 28 ******************************\n",
      "______________________________\n",
      "Initial Error [0.15388459]\n",
      "Best Error: [0.04480363]\n",
      "****************************** Episode: 29 ******************************\n",
      "______________________________\n",
      "Initial Error [0.34231296]\n",
      "Best Error: [0.03446274]\n",
      "****************************** Episode: 30 ******************************\n",
      "______________________________\n",
      "Initial Error [0.42148941]\n",
      "Best Error: [0.02573209]\n",
      "****************************** Episode: 31 ******************************\n",
      "______________________________\n",
      "Initial Error [1.67993712]\n",
      "Best Error: [0.12133537]\n",
      "****************************** Episode: 32 ******************************\n",
      "______________________________\n",
      "Initial Error [0.24071844]\n",
      "Best Error: [0.02820392]\n",
      "****************************** Episode: 33 ******************************\n",
      "______________________________\n",
      "Initial Error [2.66546132]\n",
      "Best Error: [0.11391419]\n",
      "****************************** Episode: 34 ******************************\n",
      "______________________________\n",
      "Initial Error [0.71727633]\n",
      "Best Error: [0.09395002]\n",
      "****************************** Episode: 35 ******************************\n",
      "______________________________\n",
      "Initial Error [0.54419981]\n",
      "Best Error: [0.14265272]\n",
      "****************************** Episode: 36 ******************************\n",
      "______________________________\n",
      "Initial Error [0.85254558]\n",
      "Best Error: [0.05816063]\n",
      "****************************** Episode: 37 ******************************\n",
      "______________________________\n",
      "Initial Error [0.51910119]\n",
      "Best Error: [0.03420443]\n",
      "****************************** Episode: 38 ******************************\n",
      "______________________________\n",
      "Initial Error [0.39365923]\n",
      "Best Error: [0.01690483]\n",
      "****************************** Episode: 39 ******************************\n",
      "______________________________\n",
      "Initial Error [0.12207031]\n",
      "Best Error: [0.0410929]\n",
      "****************************** Episode: 40 ******************************\n",
      "______________________________\n",
      "Initial Error [1.06187568]\n",
      "Best Error: [0.02645346]\n",
      "****************************** Episode: 41 ******************************\n",
      "______________________________\n",
      "Initial Error [0.13062751]\n",
      "Best Error: [0.03793012]\n",
      "****************************** Episode: 42 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53966195]\n",
      "Best Error: [0.0282541]\n",
      "****************************** Episode: 43 ******************************\n",
      "______________________________\n",
      "Initial Error [1.31971168]\n",
      "Best Error: [0.06238973]\n",
      "****************************** Episode: 44 ******************************\n",
      "______________________________\n",
      "Initial Error [1.05867124]\n",
      "Best Error: [0.0746261]\n",
      "****************************** Episode: 45 ******************************\n",
      "______________________________\n",
      "Initial Error [0.31228556]\n",
      "Best Error: [0.03953301]\n",
      "****************************** Episode: 46 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53631032]\n",
      "Best Error: [0.03138894]\n",
      "****************************** Episode: 47 ******************************\n",
      "______________________________\n",
      "Initial Error [1.25475089]\n",
      "Best Error: [0.14552854]\n",
      "****************************** Episode: 48 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90716108]\n",
      "Best Error: [0.05325067]\n",
      "****************************** Episode: 49 ******************************\n",
      "______________________________\n",
      "Initial Error [0.77297384]\n",
      "Best Error: [0.0137197]\n",
      "****************************** Episode: 50 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4196625]\n",
      "Best Error: [0.11921416]\n",
      "****************************** Episode: 51 ******************************\n",
      "______________________________\n",
      "Initial Error [1.59933301]\n",
      "Best Error: [0.10383215]\n",
      "****************************** Episode: 52 ******************************\n",
      "______________________________\n",
      "Initial Error [0.16908246]\n",
      "Best Error: [0.04234315]\n",
      "****************************** Episode: 53 ******************************\n",
      "______________________________\n",
      "Initial Error [0.15356651]\n",
      "Best Error: [0.02831904]\n",
      "****************************** Episode: 54 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08279282]\n",
      "Best Error: [0.0633298]\n",
      "****************************** Episode: 55 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65040186]\n",
      "Best Error: [0.05778247]\n",
      "****************************** Episode: 56 ******************************\n",
      "______________________________\n",
      "Initial Error [0.18425814]\n",
      "Best Error: [0.02211395]\n",
      "****************************** Episode: 57 ******************************\n",
      "______________________________\n",
      "Initial Error [1.60814327]\n",
      "Best Error: [0.15523456]\n",
      "****************************** Episode: 58 ******************************\n",
      "______________________________\n",
      "Initial Error [1.74978507]\n",
      "Best Error: [0.09744276]\n",
      "****************************** Episode: 59 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90716108]\n",
      "Best Error: [0.05427092]\n",
      "****************************** Episode: 60 ******************************\n",
      "______________________________\n",
      "Initial Error [1.65435627]\n",
      "Best Error: [0.10485899]\n",
      "****************************** Episode: 61 ******************************\n",
      "______________________________\n",
      "Initial Error [0.06242154]\n",
      "Best Error: [0.02898684]\n",
      "****************************** Episode: 62 ******************************\n",
      "______________________________\n",
      "Initial Error [0.06242154]\n",
      "Best Error: [0.02898684]\n",
      "****************************** Episode: 63 ******************************\n",
      "______________________________\n",
      "Initial Error [0.66646101]\n",
      "Best Error: [0.04474231]\n",
      "****************************** Episode: 64 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53166308]\n",
      "Best Error: [0.03517702]\n",
      "****************************** Episode: 65 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4115821]\n",
      "Best Error: [0.03948505]\n",
      "****************************** Episode: 66 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08376787]\n",
      "Best Error: [0.05752694]\n",
      "****************************** Episode: 67 ******************************\n",
      "______________________________\n",
      "Initial Error [0.42731814]\n",
      "Best Error: [0.04567307]\n",
      "****************************** Episode: 68 ******************************\n",
      "______________________________\n",
      "Initial Error [1.18094905]\n",
      "Best Error: [0.08075056]\n",
      "****************************** Episode: 69 ******************************\n",
      "______________________________\n",
      "Initial Error [1.41411192]\n",
      "Best Error: [0.07136099]\n",
      "****************************** Episode: 70 ******************************\n",
      "______________________________\n",
      "Initial Error [1.75587745]\n",
      "Best Error: [0.12620524]\n",
      "****************************** Episode: 71 ******************************\n",
      "______________________________\n",
      "Initial Error [0.97382989]\n",
      "Best Error: [0.07238061]\n",
      "****************************** Episode: 72 ******************************\n",
      "______________________________\n",
      "Initial Error [2.59345648]\n",
      "Best Error: [0.28403088]\n",
      "****************************** Episode: 73 ******************************\n",
      "______________________________\n",
      "Initial Error [0.45116515]\n",
      "Best Error: [0.05925721]\n",
      "****************************** Episode: 74 ******************************\n",
      "______________________________\n",
      "Initial Error [1.01947682]\n",
      "Best Error: [0.06554948]\n",
      "****************************** Episode: 75 ******************************\n",
      "______________________________\n",
      "Initial Error [0.37651639]\n",
      "Best Error: [0.01925396]\n",
      "****************************** Episode: 76 ******************************\n",
      "______________________________\n",
      "Initial Error [0.60679524]\n",
      "Best Error: [0.03146831]\n",
      "****************************** Episode: 77 ******************************\n",
      "______________________________\n",
      "Initial Error [1.1814484]\n",
      "Best Error: [0.06984012]\n",
      "****************************** Episode: 78 ******************************\n",
      "______________________________\n",
      "Initial Error [0.72728863]\n",
      "Best Error: [0.04124776]\n",
      "****************************** Episode: 79 ******************************\n",
      "______________________________\n",
      "Initial Error [0.18611475]\n",
      "Best Error: [0.03441635]\n",
      "****************************** Episode: 80 ******************************\n",
      "______________________________\n",
      "Initial Error [0.11094406]\n",
      "Best Error: [0.01919553]\n",
      "****************************** Episode: 81 ******************************\n",
      "______________________________\n",
      "Initial Error [0.17994489]\n",
      "Best Error: [0.03320211]\n",
      "****************************** Episode: 82 ******************************\n",
      "______________________________\n",
      "Initial Error [1.18094905]\n",
      "Best Error: [0.03881389]\n",
      "****************************** Episode: 83 ******************************\n",
      "______________________________\n",
      "Initial Error [2.64884414]\n",
      "Best Error: [0.06617653]\n",
      "****************************** Episode: 84 ******************************\n",
      "______________________________\n",
      "Initial Error [1.26114839]\n",
      "Best Error: [0.04624053]\n",
      "****************************** Episode: 85 ******************************\n",
      "______________________________\n",
      "Initial Error [0.55893599]\n",
      "Best Error: [0.03410979]\n",
      "****************************** Episode: 86 ******************************\n",
      "______________________________\n",
      "Initial Error [1.17011665]\n",
      "Best Error: [0.08783399]\n",
      "****************************** Episode: 87 ******************************\n",
      "______________________________\n",
      "Initial Error [0.0986767]\n",
      "Best Error: [0.02346092]\n",
      "****************************** Episode: 88 ******************************\n",
      "______________________________\n",
      "Initial Error [1.68198425]\n",
      "Best Error: [0.14215414]\n",
      "****************************** Episode: 89 ******************************\n",
      "______________________________\n",
      "Initial Error [1.31971168]\n",
      "Best Error: [0.06238973]\n",
      "****************************** Episode: 90 ******************************\n",
      "______________________________\n",
      "Initial Error [1.32220241]\n",
      "Best Error: [0.10236635]\n",
      "****************************** Episode: 91 ******************************\n",
      "______________________________\n",
      "Initial Error [1.92016434]\n",
      "Best Error: [0.14165277]\n",
      "****************************** Episode: 92 ******************************\n",
      "______________________________\n",
      "Initial Error [1.06187568]\n",
      "Best Error: [0.04518891]\n",
      "****************************** Episode: 93 ******************************\n",
      "______________________________\n",
      "Initial Error [1.37282397]\n",
      "Best Error: [0.06988816]\n",
      "****************************** Episode: 94 ******************************\n",
      "______________________________\n",
      "Initial Error [1.39136871]\n",
      "Best Error: [0.05356382]\n",
      "****************************** Episode: 95 ******************************\n",
      "______________________________\n",
      "Initial Error [0.12025804]\n",
      "Best Error: [0.0108564]\n",
      "****************************** Episode: 96 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08919835]\n",
      "Best Error: [0.10051365]\n",
      "****************************** Episode: 97 ******************************\n",
      "______________________________\n",
      "Initial Error [1.32124186]\n",
      "Best Error: [0.05189938]\n",
      "****************************** Episode: 98 ******************************\n",
      "______________________________\n",
      "Initial Error [1.45950261]\n",
      "Best Error: [0.08578852]\n",
      "****************************** Episode: 99 ******************************\n",
      "______________________________\n",
      "Initial Error [0.57654607]\n",
      "Best Error: [0.03240329]\n",
      "****************************** Episode: 100 ******************************\n",
      "______________________________\n",
      "Initial Error [1.91449785]\n",
      "Best Error: [0.20902966]\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.936\n",
      "Initial error (median) = 0.907\n",
      "Initial error (stdev) = 0.622\n",
      "Initial error (max) = 2.665\n",
      "**********************************************************\n",
      "Final error (mean) = 0.062\n",
      "Final error (median) = 0.050\n",
      "Final error (stdev) = 0.043\n",
      "Final error (max) = 0.284\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.159\n",
      "Max Deviation (median) = 0.135\n",
      "Max Deviation (stdev) = 0.119\n",
      "Max Deviation (max) = 0.920\n",
      "**********************************************************\n",
      "Max Force (mean) = 169.797\n",
      "Max Force (median) = 44.289\n",
      "Max Force (stdev) = 338.433\n",
      "Max Force (max) = 8324.552\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = -11.289\n",
      "Episode Rewards (median) = -11.323\n",
      "Episode Rewards (stdev) = 2.323\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"Surrogate\", seed=seed, port=50056+idx)\n",
    "        # env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "run_name = \"dummy\"\n",
    "# envs = make_env(env_name, 0 , 0, 10)\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Actor(envs).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230207_092039-33bs2orw/files/agent_1023999steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(100):\n",
    "        with torch.no_grad():\n",
    "            action = agent.forward(obs)\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        # print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "    print('_'*30)\n",
    "    print(\"Initial Error\", info[\"initError\"])\n",
    "    print(\"Best Error:\", minError)\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial error (mean) = 0.936\n",
    "Initial error (median) = 0.907\n",
    "Initial error (stdev) = 0.622\n",
    "Initial error (max) = 2.665\n",
    "**********************************************************\n",
    "Final error (mean) = 0.062\n",
    "Final error (median) = 0.050\n",
    "Final error (stdev) = 0.043\n",
    "Final error (max) = 0.284\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.159\n",
    "Max Deviation (median) = 0.135\n",
    "Max Deviation (stdev) = 0.119\n",
    "Max Deviation (max) = 0.920\n",
    "**********************************************************\n",
    "Max Force (mean) = 169.797\n",
    "Max Force (median) = 44.289\n",
    "Max Force (stdev) = 338.433\n",
    "Max Force (max) = 8324.552\n",
    "**********************************************************\n",
    "Episode Rewards (mean) = -11.289\n",
    "Episode Rewards (median) = -11.323\n",
    "Episode Rewards (stdev) = 2.323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LinearRegression from version 1.1.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\vector\\vector_env.py:56: DeprecationWarning: \u001b[33mWARN: Initializing vector env in old step API which returns one bool array instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future. \u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Python\\venv\\pyANSYS\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4094997]\n",
      "Best Error: [0.04207955]\n",
      "****************************** Episode: 2 ******************************\n",
      "______________________________\n",
      "Initial Error [0.04961305]\n",
      "Best Error: [0.03306554]\n",
      "****************************** Episode: 3 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65040186]\n",
      "Best Error: [0.05778247]\n",
      "****************************** Episode: 4 ******************************\n",
      "______________________________\n",
      "Initial Error [0.03102299]\n",
      "Best Error: [0.05137268]\n",
      "****************************** Episode: 5 ******************************\n",
      "______________________________\n",
      "Initial Error [1.17857945]\n",
      "Best Error: [0.06762843]\n",
      "****************************** Episode: 6 ******************************\n",
      "______________________________\n",
      "Initial Error [0.23515676]\n",
      "Best Error: [0.03223966]\n",
      "****************************** Episode: 7 ******************************\n",
      "______________________________\n",
      "Initial Error [1.96479755]\n",
      "Best Error: [0.06501089]\n",
      "****************************** Episode: 8 ******************************\n",
      "______________________________\n",
      "Initial Error [0.54429262]\n",
      "Best Error: [0.03872624]\n",
      "****************************** Episode: 9 ******************************\n",
      "______________________________\n",
      "Initial Error [1.64213275]\n",
      "Best Error: [0.04276806]\n",
      "****************************** Episode: 10 ******************************\n",
      "______________________________\n",
      "Initial Error [1.28214653]\n",
      "Best Error: [0.11992381]\n",
      "****************************** Episode: 11 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65888503]\n",
      "Best Error: [0.05125708]\n",
      "****************************** Episode: 12 ******************************\n",
      "______________________________\n",
      "Initial Error [0.81114083]\n",
      "Best Error: [0.02036134]\n",
      "****************************** Episode: 13 ******************************\n",
      "______________________________\n",
      "Initial Error [1.72059425]\n",
      "Best Error: [0.08143018]\n",
      "****************************** Episode: 14 ******************************\n",
      "______________________________\n",
      "Initial Error [0.17623476]\n",
      "Best Error: [0.0121313]\n",
      "****************************** Episode: 15 ******************************\n",
      "______________________________\n",
      "Initial Error [1.30735345]\n",
      "Best Error: [0.0490439]\n",
      "****************************** Episode: 16 ******************************\n",
      "______________________________\n",
      "Initial Error [0.34231296]\n",
      "Best Error: [0.03446274]\n",
      "****************************** Episode: 17 ******************************\n",
      "______________________________\n",
      "Initial Error [1.46432455]\n",
      "Best Error: [0.07162937]\n",
      "****************************** Episode: 18 ******************************\n",
      "______________________________\n",
      "Initial Error [1.39867507]\n",
      "Best Error: [0.06104263]\n",
      "****************************** Episode: 19 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90246989]\n",
      "Best Error: [0.0288677]\n",
      "****************************** Episode: 20 ******************************\n",
      "______________________________\n",
      "Initial Error [0.21923194]\n",
      "Best Error: [0.02409447]\n",
      "****************************** Episode: 21 ******************************\n",
      "______________________________\n",
      "Initial Error [1.3728037]\n",
      "Best Error: [0.03324711]\n",
      "****************************** Episode: 22 ******************************\n",
      "______________________________\n",
      "Initial Error [1.86586372]\n",
      "Best Error: [0.09801321]\n",
      "****************************** Episode: 23 ******************************\n",
      "______________________________\n",
      "Initial Error [1.26114839]\n",
      "Best Error: [0.06365015]\n",
      "****************************** Episode: 24 ******************************\n",
      "______________________________\n",
      "Initial Error [0.41105224]\n",
      "Best Error: [0.03310408]\n",
      "****************************** Episode: 25 ******************************\n",
      "______________________________\n",
      "Initial Error [0.39022807]\n",
      "Best Error: [0.01538546]\n",
      "****************************** Episode: 26 ******************************\n",
      "______________________________\n",
      "Initial Error [1.55968305]\n",
      "Best Error: [0.08312417]\n",
      "****************************** Episode: 27 ******************************\n",
      "______________________________\n",
      "Initial Error [0.57654607]\n",
      "Best Error: [0.02994328]\n",
      "****************************** Episode: 28 ******************************\n",
      "______________________________\n",
      "Initial Error [0.15388459]\n",
      "Best Error: [0.04480363]\n",
      "****************************** Episode: 29 ******************************\n",
      "______________________________\n",
      "Initial Error [0.34231296]\n",
      "Best Error: [0.03446274]\n",
      "****************************** Episode: 30 ******************************\n",
      "______________________________\n",
      "Initial Error [0.42148941]\n",
      "Best Error: [0.02573209]\n",
      "****************************** Episode: 31 ******************************\n",
      "______________________________\n",
      "Initial Error [1.67993712]\n",
      "Best Error: [0.12133537]\n",
      "****************************** Episode: 32 ******************************\n",
      "______________________________\n",
      "Initial Error [0.24071844]\n",
      "Best Error: [0.02820392]\n",
      "****************************** Episode: 33 ******************************\n",
      "______________________________\n",
      "Initial Error [2.66546132]\n",
      "Best Error: [0.11391419]\n",
      "****************************** Episode: 34 ******************************\n",
      "______________________________\n",
      "Initial Error [0.71727633]\n",
      "Best Error: [0.09395002]\n",
      "****************************** Episode: 35 ******************************\n",
      "______________________________\n",
      "Initial Error [0.54419981]\n",
      "Best Error: [0.14265272]\n",
      "****************************** Episode: 36 ******************************\n",
      "______________________________\n",
      "Initial Error [0.85254558]\n",
      "Best Error: [0.05816063]\n",
      "****************************** Episode: 37 ******************************\n",
      "______________________________\n",
      "Initial Error [0.51910119]\n",
      "Best Error: [0.03420443]\n",
      "****************************** Episode: 38 ******************************\n",
      "______________________________\n",
      "Initial Error [0.39365923]\n",
      "Best Error: [0.01690483]\n",
      "****************************** Episode: 39 ******************************\n",
      "______________________________\n",
      "Initial Error [0.12207031]\n",
      "Best Error: [0.0410929]\n",
      "****************************** Episode: 40 ******************************\n",
      "______________________________\n",
      "Initial Error [1.06187568]\n",
      "Best Error: [0.02645346]\n",
      "****************************** Episode: 41 ******************************\n",
      "______________________________\n",
      "Initial Error [0.13062751]\n",
      "Best Error: [0.03793012]\n",
      "****************************** Episode: 42 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53966195]\n",
      "Best Error: [0.0282541]\n",
      "****************************** Episode: 43 ******************************\n",
      "______________________________\n",
      "Initial Error [1.31971168]\n",
      "Best Error: [0.06238973]\n",
      "****************************** Episode: 44 ******************************\n",
      "______________________________\n",
      "Initial Error [1.05867124]\n",
      "Best Error: [0.0746261]\n",
      "****************************** Episode: 45 ******************************\n",
      "______________________________\n",
      "Initial Error [0.31228556]\n",
      "Best Error: [0.03953301]\n",
      "****************************** Episode: 46 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53631032]\n",
      "Best Error: [0.03138894]\n",
      "****************************** Episode: 47 ******************************\n",
      "______________________________\n",
      "Initial Error [1.25475089]\n",
      "Best Error: [0.14552854]\n",
      "****************************** Episode: 48 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90716108]\n",
      "Best Error: [0.05325067]\n",
      "****************************** Episode: 49 ******************************\n",
      "______________________________\n",
      "Initial Error [0.77297384]\n",
      "Best Error: [0.0137197]\n",
      "****************************** Episode: 50 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4196625]\n",
      "Best Error: [0.11921416]\n",
      "****************************** Episode: 51 ******************************\n",
      "______________________________\n",
      "Initial Error [1.59933301]\n",
      "Best Error: [0.10383215]\n",
      "****************************** Episode: 52 ******************************\n",
      "______________________________\n",
      "Initial Error [0.16908246]\n",
      "Best Error: [0.04234315]\n",
      "****************************** Episode: 53 ******************************\n",
      "______________________________\n",
      "Initial Error [0.15356651]\n",
      "Best Error: [0.02831904]\n",
      "****************************** Episode: 54 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08279282]\n",
      "Best Error: [0.0633298]\n",
      "****************************** Episode: 55 ******************************\n",
      "______________________________\n",
      "Initial Error [0.65040186]\n",
      "Best Error: [0.05778247]\n",
      "****************************** Episode: 56 ******************************\n",
      "______________________________\n",
      "Initial Error [0.18425814]\n",
      "Best Error: [0.02211395]\n",
      "****************************** Episode: 57 ******************************\n",
      "______________________________\n",
      "Initial Error [1.60814327]\n",
      "Best Error: [0.15523456]\n",
      "****************************** Episode: 58 ******************************\n",
      "______________________________\n",
      "Initial Error [1.74978507]\n",
      "Best Error: [0.09744276]\n",
      "****************************** Episode: 59 ******************************\n",
      "______________________________\n",
      "Initial Error [0.90716108]\n",
      "Best Error: [0.05427092]\n",
      "****************************** Episode: 60 ******************************\n",
      "______________________________\n",
      "Initial Error [1.65435627]\n",
      "Best Error: [0.10485899]\n",
      "****************************** Episode: 61 ******************************\n",
      "______________________________\n",
      "Initial Error [0.06242154]\n",
      "Best Error: [0.02898684]\n",
      "****************************** Episode: 62 ******************************\n",
      "______________________________\n",
      "Initial Error [0.06242154]\n",
      "Best Error: [0.02898684]\n",
      "****************************** Episode: 63 ******************************\n",
      "______________________________\n",
      "Initial Error [0.66646101]\n",
      "Best Error: [0.04474231]\n",
      "****************************** Episode: 64 ******************************\n",
      "______________________________\n",
      "Initial Error [0.53166308]\n",
      "Best Error: [0.03517702]\n",
      "****************************** Episode: 65 ******************************\n",
      "______________________________\n",
      "Initial Error [1.4115821]\n",
      "Best Error: [0.03948505]\n",
      "****************************** Episode: 66 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08376787]\n",
      "Best Error: [0.05752694]\n",
      "****************************** Episode: 67 ******************************\n",
      "______________________________\n",
      "Initial Error [0.42731814]\n",
      "Best Error: [0.04567307]\n",
      "****************************** Episode: 68 ******************************\n",
      "______________________________\n",
      "Initial Error [1.18094905]\n",
      "Best Error: [0.08075056]\n",
      "****************************** Episode: 69 ******************************\n",
      "______________________________\n",
      "Initial Error [1.41411192]\n",
      "Best Error: [0.07136099]\n",
      "****************************** Episode: 70 ******************************\n",
      "______________________________\n",
      "Initial Error [1.75587745]\n",
      "Best Error: [0.12620524]\n",
      "****************************** Episode: 71 ******************************\n",
      "______________________________\n",
      "Initial Error [0.97382989]\n",
      "Best Error: [0.07238061]\n",
      "****************************** Episode: 72 ******************************\n",
      "______________________________\n",
      "Initial Error [2.59345648]\n",
      "Best Error: [0.28403088]\n",
      "****************************** Episode: 73 ******************************\n",
      "______________________________\n",
      "Initial Error [0.45116515]\n",
      "Best Error: [0.05925721]\n",
      "****************************** Episode: 74 ******************************\n",
      "______________________________\n",
      "Initial Error [1.01947682]\n",
      "Best Error: [0.06554948]\n",
      "****************************** Episode: 75 ******************************\n",
      "______________________________\n",
      "Initial Error [0.37651639]\n",
      "Best Error: [0.01925396]\n",
      "****************************** Episode: 76 ******************************\n",
      "______________________________\n",
      "Initial Error [0.60679524]\n",
      "Best Error: [0.03146831]\n",
      "****************************** Episode: 77 ******************************\n",
      "______________________________\n",
      "Initial Error [1.1814484]\n",
      "Best Error: [0.06984012]\n",
      "****************************** Episode: 78 ******************************\n",
      "______________________________\n",
      "Initial Error [0.72728863]\n",
      "Best Error: [0.04124776]\n",
      "****************************** Episode: 79 ******************************\n",
      "______________________________\n",
      "Initial Error [0.18611475]\n",
      "Best Error: [0.03441635]\n",
      "****************************** Episode: 80 ******************************\n",
      "______________________________\n",
      "Initial Error [0.11094406]\n",
      "Best Error: [0.01919553]\n",
      "****************************** Episode: 81 ******************************\n",
      "______________________________\n",
      "Initial Error [0.17994489]\n",
      "Best Error: [0.03320211]\n",
      "****************************** Episode: 82 ******************************\n",
      "______________________________\n",
      "Initial Error [1.18094905]\n",
      "Best Error: [0.03881389]\n",
      "****************************** Episode: 83 ******************************\n",
      "______________________________\n",
      "Initial Error [2.64884414]\n",
      "Best Error: [0.06617653]\n",
      "****************************** Episode: 84 ******************************\n",
      "______________________________\n",
      "Initial Error [1.26114839]\n",
      "Best Error: [0.04624053]\n",
      "****************************** Episode: 85 ******************************\n",
      "______________________________\n",
      "Initial Error [0.55893599]\n",
      "Best Error: [0.03410979]\n",
      "****************************** Episode: 86 ******************************\n",
      "______________________________\n",
      "Initial Error [1.17011665]\n",
      "Best Error: [0.08783399]\n",
      "****************************** Episode: 87 ******************************\n",
      "______________________________\n",
      "Initial Error [0.0986767]\n",
      "Best Error: [0.02346092]\n",
      "****************************** Episode: 88 ******************************\n",
      "______________________________\n",
      "Initial Error [1.68198425]\n",
      "Best Error: [0.14215414]\n",
      "****************************** Episode: 89 ******************************\n",
      "______________________________\n",
      "Initial Error [1.31971168]\n",
      "Best Error: [0.06238973]\n",
      "****************************** Episode: 90 ******************************\n",
      "______________________________\n",
      "Initial Error [1.32220241]\n",
      "Best Error: [0.10236635]\n",
      "****************************** Episode: 91 ******************************\n",
      "______________________________\n",
      "Initial Error [1.92016434]\n",
      "Best Error: [0.14165277]\n",
      "****************************** Episode: 92 ******************************\n",
      "______________________________\n",
      "Initial Error [1.06187568]\n",
      "Best Error: [0.04518891]\n",
      "****************************** Episode: 93 ******************************\n",
      "______________________________\n",
      "Initial Error [1.37282397]\n",
      "Best Error: [0.06988816]\n",
      "****************************** Episode: 94 ******************************\n",
      "______________________________\n",
      "Initial Error [1.39136871]\n",
      "Best Error: [0.05356382]\n",
      "****************************** Episode: 95 ******************************\n",
      "______________________________\n",
      "Initial Error [0.12025804]\n",
      "Best Error: [0.0108564]\n",
      "****************************** Episode: 96 ******************************\n",
      "______________________________\n",
      "Initial Error [1.08919835]\n",
      "Best Error: [0.10051365]\n",
      "****************************** Episode: 97 ******************************\n",
      "______________________________\n",
      "Initial Error [1.32124186]\n",
      "Best Error: [0.05189938]\n",
      "****************************** Episode: 98 ******************************\n",
      "______________________________\n",
      "Initial Error [1.45950261]\n",
      "Best Error: [0.08578852]\n",
      "****************************** Episode: 99 ******************************\n",
      "______________________________\n",
      "Initial Error [0.57654607]\n",
      "Best Error: [0.03240329]\n",
      "****************************** Episode: 100 ******************************\n",
      "______________________________\n",
      "Initial Error [1.91449785]\n",
      "Best Error: [0.20902966]\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.936\n",
      "Initial error (median) = 0.907\n",
      "Initial error (stdev) = 0.622\n",
      "Initial error (max) = 2.665\n",
      "**********************************************************\n",
      "Final error (mean) = 0.062\n",
      "Final error (median) = 0.050\n",
      "Final error (stdev) = 0.043\n",
      "Final error (max) = 0.284\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.159\n",
      "Max Deviation (median) = 0.135\n",
      "Max Deviation (stdev) = 0.119\n",
      "Max Deviation (max) = 0.920\n",
      "**********************************************************\n",
      "Max Force (mean) = 169.797\n",
      "Max Force (median) = 44.289\n",
      "Max Force (stdev) = 338.433\n",
      "Max Force (max) = 8324.552\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = -11.289\n",
      "Episode Rewards (median) = -11.323\n",
      "Episode Rewards (stdev) = 2.323\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"Surrogate\", seed=seed, port=50056+idx)\n",
    "        # env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.single_observation_space.shape).prod(), 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mu = nn.Linear(256, np.prod(env.single_action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "# Make the environment (test mode)\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "run_name = \"dummy\"\n",
    "# envs = make_env(env_name, 0 , 0, 10)\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10) for i in range(1)]\n",
    "    )\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Actor(envs).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230207_092039-33bs2orw/files/agent_1023999steps.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Load the trained policy\n",
    "\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxForces = []\n",
    "n_actuators = []\n",
    "maxDevs = []\n",
    "rewards = []\n",
    "# Loop over all files\n",
    "for i in range(100):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(100):\n",
    "        with torch.no_grad():\n",
    "            action = agent.forward(obs)\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        # print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "    print('_'*30)\n",
    "    print(\"Initial Error\", info[\"initError\"])\n",
    "    print(\"Best Error:\", minError)\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))\n",
    "# print(\"**********************************************************\")\n",
    "# print(\"Number of actuators (mean) = %.3f\" %np.mean(n_actuators))\n",
    "# print(\"Number of actuators (median) = %.3f\" %np.median(n_actuators))\n",
    "# print(\"Number of actuators (stdev) = %.3f\" %np.std(n_actuators))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial error (mean) = 0.936\n",
    "Initial error (median) = 0.907\n",
    "Initial error (stdev) = 0.622\n",
    "Initial error (max) = 2.665\n",
    "**********************************************************\n",
    "Final error (mean) = 0.062\n",
    "Final error (median) = 0.050\n",
    "Final error (stdev) = 0.043\n",
    "Final error (max) = 0.284\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.159\n",
    "Max Deviation (median) = 0.135\n",
    "Max Deviation (stdev) = 0.119\n",
    "Max Deviation (max) = 0.920\n",
    "**********************************************************\n",
    "Max Force (mean) = 169.797\n",
    "Max Force (median) = 44.289\n",
    "Max Force (stdev) = 338.433\n",
    "Max Force (max) = 8324.552\n",
    "**********************************************************\n",
    "Episode Rewards (mean) = -11.289\n",
    "Episode Rewards (median) = -11.323\n",
    "Episode Rewards (stdev) = 2.323"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run-20230217_131757-1fkv29v4 (anneal action variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function VectorEnv.__del__ at 0x000001BC755269E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 294, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 221, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in close_extras\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in <listcomp>\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"c:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py\", line 322, in close\n",
      "    self.mapdl.exit() # close ANSYS\n",
      "AttributeError: 'FuselageActuatorsEnv' object has no attribute 'mapdl'\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: [0.96718062]\n",
      "Final Error: [0.00981279]\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.94158648]\n",
      "Final Error: [0.00928306]\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.98009813]\n",
      "Final Error: [0.01120078]\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.98476327]\n",
      "Final Error: [0.00878146]\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.96652764]\n",
      "Final Error: [0.0136032]\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.97904234]\n",
      "Final Error: [0.00584052]\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.96593184]\n",
      "Final Error: [0.02662083]\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.9845068]\n",
      "Final Error: [0.00747469]\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.98578822]\n",
      "Final Error: [0.00906683]\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.98017685]\n",
      "Final Error: [0.01280723]\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.94476558]\n",
      "Final Error: [0.00830962]\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.94223366]\n",
      "Final Error: [0.01387838]\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.98636971]\n",
      "Final Error: [0.00850858]\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.96084243]\n",
      "Final Error: [0.00971512]\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.95103878]\n",
      "Final Error: [0.01244876]\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.96850327]\n",
      "Final Error: [0.02840989]\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.98485192]\n",
      "Final Error: [0.00961321]\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.98344183]\n",
      "Final Error: [0.01528812]\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Reward: [0.87905396]\n",
      "Final Error: [0.0086279]\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.467\n",
      "Initial error (median) = 0.482\n",
      "Initial error (stdev) = 0.251\n",
      "Initial error (max) = 0.923\n",
      "**********************************************************\n",
      "Final error (mean) = 0.012\n",
      "Final error (median) = 0.010\n",
      "Final error (stdev) = 0.006\n",
      "Final error (max) = 0.028\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.037\n",
      "Max Deviation (median) = 0.036\n",
      "Max Deviation (stdev) = 0.013\n",
      "Max Deviation (max) = 0.076\n",
      "**********************************************************\n",
      "Max Force (mean) = 42.936\n",
      "Max Force (median) = 8.154\n",
      "Max Force (stdev) = 56.856\n",
      "Max Force (max) = 268.755\n",
      "**********************************************************\n",
      "Episode Rewards (mean) = 0.965\n",
      "Episode Rewards (median) = 0.969\n",
      "Episode Rewards (stdev) = 0.025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.single_action_space.shape)), requires_grad=False)  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None, scaleStd=1):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        action_mean = torch.tanh(self.fc3(x))\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)*scaleStd\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cpu\")\n",
    "agent = Agent(envs).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230217_131757-1fkv29v4/files/agent_32767872steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files[:-1]:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    env_name = \"FuselageActuators-v22\"\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    )\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    #initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    # done=False\n",
    "    # while not done:\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "    obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "    obs = torch.Tensor(obs).to(device)\n",
    "    episodeReward += reward\n",
    "    print(\"Reward:\", episodeReward)\n",
    "    print(\"Final Error:\", info[\"Error\"])\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(info[\"Error\"])\n",
    "    maxDevs.append(info[\"maxDev\"])\n",
    "    maxForces.append(np.max(np.abs(info[\"Forces\"])))\n",
    "    rewards.append(episodeReward)\n",
    "    # n_actuators.append(np.count_nonzero(envs.forces))\n",
    "\n",
    "envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.3f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.3f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.3f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.3f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.3f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.3f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.3f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.3f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.3f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.3f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.3f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.3f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.3f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.3f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.3f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.3f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Episode Rewards (mean) = %.3f\" %np.mean(rewards))\n",
    "print(\"Episode Rewards (median) = %.3f\" %np.median(rewards))\n",
    "print(\"Episode Rewards (stdev) = %.3f\" %np.std(rewards))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************\n",
    "Initial error (mean) = 0.467\n",
    "\n",
    "Initial error (median) = 0.482\n",
    "\n",
    "Initial error (stdev) = 0.251\n",
    "\n",
    "Initial error (max) = 0.923\n",
    "**********************************************************\n",
    "Final error (mean) = 0.012\n",
    "\n",
    "Final error (median) = 0.010\n",
    "\n",
    "Final error (stdev) = 0.006\n",
    "\n",
    "Final error (max) = 0.028\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.037\n",
    "\n",
    "Max Deviation (median) = 0.036\n",
    "\n",
    "Max Deviation (stdev) = 0.013\n",
    "\n",
    "Max Deviation (max) = 0.076\n",
    "**********************************************************\n",
    "Max Force (mean) = 42.936\n",
    "\n",
    "Max Force (median) = 8.154\n",
    "\n",
    "Max Force (stdev) = 56.856\n",
    "\n",
    "Max Force (max) = 268.755\n",
    "**********************************************************\n",
    "Episode Rewards (mean) = 0.965\n",
    "\n",
    "Episode Rewards (median) = 0.969\n",
    "\n",
    "Episode Rewards (stdev) = 0.025"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple refinements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** File: SolutionInputDP41.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.009812788319896537\n",
      "Intermediate Error: 0.009566103180384084\n",
      "Intermediate Error: 0.029143302538397236\n",
      "Intermediate Error: 0.009623502792903617\n",
      "Intermediate Error: 0.012324360217118436\n",
      "Intermediate Error: 0.03788054748716914\n",
      "Intermediate Error: 0.009551541400808746\n",
      "Intermediate Error: 0.0130421704813252\n",
      "Intermediate Error: 0.009717816194979204\n",
      "Intermediate Error: 0.009790655700307033\n",
      "______________________________\n",
      "Best Error: 0.009551541400808746\n",
      "****************************** File: SolutionInputDP42.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.009283062839353502\n",
      "Intermediate Error: 0.39141019396367255\n",
      "Intermediate Error: 0.19594118643693775\n",
      "Intermediate Error: 0.02133275803152623\n",
      "Intermediate Error: 0.009819965195586812\n",
      "Intermediate Error: 0.009718693911034594\n",
      "Intermediate Error: 0.009805772509530415\n",
      "Intermediate Error: 0.009902473837873313\n",
      "Intermediate Error: 0.010011667772221294\n",
      "Intermediate Error: 0.010132438302984577\n",
      "______________________________\n",
      "Best Error: 0.009283062839353502\n",
      "****************************** File: SolutionInputDP43.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.01120077853842241\n",
      "Intermediate Error: 0.011725643565204405\n",
      "Intermediate Error: 0.34023183208851804\n",
      "Intermediate Error: 0.1384297884864725\n",
      "Intermediate Error: 0.05290439217391294\n",
      "Intermediate Error: 0.011529961829356573\n",
      "Intermediate Error: 0.0901306218375245\n",
      "Intermediate Error: 0.04323307880610919\n",
      "Intermediate Error: 0.10615106888524503\n",
      "Intermediate Error: 0.35961480361180675\n",
      "______________________________\n",
      "Best Error: 0.01120077853842241\n",
      "****************************** File: SolutionInputDP44.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.008781459289463295\n",
      "Intermediate Error: 0.011097522513138506\n",
      "Intermediate Error: 0.011153492096179884\n",
      "Intermediate Error: 0.011087627099987769\n",
      "Intermediate Error: 0.011044197885670657\n",
      "Intermediate Error: 0.01101412384734097\n",
      "Intermediate Error: 0.010997491376227584\n",
      "Intermediate Error: 0.010994147079540469\n",
      "Intermediate Error: 0.011003846395751759\n",
      "Intermediate Error: 0.011026307769584659\n",
      "______________________________\n",
      "Best Error: 0.008781459289463295\n",
      "****************************** File: SolutionInputDP45.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.013603198635012735\n",
      "Intermediate Error: 0.41954746941172816\n",
      "Intermediate Error: 0.01323728666603836\n",
      "Intermediate Error: 0.01291873186409191\n",
      "Intermediate Error: 0.012626903542299306\n",
      "Intermediate Error: 0.012381938733201727\n",
      "Intermediate Error: 0.012169976223266214\n",
      "Intermediate Error: 0.01199057980149071\n",
      "Intermediate Error: 0.011843420413738731\n",
      "Intermediate Error: 0.011728088747405862\n",
      "______________________________\n",
      "Best Error: 0.011728088747405862\n",
      "****************************** File: SolutionInputDP46.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.0058405217097676565\n",
      "Intermediate Error: 0.43184129707600327\n",
      "Intermediate Error: 0.005375917428764298\n",
      "Intermediate Error: 0.053674517241755065\n",
      "Intermediate Error: 0.005061942232644946\n",
      "Intermediate Error: 0.005324508286341758\n",
      "Intermediate Error: 0.01942376274583165\n",
      "Intermediate Error: 0.3252189206053649\n",
      "Intermediate Error: 0.005740150860693231\n",
      "Intermediate Error: 0.005760972134831205\n",
      "______________________________\n",
      "Best Error: 0.005061942232644946\n",
      "****************************** File: SolutionInputDP47.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.02662082927886544\n",
      "Intermediate Error: 0.012086921303410696\n",
      "Intermediate Error: 0.06836888350401978\n",
      "Intermediate Error: 0.015092076243659621\n",
      "Intermediate Error: 0.014768550597308677\n",
      "Intermediate Error: 0.014860139355750361\n",
      "Intermediate Error: 0.014977643995852467\n",
      "Intermediate Error: 0.015102683010196828\n",
      "Intermediate Error: 0.01523481436067526\n",
      "Intermediate Error: 0.0153736934974786\n",
      "______________________________\n",
      "Best Error: 0.012086921303410696\n",
      "****************************** File: SolutionInputDP48.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.0074746867841259685\n",
      "Intermediate Error: 0.009875970001228147\n",
      "Intermediate Error: 0.016144483803687014\n",
      "Intermediate Error: 0.00974117250412523\n",
      "Intermediate Error: 0.013887869206982156\n",
      "Intermediate Error: 0.009755306531420224\n",
      "Intermediate Error: 0.012794104128865823\n",
      "Intermediate Error: 0.009839747040290866\n",
      "Intermediate Error: 0.012519756270520396\n",
      "Intermediate Error: 0.009989268479580869\n",
      "______________________________\n",
      "Best Error: 0.0074746867841259685\n",
      "****************************** File: SolutionInputDP49.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.009066828979807236\n",
      "Intermediate Error: 0.054406165004806814\n",
      "Intermediate Error: 0.018683858052873623\n",
      "Intermediate Error: 0.2689485644764085\n",
      "Intermediate Error: 0.015173267948303287\n",
      "Intermediate Error: 0.020149715506109202\n",
      "Intermediate Error: 0.020206308242652977\n",
      "Intermediate Error: 0.21639961710518021\n",
      "Intermediate Error: 0.09541361242013145\n",
      "Intermediate Error: 0.18120323820887055\n",
      "______________________________\n",
      "Best Error: 0.009066828979807236\n",
      "****************************** File: SolutionInputDP50.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Reconnect failed - remote exit again\n",
      "Wait and try to reconnect again - attempt 1\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 2\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.012807229154297134\n",
      "Intermediate Error: 0.01102703936835705\n",
      "Intermediate Error: 0.18461829511882796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function VectorEnv.__del__ at 0x000001BC755269E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 294, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 221, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in close_extras\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in <listcomp>\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"c:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py\", line 322, in close\n",
      "    self.mapdl.exit() # close ANSYS\n",
      "AttributeError: 'FuselageActuatorsEnv' object has no attribute 'mapdl'\n",
      "Exception ignored in: <function VectorEnv.__del__ at 0x000001BC755269E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 294, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\vector_env.py\", line 221, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in close_extras\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\vector\\sync_vector_env.py\", line 234, in <listcomp>\n",
      "    [env.close() for env in self.envs]\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  File \"c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\core.py\", line 435, in close\n",
      "    return self.env.close()\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"c:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py\", line 322, in close\n",
      "    self.mapdl.exit() # close ANSYS\n",
      "AttributeError: 'FuselageActuatorsEnv' object has no attribute 'mapdl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Error: 0.011465180619962934\n",
      "Intermediate Error: 0.010346405936671242\n",
      "Intermediate Error: 0.13958319083182827\n",
      "Intermediate Error: 0.09949363657676186\n",
      "Intermediate Error: 0.05642071904489391\n",
      "Intermediate Error: 0.009537327842655301\n",
      "Intermediate Error: 0.06876661187287697\n",
      "______________________________\n",
      "Best Error: 0.009537327842655301\n",
      "****************************** File: SolutionInputDP51.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.008309622019290782\n",
      "Intermediate Error: 0.0082851463175274\n",
      "Intermediate Error: 0.0075398874900382586\n",
      "Intermediate Error: 0.007252644546075483\n",
      "Intermediate Error: 0.006976943497692868\n",
      "Intermediate Error: 0.006724838361895961\n",
      "Intermediate Error: 0.006497227445653603\n",
      "Intermediate Error: 0.006295972374072944\n",
      "Intermediate Error: 0.006122846908763734\n",
      "Intermediate Error: 0.005979381322364176\n",
      "______________________________\n",
      "Best Error: 0.005979381322364176\n",
      "****************************** File: SolutionInputDP52.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.013878382282100149\n",
      "Intermediate Error: 0.09182431521386307\n",
      "Intermediate Error: 0.3437707179433513\n",
      "Intermediate Error: 0.012891036364671794\n",
      "Intermediate Error: 0.020948249727957353\n",
      "Intermediate Error: 0.2781226142363076\n",
      "Intermediate Error: 0.011970860321232986\n",
      "Intermediate Error: 0.012319918636170178\n",
      "Intermediate Error: 0.010648024572868587\n",
      "Intermediate Error: 0.01158856960706052\n",
      "______________________________\n",
      "Best Error: 0.010648024572868587\n",
      "****************************** File: SolutionInputDP53.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.008508575924480425\n",
      "Intermediate Error: 0.03350375551162011\n",
      "Intermediate Error: 0.5121539831461631\n",
      "Intermediate Error: 0.013887844447917148\n",
      "Intermediate Error: 0.4055155563985099\n",
      "Intermediate Error: 0.01250323500055907\n",
      "Exit Ansys and try to reconnect\n",
      "Remote exit\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.02050453316148027\n",
      "Intermediate Error: 0.01990163583863354\n",
      "Intermediate Error: 0.01989245965319814\n",
      "Intermediate Error: 0.02002800315595329\n",
      "______________________________\n",
      "Best Error: 0.008508575924480425\n",
      "****************************** File: SolutionInputDP54.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.009715122635226859\n",
      "Intermediate Error: 0.01106002184506326\n",
      "Intermediate Error: 0.020219868536656114\n",
      "Intermediate Error: 0.011483859259537214\n",
      "Intermediate Error: 0.014217133655248447\n",
      "Intermediate Error: 0.01158141831559003\n",
      "Intermediate Error: 0.010881442738235861\n",
      "Intermediate Error: 0.011745089295082436\n",
      "Intermediate Error: 0.010055221799782759\n",
      "Intermediate Error: 0.01045470721826988\n",
      "______________________________\n",
      "Best Error: 0.009715122635226859\n",
      "****************************** File: SolutionInputDP55.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.012448758220990728\n",
      "Intermediate Error: 0.012505774491426706\n",
      "Intermediate Error: 0.012820094533417137\n",
      "Intermediate Error: 0.012749113304237721\n",
      "Intermediate Error: 0.01270415870709861\n",
      "Intermediate Error: 0.012681830776928929\n",
      "Intermediate Error: 0.012681144112807172\n",
      "Intermediate Error: 0.012701477875656635\n",
      "Intermediate Error: 0.01274211696201072\n",
      "Intermediate Error: 0.012802193258335466\n",
      "______________________________\n",
      "Best Error: 0.012448758220990728\n",
      "****************************** File: SolutionInputDP56.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.028409886199216045\n",
      "Intermediate Error: 0.015166179560272967\n",
      "Intermediate Error: 0.05990530594950759\n",
      "Intermediate Error: 0.04400949081067714\n",
      "Intermediate Error: 0.014790073519269501\n",
      "Intermediate Error: 0.026475508003152095\n",
      "Intermediate Error: 0.14122802328948875\n",
      "Intermediate Error: 0.03181612985227139\n",
      "Intermediate Error: 0.016525770886840492\n",
      "Intermediate Error: 0.031858709320751584\n",
      "______________________________\n",
      "Best Error: 0.014790073519269501\n",
      "****************************** File: SolutionInputDP57.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.00961321015514201\n",
      "Intermediate Error: 0.010483682942903548\n",
      "Intermediate Error: 0.010316445582820747\n",
      "Intermediate Error: 0.01014649791178209\n",
      "Intermediate Error: 0.009995975197132495\n",
      "Intermediate Error: 0.009869533523504451\n",
      "Intermediate Error: 0.009766726454899184\n",
      "Intermediate Error: 0.009687500174337308\n",
      "Intermediate Error: 0.009631660768055425\n",
      "Intermediate Error: 0.0095988212274584\n",
      "______________________________\n",
      "Best Error: 0.0095988212274584\n",
      "****************************** File: SolutionInputDP58.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.01528812196534611\n",
      "Intermediate Error: 0.033652583027908155\n",
      "Intermediate Error: 0.07058977967882657\n",
      "Intermediate Error: 0.03230828771846451\n",
      "Intermediate Error: 0.04365213093245594\n",
      "Intermediate Error: 0.039933807277552365\n",
      "Intermediate Error: 0.03275912396395485\n",
      "Intermediate Error: 0.037109386279010635\n",
      "Intermediate Error: 0.03234631276957775\n",
      "Intermediate Error: 0.03059556917071865\n",
      "______________________________\n",
      "Best Error: 0.01528812196534611\n",
      "****************************** File: SolutionInputDP59.inp ******************************\n",
      "Exit Ansys and try to reconnect\n",
      "No active Ansys process found. Wait and try to reconnect\n",
      "Product:             Ansys Mechanical Enterprise Academic Teaching\n",
      "MAPDL Version:       22.1\n",
      "ansys.mapdl Version: 0.61.2\n",
      "\n",
      "Running on 4 processors\n",
      "Sucessfully reconnected to Ansys on attempt 1\n",
      "Try running again\n",
      "Simulation setup complete\n",
      "Applied forces\n",
      "Solve finished\n",
      "Results ready\n",
      "Intermediate Error: 0.008627899474114112\n",
      "Intermediate Error: 0.00823971101662563\n",
      "Intermediate Error: 0.00807157894628324\n",
      "Intermediate Error: 0.01091286659314698\n",
      "Intermediate Error: 0.3397761212398295\n",
      "Intermediate Error: 0.009758083998189662\n",
      "Intermediate Error: 0.2625065805015005\n",
      "Intermediate Error: 0.010410019907418687\n",
      "Intermediate Error: 0.21364280625976506\n",
      "Intermediate Error: 0.01096100746580226\n",
      "______________________________\n",
      "Best Error: 0.00807157894628324\n",
      "**********************************************************\n",
      "Initial error (mean) = 0.467\n",
      "Initial error (median) = 0.482\n",
      "Initial error (stdev) = 0.251\n",
      "Initial error (max) = 0.923\n",
      "**********************************************************\n",
      "Final error (mean) = 0.010\n",
      "Final error (median) = 0.010\n",
      "Final error (stdev) = 0.003\n",
      "Final error (max) = 0.015\n",
      "**********************************************************\n",
      "Max Deviation (mean) = 0.034\n",
      "Max Deviation (median) = 0.035\n",
      "Max Deviation (stdev) = 0.009\n",
      "Max Deviation (max) = 0.056\n",
      "**********************************************************\n",
      "Max Force (mean) = 214.811\n",
      "Max Force (median) = 207.556\n",
      "Max Force (stdev) = 89.130\n",
      "Max Force (max) = 432.262\n",
      "**********************************************************\n",
      "Best Forces: [array([ -48.908607,  263.65738 , -171.87653 , -127.78208 ,    6.419945,\n",
      "          0.      ,  -16.233547,    0.      ,    0.      ,    0.      ,\n",
      "          0.      ,    0.      ,    0.      , -161.35428 ,  294.33905 ,\n",
      "       -236.72964 ,    0.      ,  -73.93232 ], dtype=float32), array([ -41.853867 ,  309.7627   ,  -42.605522 ,  -44.9723   ,\n",
      "         51.612507 ,    0.       ,    0.8778415,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       , -231.32393  ,  103.7371   ,   48.126053 ,\n",
      "          0.       ,  153.56876  ], dtype=float32), array([   0.       ,  151.17896  ,   23.643522 ,   16.007038 ,\n",
      "         36.215454 ,    0.       ,  153.4598   ,    0.       ,\n",
      "          0.       ,   -1.3686525,    0.       ,    0.       ,\n",
      "          0.       , -487.848    ,  -27.819506 ,  -12.582088 ,\n",
      "          0.       ,   76.188225 ], dtype=float32), array([  54.79541  ,  185.5225   ,  105.30403  ,   44.919647 ,\n",
      "        126.92547  ,    0.       ,    7.7070575,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       , -133.71098  ,   72.94487  ,   12.761428 ,\n",
      "          0.       ,  176.38289  ], dtype=float32), array([ -45.41841 ,  172.02068 , -207.37152 ,  -99.04781 ,  -12.094856,\n",
      "          0.      , -181.8081  ,    0.      ,    0.      ,    0.      ,\n",
      "          0.      ,    0.      ,    0.      ,  -78.776276,  290.29163 ,\n",
      "       -432.26227 ,    0.      , -114.57661 ], dtype=float32), array([  -7.826947,  -64.37797 , -153.31958 ,   54.542866,  155.17781 ,\n",
      "          0.      ,   25.022495,    0.      ,    0.      ,    0.      ,\n",
      "          0.      ,    0.      ,    0.      ,   61.758606,  232.95917 ,\n",
      "       -337.61475 ,    0.      , -105.96288 ], dtype=float32), array([-224.24611,  439.74054, -231.70178, -197.80527,  -76.96854,\n",
      "          0.     ,  -78.13535,    0.     ,    0.     ,    0.     ,\n",
      "          0.     ,    0.     ,    0.     , -126.0265 ,  433.03662,\n",
      "       -160.964  ,    0.     ,   68.81666], dtype=float32), array([   4.7996893,  178.60568  ,    1.4434576,   15.56208  ,\n",
      "         96.65494  ,    0.       ,   41.646584 ,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       , -109.85656  ,  163.02512  ,  -34.150917 ,\n",
      "          0.       ,    7.0154543], dtype=float32), array([-343.8229   ,  554.1649   , -207.19392  , -239.34645  ,\n",
      "        -21.939676 ,    0.       ,  -78.97061  ,   -1.8200352,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       , -255.6346   ,  354.88843  , -106.83055  ,\n",
      "          0.       ,    0.       ], dtype=float32), array([  41.60564  ,   91.733246 ,  -30.095795 ,  -47.19876  ,\n",
      "         11.774293 ,    0.       ,   68.73692  ,   -1.8244779,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       , -101.0049   ,  116.14839  , -233.60869  ,\n",
      "          0.       ,    0.       ], dtype=float32), array([-192.06065 ,  159.69006 ,  -87.22751 ,   48.63133 ,  176.4208  ,\n",
      "          0.      ,  174.91225 ,    0.      ,    0.      ,    0.      ,\n",
      "          0.      ,    0.      ,    0.      ,  -11.701187,  225.1264  ,\n",
      "       -159.825   ,    0.      ,  -40.197384], dtype=float32), array([  -5.8145857,  180.70341  , -242.73956  , -177.18146  ,\n",
      "          0.       ,    0.       ,  -62.109566 ,   -2.0903366,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       ,  -57.044975 ,  412.98737  , -242.33339  ,\n",
      "          0.       , -103.36903  ], dtype=float32), array([-3.7795886e+02,  5.7859216e+02, -6.1043930e+01, -1.6284579e+02,\n",
      "        1.2764788e-01,  0.0000000e+00,  1.0657869e+01,  0.0000000e+00,\n",
      "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
      "        0.0000000e+00, -2.5086694e+02,  2.5049840e+02,  1.0280165e+02,\n",
      "        0.0000000e+00,  1.3832387e+02], dtype=float32), array([-225.21663  ,  265.74442  , -114.87609  ,  -55.91156  ,\n",
      "         62.9592   ,    0.       ,   -3.2181048,   -3.9198523,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       ,  -69.26674  ,  277.67163  , -148.35844  ,\n",
      "          0.       ,    0.       ], dtype=float32), array([ -93.319626,  310.39227 ,  -94.65555 ,  -97.89765 ,   48.60538 ,\n",
      "          0.      ,  -77.50452 ,    0.      ,    0.      ,    0.      ,\n",
      "          0.      ,    0.      ,    0.      ,    7.384687,  313.90918 ,\n",
      "       -150.12364 ,    0.      ,   81.3491  ], dtype=float32), array([-455.45743  ,  308.92325  , -258.81287  ,  -58.131195 ,\n",
      "         33.53339  ,    0.       ,    6.575047 ,   -1.7924726,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       , -139.4696   ,  334.27487  , -201.27582  ,\n",
      "          0.       ,    0.       ], dtype=float32), array([ -68.56815  ,  121.63669  ,    1.6931915,   59.75154  ,\n",
      "        155.32724  ,    0.       ,   91.491234 ,    0.       ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       ,  -80.628494 ,  112.892296 , -160.5085   ,\n",
      "          0.       ,   -3.7730932], dtype=float32), array([-314.74924 ,  429.15317 , -145.0956  , -167.90437 ,    0.      ,\n",
      "          0.      ,   11.423353,   -5.937129,    0.      ,    0.      ,\n",
      "          0.      ,    0.      ,    0.      , -211.87193 ,  321.26465 ,\n",
      "          4.149229,    0.      ,  133.38179 ], dtype=float32), array([  17.134027 ,   54.80349  , -157.41905  ,  -11.679117 ,\n",
      "         39.385475 ,    0.       ,    0.       ,   -8.201815 ,\n",
      "          0.       ,    0.       ,    0.       ,    0.       ,\n",
      "          0.       ,   -2.1378474,  307.20517  , -247.94601  ,\n",
      "          0.       ,  -76.39739  ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    env = gym.make(env_id, n_actuators=n_actions, mode=\"File\", file1=file1, file2=file2, record=record, seed=seed, port=50056+idx)\n",
    "    #env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.action_space.shape)), requires_grad=False)  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None, scaleStd=1):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        action_mean = torch.tanh(self.fc3(x))\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd#.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)*scaleStd\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#     [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "# )\n",
    "\n",
    "envs = make_env(env_name, 0, 0, 10, file1, file2, False)\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cpu\")\n",
    "agent = Agent(envs).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230217_131757-1fkv29v4/files/agent_32767872steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "bestForces = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files[:-1]:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    # env_name = \"FuselageActuators-v22\"\n",
    "    # envs = gym.vector.SyncVectorEnv(\n",
    "    #     [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    # )\n",
    "\n",
    "    envs = make_env(env_name, 0, 0, 10, file1, file2, False)\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    # initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(10):\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "            bestForce = info[\"Forces\"]\n",
    "    print('_'*30)\n",
    "    print(\"Best Error:\", minError)\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    bestForces.append(bestForce)\n",
    "\n",
    "    envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.4f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.4f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.4f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.4f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.4f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.4f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.4f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.4f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.4f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.4f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.4f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.4f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.4f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.4f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.4f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.4f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Best Forces:\", bestForces)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple refinements (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\scooby\\knowledge.py:14: DeprecationWarning: The distutils.sysconfig module is deprecated, use sysconfig instead\n",
      "  import distutils.sysconfig as sysconfig\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMAPDL is taking longer than expected to connect to an MAPDL session.\n",
      "Checking if there are any available licenses...\n",
      "PyMAPDL is taking longer than expected to connect to an MAPDL session.\n",
      "Checking if there are any available licenses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1072: DeprecationWarning: invalid escape sequence '\\d'\n",
      "  and re.search(\"ansys\\d\\d\\d\", os.path.basename(os.path.normpath(exe_loc)))\n",
      "c:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1249: DeprecationWarning: invalid escape sequence '\\-'\n",
      "  \"\"\"Start MAPDL locally.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMapdlConnectionError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1851\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, remove_temp_files, remove_temp_dir_on_exit, verbose_mapdl, license_server_check, license_type, print_com, add_env_vars, replace_env_vars, version, **kwargs)\u001b[0m\n\u001b[0;32m   1843\u001b[0m port, actual_run_location, process \u001b[39m=\u001b[39m launch_grpc(\n\u001b[0;32m   1844\u001b[0m     port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1845\u001b[0m     ip\u001b[39m=\u001b[39mip,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1850\u001b[0m )\n\u001b[1;32m-> 1851\u001b[0m mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n\u001b[0;32m   1852\u001b[0m     ip\u001b[39m=\u001b[39mip,\n\u001b[0;32m   1853\u001b[0m     port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1854\u001b[0m     cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n\u001b[0;32m   1855\u001b[0m     loglevel\u001b[39m=\u001b[39mloglevel,\n\u001b[0;32m   1856\u001b[0m     set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n\u001b[0;32m   1857\u001b[0m     remove_temp_dir_on_exit\u001b[39m=\u001b[39mremove_temp_dir_on_exit,\n\u001b[0;32m   1858\u001b[0m     log_apdl\u001b[39m=\u001b[39mlog_apdl,\n\u001b[0;32m   1859\u001b[0m     process\u001b[39m=\u001b[39mprocess,\n\u001b[0;32m   1860\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1861\u001b[0m )\n\u001b[0;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:386\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, remove_temp_dir_on_exit, print_com, channel, remote_instance, **start_parm)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_mortem_checks()\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mraise\u001b[39;00m err  \u001b[39m# Raise original error if we couldn't catch it in post-mortem analysis\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:383\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, remove_temp_dir_on_exit, print_com, channel, remote_instance, **start_parm)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 383\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect(timeout\u001b[39m=\u001b[39;49mtimeout, set_no_abort\u001b[39m=\u001b[39;49mset_no_abort)\n\u001b[0;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m MapdlConnectionError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:454\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m connected:\n\u001b[1;32m--> 454\u001b[0m     \u001b[39mraise\u001b[39;00m MapdlConnectionError(\n\u001b[0;32m    455\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to connect to MAPDL gRPC instance at \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_channel_str\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mMapdlConnectionError\u001b[0m: Unable to connect to MAPDL gRPC instance at 127.0.0.1:50052.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLicenseServerConnectionError\u001b[0m              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py:486\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    485\u001b[0m n_cpu \u001b[39m=\u001b[39m psutil\u001b[39m.\u001b[39mcpu_count(logical\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m    487\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1870\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, remove_temp_files, remove_temp_dir_on_exit, verbose_mapdl, license_server_check, license_type, print_com, add_env_vars, replace_env_vars, version, **kwargs)\u001b[0m\n\u001b[0;32m   1869\u001b[0m     LOG\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mChecking license server.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1870\u001b[0m     lic_check\u001b[39m.\u001b[39;49mcheck()\n\u001b[0;32m   1872\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\licensing.py:160\u001b[0m, in \u001b[0;36mLicenseChecker.check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_file_success \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mraise\u001b[39;00m LicenseServerConnectionError(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_file_msg))\n\u001b[0;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_license_checkout_success:\n",
      "\u001b[1;31mLicenseServerConnectionError\u001b[0m: Exceeded timeout of 13 seconds while examining:\nC:\\Users\\TL\\AppData\\Local\\Temp\\.ansys\\licdebug.FECONL24TS.FEAT_ANSYS.221.out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 67\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m file1 \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# envs = gym.vector.SyncVectorEnv(\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m#     [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m envs \u001b[39m=\u001b[39m make_env(env_name, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m10\u001b[39;49m, file1, file2, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Create agent\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\Benchmark.ipynb Cell 67\u001b[0m in \u001b[0;36mmake_env\u001b[1;34m(env_id, seed, idx, n_actions, file1, file2, record)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_env\u001b[39m(env_id, seed, idx, n_actions, file1, file2, record):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(env_id, n_actuators\u001b[39m=\u001b[39;49mn_actions, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTest\u001b[39;49m\u001b[39m\"\u001b[39;49m, record\u001b[39m=\u001b[39;49mrecord, seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m#env = gym.wrappers.RecordEpisodeStatistics(env)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TL/Projects/Fuselage%20Actuator%20Reinforcement%20Learning/AssemblyGym/Benchmark.ipynb#Y124sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\gym\\envs\\registration.py:649\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[0;32m    645\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid render_mode provided: \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m. Valid render_modes: None, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(render_modes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    646\u001b[0m         )\n\u001b[0;32m    648\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m    650\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    651\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    652\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    653\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[0;32m    654\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py:76\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv.__init__\u001b[1;34m(self, render_mode, n_actuators, mode, port, file1, file2, record, seed)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSurrogate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m     \u001b[39m# Check if MAPDL server is active, and start it if it's not\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_process(\u001b[39m'\u001b[39m\u001b[39mansys\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 76\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch_ansys()\n\u001b[0;32m     77\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSurrogate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurrogate \u001b[39m=\u001b[39m load(path\u001b[39m.\u001b[39mjoin(path\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mSurrogates\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msurrogate_likeDu_v22.joblib\u001b[39m\u001b[39m'\u001b[39m) )\n",
      "File \u001b[1;32mc:\\Users\\TL\\Projects\\Fuselage Actuator Reinforcement Learning\\AssemblyGym\\AssemblyGym\\envs\\FuselageActuators\\FuselageActuators_env_v22.py:491\u001b[0m, in \u001b[0;36mFuselageActuatorsEnv._launch_ansys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m     n_cpu\u001b[39m=\u001b[39m\u001b[39mmin\u001b[39m(\u001b[39m4\u001b[39m, n_cpu) \u001b[39m#license sometimes won't let me use more than 4 processors?\u001b[39;00m\n\u001b[1;32m--> 491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl \u001b[39m=\u001b[39m launch_mapdl(loglevel\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mERROR\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, port\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport, nproc\u001b[39m=\u001b[39;49mn_cpu, cleanup_on_exit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, override\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m    492\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmapdl)\n\u001b[0;32m    493\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning on\u001b[39m\u001b[39m\"\u001b[39m, n_cpu, \u001b[39m\"\u001b[39m\u001b[39mprocessors\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\launcher.py:1851\u001b[0m, in \u001b[0;36mlaunch_mapdl\u001b[1;34m(exec_file, run_location, jobname, nproc, ram, mode, override, loglevel, additional_switches, start_timeout, port, cleanup_on_exit, start_instance, ip, clear_on_connect, log_apdl, remove_temp_files, remove_temp_dir_on_exit, verbose_mapdl, license_server_check, license_type, print_com, add_env_vars, replace_env_vars, version, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgrpc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1843\u001b[0m     port, actual_run_location, process \u001b[39m=\u001b[39m launch_grpc(\n\u001b[0;32m   1844\u001b[0m         port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1845\u001b[0m         ip\u001b[39m=\u001b[39mip,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1850\u001b[0m     )\n\u001b[1;32m-> 1851\u001b[0m     mapdl \u001b[39m=\u001b[39m MapdlGrpc(\n\u001b[0;32m   1852\u001b[0m         ip\u001b[39m=\u001b[39mip,\n\u001b[0;32m   1853\u001b[0m         port\u001b[39m=\u001b[39mport,\n\u001b[0;32m   1854\u001b[0m         cleanup_on_exit\u001b[39m=\u001b[39mcleanup_on_exit,\n\u001b[0;32m   1855\u001b[0m         loglevel\u001b[39m=\u001b[39mloglevel,\n\u001b[0;32m   1856\u001b[0m         set_no_abort\u001b[39m=\u001b[39mset_no_abort,\n\u001b[0;32m   1857\u001b[0m         remove_temp_dir_on_exit\u001b[39m=\u001b[39mremove_temp_dir_on_exit,\n\u001b[0;32m   1858\u001b[0m         log_apdl\u001b[39m=\u001b[39mlog_apdl,\n\u001b[0;32m   1859\u001b[0m         process\u001b[39m=\u001b[39mprocess,\n\u001b[0;32m   1860\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstart_parm,\n\u001b[0;32m   1861\u001b[0m     )\n\u001b[0;32m   1862\u001b[0m     \u001b[39mif\u001b[39;00m run_location \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1863\u001b[0m         mapdl\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m actual_run_location\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:383\u001b[0m, in \u001b[0;36mMapdlGrpc.__init__\u001b[1;34m(self, ip, port, timeout, loglevel, log_file, cleanup_on_exit, log_apdl, set_no_abort, remove_temp_files, remove_temp_dir_on_exit, print_com, channel, remote_instance, **start_parm)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mapdl_process \u001b[39m=\u001b[39m process\n\u001b[0;32m    382\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 383\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_connect(timeout\u001b[39m=\u001b[39;49mtimeout, set_no_abort\u001b[39m=\u001b[39;49mset_no_abort)\n\u001b[0;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m MapdlConnectionError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_mortem_checks()\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:439\u001b[0m, in \u001b[0;36mMapdlGrpc._multi_connect\u001b[1;34m(self, n_attempts, timeout, set_no_abort)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mwhile\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m<\u001b[39m max_time \u001b[39mand\u001b[39;00m i \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_attempts:\n\u001b[0;32m    438\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mConnection attempt \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 439\u001b[0m     connected \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connect(\n\u001b[0;32m    440\u001b[0m         timeout\u001b[39m=\u001b[39;49mattempt_timeout, set_no_abort\u001b[39m=\u001b[39;49mset_no_abort\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    443\u001b[0m     \u001b[39mif\u001b[39;00m connected:\n",
      "File \u001b[1;32mc:\\Users\\TL\\Documents\\PythonVENV\\venv3_10-pyAnsys\\lib\\site-packages\\ansys\\mapdl\\core\\mapdl_grpc.py:641\u001b[0m, in \u001b[0;36mMapdlGrpc._connect\u001b[1;34m(self, timeout, set_no_abort, enable_health_check)\u001b[0m\n\u001b[0;32m    639\u001b[0m tstart \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    640\u001b[0m \u001b[39mwhile\u001b[39;00m ((time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m tstart) \u001b[39m<\u001b[39m timeout) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39m_matured:\n\u001b[1;32m--> 641\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m    643\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39m_matured:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from AssemblyGym.envs import FuselageActuators\n",
    "\n",
    "def make_env(env_id, seed, idx, n_actions, file1, file2, record):\n",
    "    env = gym.make(env_id, n_actuators=n_actions, mode=\"Test\", record=record, seed=seed)\n",
    "    #env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        # layers for self.actor_mean\n",
    "        self.fc1 = layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64))\n",
    "        self.fc2 = layer_init(nn.Linear(64, 64))\n",
    "        self.fc3 = layer_init(nn.Linear(64, np.prod(envs.action_space.shape)), std=0.01)\n",
    "        \n",
    "        self.actor_logstd = nn.Parameter(-5*torch.ones(1, np.prod(envs.action_space.shape)), requires_grad=False)  # initial action_std = exp(actor_logstd)\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None, scaleStd=1):\n",
    "        # Start with standard MLP\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        action_mean = torch.tanh(self.fc3(x))\n",
    "        # Build action distribution\n",
    "        action_logstd = self.actor_logstd#.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)*scaleStd\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if action == \"deterministic\":\n",
    "            action = action_mean\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(obs)\n",
    "\n",
    "# Make the environment\n",
    "env_name = \"FuselageActuators-v22\"\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "file1 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "# envs = gym.vector.SyncVectorEnv(\n",
    "#     [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "# )\n",
    "\n",
    "envs = make_env(env_name, 0, 0, 10, file1, file2, False)\n",
    "\n",
    "# Create agent\n",
    "device = torch.device(\"cpu\")\n",
    "agent = Agent(envs).to(device)\n",
    "agent.load_state_dict(torch.load(\n",
    "            \"./wandb/run-20230217_131757-1fkv29v4/files/agent_32767872steps.pt\", map_location=device))\n",
    "\n",
    "# Initialze variables\n",
    "initErrors = []\n",
    "finalErrors = []\n",
    "maxDevs = []\n",
    "maxForces = []\n",
    "rewards = []\n",
    "bestForces = []\n",
    "\n",
    "# Select files\n",
    "file2 ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Benchmark/SolutionInputUndeformed.inp'\n",
    "folder ='C:/Users/TL/Projects/Fuselage Actuator Reinforcement Learning/AssemblyGym/AssemblyGym/envs/FuselageActuators/AnsysFiles/Test/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over all files\n",
    "for f in files[:-1]:\n",
    "    file1 = path.join(folder, f)\n",
    "    dp = file1[-8:-4] # Design point\n",
    "\n",
    "    # Make the environment\n",
    "    # env_name = \"FuselageActuators-v22\"\n",
    "    # envs = gym.vector.SyncVectorEnv(\n",
    "    #     [make_env(env_name, 0 + i, i, 10, file1, file2, False) for i in range(1)]\n",
    "    # )\n",
    "\n",
    "    envs = make_env(env_name, 0, 0, 10, file1, file2, False)\n",
    "\n",
    "    print('*' * 30, f'File: {f}', '*' * 30)\n",
    "    # Perform test and track error\n",
    "    obs = torch.Tensor(envs.reset()).to(device)\n",
    "    # initErrors.append(envs.error_initial)\n",
    "    episodeReward = 0\n",
    "    minError=10\n",
    "    \n",
    "    for j in range(10):\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(obs, action = \"deterministic\")\n",
    "        obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        obs = torch.Tensor(obs).to(device)\n",
    "        episodeReward += reward\n",
    "        # print(\"Intermediate Reward:\", reward)\n",
    "        print(\"Intermediate Error:\", info[\"Error\"])\n",
    "        if info[\"Error\"]<minError:\n",
    "            minError= info[\"Error\"]\n",
    "            maxDev = info[\"maxDev\"]\n",
    "            maxForce = np.max(np.abs(info[\"Forces\"]))\n",
    "            bestForce = info[\"Forces\"]\n",
    "    print('_'*30)\n",
    "    print(\"Best Error:\", minError)\n",
    "    initErrors.append(info[\"initError\"])\n",
    "    finalErrors.append(minError)\n",
    "    maxForces.append(maxForce)\n",
    "    maxDevs.append(maxDev)\n",
    "    rewards.append(episodeReward)\n",
    "    bestForces.append(bestForce)\n",
    "\n",
    "    envs.close()\n",
    "\n",
    "print(\"**********************************************************\")\n",
    "print(\"Initial error (mean) = %.4f\" %np.mean(initErrors))\n",
    "print(\"Initial error (median) = %.4f\" %np.median(initErrors))\n",
    "print(\"Initial error (stdev) = %.4f\" %np.std(initErrors))\n",
    "print(\"Initial error (max) = %.4f\" %np.max(initErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Final error (mean) = %.4f\" %np.mean(finalErrors))\n",
    "print(\"Final error (median) = %.4f\" %np.median(finalErrors))\n",
    "print(\"Final error (stdev) = %.4f\" %np.std(finalErrors))\n",
    "print(\"Final error (max) = %.4f\" %np.max(finalErrors))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Deviation (mean) = %.4f\" %np.mean(maxDevs))\n",
    "print(\"Max Deviation (median) = %.4f\" %np.median(maxDevs))\n",
    "print(\"Max Deviation (stdev) = %.4f\" %np.std(maxDevs))\n",
    "print(\"Max Deviation (max) = %.4f\" %np.max(maxDevs))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Max Force (mean) = %.4f\" %np.mean(maxForces))\n",
    "print(\"Max Force (median) = %.4f\" %np.median(maxForces))\n",
    "print(\"Max Force (stdev) = %.4f\" %np.std(maxForces))\n",
    "print(\"Max Force (max) = %.4f\" %np.max(maxForces))\n",
    "print(\"**********************************************************\")\n",
    "print(\"Best Forces:\", bestForces)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial error (mean) = 0.467\n",
    "Initial error (median) = 0.482\n",
    "Initial error (stdev) = 0.251\n",
    "Initial error (max) = 0.923\n",
    "**********************************************************\n",
    "Final error (mean) = 0.010\n",
    "Final error (median) = 0.010\n",
    "Final error (stdev) = 0.003\n",
    "Final error (max) = 0.015\n",
    "**********************************************************\n",
    "Max Deviation (mean) = 0.034\n",
    "Max Deviation (median) = 0.035\n",
    "Max Deviation (stdev) = 0.009\n",
    "Max Deviation (max) = 0.056\n",
    "**********************************************************\n",
    "Max Force (mean) = 214.811\n",
    "Max Force (median) = 207.556\n",
    "Max Force (stdev) = 89.130\n",
    "Max Force (max) = 432.262\n",
    "**********************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3_10-pyAnsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd863a7d14884af62bdfc34f7d5bcc83130f12e68ccc24c922df4ad6628ca258"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
